{"posts":[{"title":"腾讯云操作系统镜像使用方法","text":"最近一个项目部署在腾讯云服务器(http://www.qcloud.com/)上，在服务器配置时使用了操作系统镜像功能，感觉很赞。因此把使用方法记录下来，供需要的同事参考。为什么需要镜像功能写文档时，Ctrl C 加 Ctrl V，多方便啊。有需要多台服务器时，只要配置一台的各种软件及服务，其它的机器都复制过去，是不是也很省事啊？这就是镜像功能存在的意义。 使用流程先解释两个名词：1. 参考服务器。指用于制作镜像的机器，其配置将被复制到其它机器上。2. 目标服务器。指需要镜像的机器，将复杂参考服务器上的配置。 流程如下： 图1 1.配置参考服务器安装需要的软件，完成配置。2.制作参考服务器镜像这里需要注意两点： 只对系统盘做镜像，数据盘需要另外处理。 制作过程中需要关机，整个过程持续十几分钟。 具体过程如下图所示。 图2 图3 制作镜像时，需要知道那个是系统盘： # df -lh Filesystem Size Used Avail Use% Mounted on /dev/vda1 7.9G 2.9G 4.6G 39% / udev 7.9G 148K 7.9G 1% /dev /dev/vdb1 493G 245M 467G 1% /data 但没有图3中/dev/xvda。咨询后，得到如下信息： /dev/xvda是xen虚拟化的系统盘 /dev/vda是kvm虚拟化的系统盘 这样系统盘就是/dev/vda1，而数据盘是/dev/vdb1 经过十几分钟，在镜像管理页面可以看到已制作的镜像。 图4 3.目标服务器重装操作系统 这里腾讯云有完整的教程，不再赘述。传送门 收尾工作 在目标服务器重装操作系统后，工作基本结束。不过如果服务器挂载了数据盘，还要做下最后的收尾工作。 使用命令“mkdir /mydata”创建mydata目录，再通过“mount /dev/xvdb1 /mydata”命令手动挂载新分区后，用“df -h”命令查看，出现以下信息说明挂载成功，即可以查看到数据盘了。 图5 至此，已成功将参考服务器的镜像复制到目标服务器，Enjoy。","link":"/2014/10/21/operation-system-image-in-tencent-cloud.html"},{"title":"Coding Tips -- Compare The Version Numbers Between Two Releases","text":"Given two version numbers, How to check which release is newer ? For example: 1.1 is newer than 1.0 (1.1 &gt; 1.0); 1.0.0 is equal to 1.0(1.0.0 = 1.0); 0.9 is older than 1.0(0.9 &lt; 1.0). Function implement is as below: /** * Compare two versions. * @param string $newVersion * @param string $oldVersion * @return int. 1 represents '&gt;',0 represents '=',-1 represents '&lt;' */ function versionCompare($newVersion,$oldVersion) { $newArray = explode(&quot;.&quot;,$newVersion); $oldArray = explode(&quot;.&quot;,$oldVersion); do{ $v1 = array_shift($newArray); $v2 = array_shift($oldArray); if($v1 == NULL){ $v1 = 0; } if($v2 == NULL){ $v2 = 0; } if($v1 &gt; $v2){ return 1; }else if($v1 &lt; $v2){ return -1; } }while(count($newArray) &gt; 0 || count($oldArray) &gt; 0); return 0; } Test Cases is as below: var_dump(versionCompare('','')); // = var_dump(versionCompare('1.0','1.0')); // = var_dump(versionCompare('1.1','1.0')); // &gt; var_dump(versionCompare('0.9','1.0')); // &lt; var_dump(versionCompare('1.0.0','1.0')); // = var_dump(versionCompare('1.0.1','1.0')); // &gt; var_dump(versionCompare('1.0.1','1.0.2')); // &lt;","link":"/2014/12/20/version-compare-function.html"},{"title":"鼠标无法拖放的解决办法","text":"话说fiddler有个强大的功能，就是把Session请求拖到Composer里，很方便地构造请求。我偶然发现不能拖放了。 刚开始以为fiddler的bug，于是找了N多新旧版本，试了还是不行。后来发现不但fiddler不能拖放，连文件都不能拖放了。那就是系统的配置问题了。 最终的解决办法是： 按两下esc键 奇迹般地好了！原因不明，如果网友刚好也遇到这样的问题，不妨一试。","link":"/2015/01/29/fiddler-can-not-drag-session-to-composer.html"},{"title":"HTTP 416错误与断点续传","text":"看房团 App 1.8.2更新版本时，偶然发现无法从1.8.1升级到1.8.2，于是探究一番，发现大有深意。 ##问题升级，无非是下载新版软件包，再进行安装。无法更新，最直接的反应是下载链接是不是404了？ 那抓下包吧。 出人意料的416。 把地址放到浏览器中，能成功下载；传到手机上，也能安装。说明不是下载链接的问题。 先看下HTTP 416错误代表什么吧？ 所请求的范围无法满足 (Requested Range not satisfiable) 看了不明觉厉，因为从没遇见过。 ##探索问了下客户端的同学，发现下载使用的是HttpURLConnection，于是Google一下，得到一些关键信息： HTTP response code: 416是由于读取文件时设置的Range有误造成的，具体的说就是下面这行代码有误： httpConnection.setRequestProperty(&quot;RANGE&quot;, &quot;bytes=1024-&quot;); 这个RANGE显然不能超出文件的size 而客户端设置的RANGE为文件大小。 试想，文件存在远程服务器上，如何知道文件大小？ 至少要发起两次请求。第一次请求，不需要下载整个文件，只需要获得Response的Content-Length大小；第二次请求，将Content-Length值写进RANGE，实现下载。 看第一次请求时，发现如下： 这是罪魁祸首！ 为了验证，将Range去掉，再构造请求，一切OK。 ##结论造成返回码416的原因，是设置的Range有误。解决办法也很简单，将第一次请求时的Range去掉。 //删掉之后，整个世界都清净了！ conn.setRequestProperty(&quot;Range&quot;, &quot;bytes=&quot; + startPosition);// startPosition=0 ##启示解决bug并不难，难的是版本已发，鞭长莫及。这时我想到的解决办法是：用其它链接替换，或服务器做一个中转。但前者需要更换CDN提供商，后者需要接入层服务器的支持，都没有着眼于问题的本质。 问题的本质是，无论CDN服务器支持与否，客户端请求的方式都存在不足之处，在下一个版本中修正即可。而且从报表上看，用户自发地通过其它途径升级到了最新版本！ 程序员感叹：不要试图用代码解决所有问题。 讨论下载地址是CDN地址，莫非CDN不支持断点续传？ 恰好相反，416正是支持断点续传的标志。服务器得到一个Range之后，需要对它的取值进行检验，包括： 开始位置非负 结束位置需要大于开始位置 开始位置需要小于文件长度减一 (因为这里的位置索引是从0开始的) 若结束位置大于文件长度减一，则需要把它的值设置为文件长度减一 如果Range的取值不合法，则需要终止程序并告知浏览器： header('HTTP/1.1 416 Requested Range Not Satisfiable'); 在本文的例子中，CDN服务器尽职尽责地告诉客户端，你的请求错了！ CDN服务器没有错，但可以采用稍微柔性的处理方法。Range为0，可以啊，就从头开始下载吧。 七牛云存储就是这么干的。 PS：别问我最后一句是怎么来的，七牛给过我一顶太阳帽^_^。 参考网址： http://www.checkupdown.com/status/E416_cn.html http://blog.csdn.net/zollty/article/details/9176829 http://www.pureweber.com/article/resumable-download/","link":"/2015/01/30/http-code-416-and-download.html"},{"title":"PHP扩展开发Demo","text":"为了深入学习PHP，就从一个简单的PHP扩展开始，了解一下制作过程。 准备工作在制作PHP扩展之前，需要两个条件： 1.正常安装的PHP环境 2.PHP源码 这里我的PHP安装在/usr/local/php5.3.10目录，源码在/data/webdev/php-5.3.10 生成扩展基本框架运行以下命令： cd /data/webdev/php-5.3.10/ext ./ext_skel helloyo vi helloyo/config.m4 编辑文件， dnl PHP_ARG_ENABLE(helloyo, whether to enable helloyo support, dnl Make sure that the comment is aligned: dnl [ --enable-helloyo Enable helloyo support]) 其中dnl为注释，删掉dnl及第二行，最后的效果如下： PHP_ARG_ENABLE(helloyo, whether to enable helloyo support, [ --enable-helloyo Enable helloyo support]) 保存退出。 编写C代码进入helloyo目录，用ls命令看下文件列表： /data/webdev/php-5.3.10/ext/helloyo # ls config.m4 config.w32 CREDITS EXPERIMENTAL helloyo.c helloyo.php php_helloyo.h .svnignore tests 编辑php_helloyo.h，加入say_hello函数入口 vi php_helloyo.h 编辑helloyo.c，需要修改两处内容 vi helloyo.c 1.为保证say_hello外部可以使用，需要加入在zend_function_entry helloyo_functions数组里加入一行： PHP_FE(say_hello, NULL) /* say hello to someone. */ 2.需要实现say_hello函数。在文件末尾加入以下代码： PHP_FUNCTION(say_hello) { char *arg = NULL; int arg_len, len; char *strg; if (zend_parse_parameters(ZEND_NUM_ARGS() TSRMLS_CC, &quot;s&quot;, &amp;arg, &amp;arg_len) == FAILURE) { return; } len = spprintf(&amp;strg, 0, &quot;Hello,%s!\\n&quot;, arg); RETURN_STRINGL(strg, len, 0); } 保存退出。 编译安装1.在helloyo目录下运行phpize命令。phpize命令用来为php扩展的构建环境作准备工作。php官方文档中的说明如下： The phpize command is used to prepare the build environment for a PHP extension. /data/webdev/php-5.3.10/ext/helloyo # /usr/local/php5.3.10/bin/phpize Configuring for: PHP Api Version: 20090626 Zend Module Api No: 20090626 Zend Extension Api No: 220090626 完成后，目录下会生成configure文件： 2.然后运行如下命令 ./configure --with-php-config=/usr/local/php5.3.10/bin/php-config make make test make install 这样就会生成扩展helloyo.so了。其中helloyo.so的生成目录如下： /data/webdev/php-5.3.10/ext/helloyo # make install Installing shared extensions: /usr/local/php5.3.10/lib/php/extensions/no-debug-non-zts-20090626/ 如果不知道扩展目录，可以用phpinfo命令查看extension_dir路径，如下图所示： 3.编辑php.ini，将扩展加入进去 只要找到php.ini，在文件中加入一行配置即可： extension=helloyo.so 问题时，在某种情况下，目录中有多个php.ini存在，如下 这三个php.ini哪个才是真正的配置文件呢？挨个试~~~~ Good Idea！但是，可以用phpinfo来看，一目了然。 运行以下命令，编辑php.ini： /usr/local/php5.3.10 # vi ./etc/php.ini 最终效果如下： 验证与运行1.模块是否加载 /usr/local/php5.3.10 # ./bin/php –m 结果如下： 可以看到helloyo已经被加载了。 2.cli命令行命令如下： /usr/local/php5.3.10 # ./bin/php -r &quot;echo say_hello('davidyan');&quot; Hello,davidyan! 效果如下： 3.cgi网页在网页内查看，需要重启apache。在phpinfo里可以看到模块已加载了。 再写一小段程序： public function ActionSayHello(){ $name = $_GET['name']; echo say_hello($name); } 访问sayhello?name=davidyan，结果如下： 小结通过step by step的操作，了解了php扩展的开发方法。由易而难，以后遇到扩展需要开发时，就知道从何下手了。","link":"/2014/10/10/php-extension-demo.html"},{"title":"Mysql Tips -- 显示执行SQL耗时，精确到毫秒","text":"MySQL执行一个SQL语句时，执行时间精确到秒。如下： mysql&gt; select * from test +----+-------+ | id | name | +----+-------+ | 1 | david | +----+-------+ 1 row in set (0.00 sec) 如何精确到毫秒呢？MySQL有个内置语句(show profile)可以查看执行耗时。 mysql&gt; set profiling=1; Query OK, 0 rows affected (0.00 sec) 然后执行查询语句。使用show profiles就可以看到执行时间了。 mysql&gt; show profiles; +----------+------------+--------------------+ | Query_ID | Duration | Query | +----------+------------+--------------------+ | 1 | 0.00034500 | select * from test | +----------+------------+--------------------+ 1 row in set (0.04 sec) Duration就是该SQL的执行时间，将其乘以1000就是毫秒数了。","link":"/2015/02/27/mysql-show-profiles-get-execute-time.html"},{"title":"强大的弱关系","text":"今天看 TED Why 30 is not the new 20 时，看到三条很有意义的话： Get some identity capital Use your weak ties Pick your family 对于第二条，演讲者Meg Jay说到，很多人靠私人联系找到工作。奇怪的是，这些私人联系更多的是熟人，而非亲近的朋友。 搜了下“weak ties”，发现美国康乃尔大学有这样的研究，并有本书（Networks, Crowds, and Markets: Reasoning About a Highly Connected World）。其中第三章讲了Weak Ties（Chapter 3 Strong and Weak Ties），看懂一些，分享出来跟大家讨论一下。 强弱关系社交网络中，每个人都是网络的节点，人与人之间的关系构成网络的边。在下图中，A认识B和C，A和B、A和C之间为强关系。B和C很可能会认识，形成弱关系。形象地说，强弱关系的区别如同朋友/熟人。 图1 三元闭合在图1中，“B和C很可能会认识”。直觉上，人们会把朋友介绍给另一个朋友。其实可以利用概率的知识简单说明。如下图，黑线代表每天的24小时。一天之内，A和B有接触时间（红色），A和C也有接触时间（黄色）。如果接触时间重合，就代表B和C遇见了，就有可能认识。 图2 这种现象就是三元闭合（Triadic Closure）。 图3 桥常常有这种情况：你认识了一个人，然后他将你带入一个新的朋友圈。这个人就是“桥”。在下图中，B是A的桥。 图4 弱关系的意义好机会总是稀少的。在图4中，如果A想获得新信息，那么机会很可能来自于由B“桥”连接的未知朋友圈。尽管A的亲密朋友很乐意帮忙，但可能他们知道的信息和A本人差不多。 启示 利用弱关系 建立自己的弱关系网络 倾听他们的声音 帮助他们解决问题 请求他们的帮助 结识新朋友，不忘老朋友 壮大自己，争取做“桥”","link":"/2015/01/14/strong-weak-ties.html"},{"title":"cgi与fastcgi的区别","text":"青石街有两个卖烧饼的，王大与李二。 顾客很少。 王大很闲。每次有顾客，王大就生火、和面、擀饼、下炉、出锅。不出一会儿，热腾腾的烧饼就送到顾客面前。顾客走后，王大就收拾炉子，熄火休息。 李二却很忙。每天早晨，李二就生火、和面，等着顾客上门。白天里，顾客来，李二就擀饼、下炉、出锅，然后等下一位顾客。直到夜幕降临，李二才打烊。 后来青石街城镇化 ，人多起来，买烧饼的人也多了。 王大很忙。每个顾客来，都要生火、和面、熄火 ，不胜其烦；顾客的等待时间在延长，不满情绪在酝酿。 李二也很忙，不过还是和以前一样，不太累，顾客也满意。 故事很傻，勿喷啊，讲的是cgi与fastcgi的区别。 cgi像王大。每次请求，web服务器都会根据请求的内容，fork一个新进程来运行外部程序或解释器， 这个进程会把处理完的数据返回给web服务器，最后web服务器把内容发送给用户，刚才fork的进程也随之退出。 如果下次用户还请求该动态脚本，那么web服务器又再次fork一个新进程，周而复始的进行。 fastcgi像李二。web服务器启动时，就开启一个进程来等待请求。收到一个请求时，不会重新fork一个进程（因为这个进程在web服务器启动时就开启了，而且不会退出），web服务器直接把内容传递给这个进程（进程间通信，但fastcgi使用了别的方式，tcp方式通信），这个进程收到请求后进行处理，把结果返回给web服务器，最后自己接着等待下一个请求的到来，而不是退出。 总结一下： 在web服务器方面 cgi：fork一个新的进程进行处理 fastcgi：用tcp方式跟远程机子上的进程或本地进程建立连接 在对数据进行处理的进程方面 cgi：读取参数，处理数据，然后就结束生命期 fastcgi： 要开启tcp端口，进入循环，等待数据的到来，处理数据 参考资料：http://www.cnblogs.com/jamesbd/p/3567682.html","link":"/2015/01/25/cgi-and-fastcgi.html"},{"title":"Mysql Tips -- 合理利用查询缓存优化查询效率","text":"最近开发会员中心项目，遇到多表查询的问题，发现响应极慢，就动手查下原因，并进行一些优化。先说下成果吧，由6-7秒降到200ms以下。吃公鸡下的蛋之前，走道是这样的： 图1 吃完了之后，那家伙，再看，就成了这样： 图2 降到还可以接受的范围了。 问题我在review代码和需求时，发现用户列表页访问很慢。该页面根据图3中的查询条件，筛选出符合条件的用户记录，每页显示15条记录，显示如图4所示。 图3 图4 看下数据表的规模： select count(*) from t_uc_user; +----------+ | count(*) | +----------+ | 2001935 | +----------+ 200万条记录。 定位问题听前辈说，解决查询问题的瓶颈，最重要的是查出瓶颈在哪。我深以为然，所以第一步就是把SQL打印出来。我发现的可疑点有： 1） 其中好几条SQL都是包含正则表达式的查询，耗时长达2秒以上。标记一下，稍后优化。 select count(*) as num from t_uc_search_101_user where (((FStatus='1'))) and ((((concat('|',FBusinessInfo, '|') regexp '\\\\|1_[[:alnum:]]*_[[:alnum:]]*_[[:alnum:]]*_[[:alnum:]]*\\\\|')))) and ((((concat('|',FAttributeInfo, '|') regexp '\\\\|1_-1_[[:alnum:]]*\\\\|')))); +-----+ | num | +-----+ | 3 | +-----+ 1 row in set (2.18 sec) 2） 有至少三条count(*)查询，看下存储引擎，如果是Innodb，可以考虑用MYISAM。结果是MYISAM，此路不通。 3） 重复执行相同的SQL语句。这些SQL语句的功能是根据查询到的用户记录ID，去取用户的详细信息。以每页15条来看，可以减少15次查询。 select * from t_uc_records_101 where ( (FId='5878970') OR (FId='5878969') OR (FId='5880317') ) AND FStatus!=-1 select * from t_uc_records_101 where ( (FId='5878970') OR (FId='5878969') OR (FId='5880317') ) AND FStatus!=-1 select * from t_uc_records_101 where ( (FId='5877911') ) AND FStatus!=-1 select * from t_uc_records_101 where ( (FId='5877911') ) AND FStatus!=-1 4） 最大的问题是，每次以相同的查询条件加载，耗时好像没有减少。难道Mysql没有查询缓存吗？ 解决问题合理利用缓存技术，能提高网页访问速度。即便最终没能解决SQL语句的优化，也能在第二次加载时提高速度，带来良好体验。 1） 查询缓存是否开启？ mysql&gt; select @@query_cache_type; +--------------------+ | @@query_cache_type | +--------------------+ | ON | +--------------------+ 1 row in set (0.01 sec) 已开启。 2） 查询缓存是否可用？ mysql&gt; show variables like 'have_query_cache'; +------------------+-------+ | Variable_name | Value | +------------------+-------+ | have_query_cache | YES | +------------------+-------+ 1 row in set (0.01 sec) 可用。 3） 查询缓存大小？ mysql&gt; select @@global.query_cache_size; +---------------------------+ | @@global.query_cache_size | +---------------------------+ | 0 | +---------------------------+ 1 row in set (0.00 sec) 分配给查询缓存内存为0，就是没分配。 没有任何效果。 4） 设置查询缓存大小。大约10M，不够以后再加。 mysql&gt; set @@global.query_cache_size=10000000; Query OK, 0 rows affected, 1 warning (0.00 sec) 完成这些以后，发现第一次访问速度还是龟速。再刷新，就瞬间加载了。 #结论#利用查询缓存来优化查询，能将访问速度减少到200ms以下，能满足当前需求。但是最终的解决之道是对耗时较多、冗余的SQL语句进行优化。所以，革命尚未成功，同志仍需努力！","link":"/2015/03/08/use-mysql-query-cache.html"},{"title":"Office 输入URL时单词中间出现长空格的解决办法","text":"Office在输入英文长单词时，默认不会从中间断开换行，这是很好的功能。但输入URL时也不会换行，导致出现“长空格”，如下图所示： 这时，可以选中文字，进入“段落”=&gt;“中文版式”，将”允许西文在单词中间换行”项打勾即可。 最终结果是，“长空格”不见了，文字变得紧凑。","link":"/2015/03/17/office-url-not-break.html"},{"title":"Mysql Tips -- 检索每个分组的最后一条记录","text":"有这样一类问题： 检索论坛中某一版块所有主题的最新一条帖子 查找所有会话中最新一条消息 查找一类商品的最新报价 这类问题的共同点是：需要按某个字段分组，且每组只能取一条记录；按某个字段倒序。 举例来说，有这样一个表： mysql&gt; select * from t_tmp; +-----+---------+---------------------+ | FId | FCityId | FUpdateTime | +-----+---------+---------------------+ | 1 | 1 | 2014-10-10 00:00:00 | | 2 | 1 | 2014-10-11 00:00:00 | | 3 | 2 | 2014-10-10 10:00:00 | +-----+---------+---------------------+ 表1 希望从中找出每个FCityId的最新更新记录，即筛选出这样的结果： +-----+---------+---------------------+ | FId | FCityId | FUpdateTime | +-----+---------+---------------------+ | 2 | 1 | 2014-10-11 00:00:00 | | 3 | 2 | 2014-10-10 10:00:00 | +-----+---------+---------------------+ 表2 Group ByPo主最开始的方法是用group by（So Easy！） select * from t_tmp group by FCityId order by FUpdateTime desc;` 结果却是错误的！ +-----+---------+---------------------+ | FId | FCityId | FUpdateTime | +-----+---------+---------------------+ | 3 | 2 | 2014-10-10 10:00:00 | | 1 | 1 | 2014-10-10 00:00:00 | +-----+---------+---------------------+ 表3 其原因是：Group By 要先于 Order By执行。Group By执行分组之后，记录中只剩下1和3了： mysql&gt; SELECT * FROM t_tmp GROUP BY FCityId LIMIT 0 , 30; +-----+---------+---------------------+ | FId | FCityId | FUpdateTime | +-----+---------+---------------------+ | 1 | 1 | 2014-10-10 00:00:00 | | 3 | 2 | 2014-10-10 10:00:00 | +-----+---------+---------------------+ 表4 然后执行Order By，就变成了图3的结果。 TIPS：执行顺序 Where &gt; Group By &gt; Order By SUB SELECT很自然的一个解决方法就是：既然Group By先于Order By改变结果，那么就在Group By之前纠正结果。方法是子查询。如表4，Group By从头到尾扫一遍，留下了第1和第3两条记录。如果从尾到头扫一遍，就留下3和2两条记录。然后执行Order By，就能得到期望的结果。 mysql&gt; SELECT * FROM (SELECT * FROM t_tmp ORDER BY FUpdateTime DESC)tmptable GROUP BY FCityId ORDER BY FUpdateTime DESC; +-----+---------+---------------------+ | FId | FCityId | FUpdateTime | +-----+---------+---------------------+ | 2 | 1 | 2014-10-11 00:00:00 | | 3 | 2 | 2014-10-10 10:00:00 | +-----+---------+---------------------+ 表5 但子查询执行效率不高，容易形成慢SQL。 LEFT JOIN再分析整个问题，其中蕴含着组内FId从大到小排列的条件，这种表内字段的自我比较，可以用JOIN命令来做。 如图： 我们可以用LEFT JOIN去掉部分记录（即FId不按从大到小排列的记录）： mysql&gt; SELECT m1. * FROM t_tmp m1 LEFT JOIN t_tmp m2 ON ( m1.FCityId = m2.FCity Id AND m1.FId &lt; m2.FId ) WHERE m2.FId IS NULL ORDER BY FUpdateTime DESC LIMIT 0 , 30; +-----+---------+---------------------+ | FId | FCityId | FUpdateTime | +-----+---------+---------------------+ | 2 | 1 | 2014-10-11 00:00:00 | | 3 | 2 | 2014-10-10 10:00:00 | +-----+---------+---------------------+ 表6 通用方法经过以上的讨论，可以形成此类问题的较为通用的方法。 直观但低效的方法： select * from (select * from messages ORDER BY id DESC) AS x GROUP BY name 难懂但高效的方法： SELECT m1.* FROM messages m1 LEFT JOIN messages m2 ON (m1.name = m2.name AND m1.id &lt; m2.id) WHERE m2.id IS NULL;","link":"/2015/04/03/retrieve-the-last-record-in-each-group.html"},{"title":"Redis Tips -- Use Sorted Set To Storage A Dynamically Changing List","text":"When I was developing an instant messenger (IM) system, I found that high concurrency is an essential feature while numerous clients are online chatting simultaneously. Therefore I use Redis to cache data in order to release pressure on database. A typical scene of IM is chatting with someone else, as shown in the following figure. This conversion contains several messages , which are stored in database in ascending order of their timing. As is shown in the following table. +----+------------------+--------+---------------------+ | id | content | isread | pubtime | +----+------------------+--------+---------------------+ | 1 | hello | 1 | 2014-05-22 11:53:00 | | 2 | Hi! | 1 | 2014-05-22 11:54:00 | | 3 | Nice to meet you | 1 | 2014-05-22 11:55:00 | | 4 | Me too | 0 | 2014-05-22 11:56:00 | +----+------------------+--------+---------------------+ The “isread” field represents whether the message is read by someone. Store Data In A Key ?#The following figure shows how we use Redis as cache container. When we need to retrieve some data , the first step is to check whether the data is in the cache container. If cache is hit, We get the data and return; If cache is missing , we query the data from database and store it in cache so that we can get it faster next time. We can store a conversion in a key, so we can get all the containing messages from cache in one attempt. But this method has several disadvantages: When a new message arrives, the cache has to be refreshed to add latest message. When a message is set to Already Read status, the cache has to be destroyed to keep accuracy. When a message is deleted by its owner, the cache has to be refreshed again to remove that item. The main reason for these disadvantages is that a conversion is a dynamically changing list. Each change will cause the cache out of date. #Store List In Sorted Set# What Sorted Set is ? I quote the following content from Redis official website (See this): Redis Sorted Sets are, similarly to Redis Sets, non repeating collections of Strings. The difference is that every member of a Sorted Set is associated with score, that is used in order to take the sorted set ordered, from the smallest to the greatest score. While members are unique, scores may be repeated. We can store Ids of each message as index, and store each message separately, as shown in the following figure. We deal with each case as follows: Send Message and Receive Message. Add Id of message into the cache. Delete Message. Remove Id of message from the cache. Set Message Read. Leave the cache unchanged and Edit the corresponding message’s cache. #Code Sample# Some Useful functions: //Add one or more members to a sorted set, or update its score if it already exists function zAdd(key, score, member) //Return a range of members in a sorted set, by index function zRange(key, start, stop) //Remove one or more members from a sorted set function zRem(key, member) //Set multiple keys to multiple values function mSet(key,value, ...) //Get the values of all the given keys function mGet(key, ...) Store message Id as index, and its “pubtime” as score. pubtimestamp = strtotime(pubtime); zAdd(key,pubtimestamp,id); Store messages with mSet Command: mSet(key,value, ...); Retrieve messages like this: indexes = zRange(key, start, stop) msgs = mGet(indexes)","link":"/2015/04/09/use-sorted-set-to-storage-dynamically-changing-list.html"},{"title":"深夜中消失的广告位","text":"旧文重发。这篇博客记录了Po主在腾讯微博时的解决的一个Bug，不是大问题，但追查的过程很有意思。也以此纪念一下。 诡秘的空白广告位一碗水，放置一晚上，会发生什么呢？ 会蒸发，会流失，可能会被小猫喝掉，或被不注意的人踢翻。 把一个页面打开，放置一晚上，会出现什么变化呢？ 拿腾讯微博首页来说，每隔30秒去查看计数，隔1分钟去拉取新消息，隔3分钟去看每日任务完成了没。 这些都是正常的请求，但为什么好好的广告会消失了呢。 初始状态，正常显示广告。 放置一个晚上后，广告消失，剩下空白！ 找出真凶广告位原先存在，后来消失，一定是有什么改动了dom结构。 经过观察，发现广告位消失的重现规则是这样的： 页面加载时，正常 白天放置一段时间，正常 放置一个晚上，异常。 由1和2知道，可能存在某种超时或定时操作，改变了dom结构；由3知道，这个时机发生在晚上。 于是我设想，在一个整晚，页面肯定通过超时或定时请求了某个js文件，而这个js文件会更改dom结构。 抓包只知道发生在晚上，但具体什么时候不清楚，所以我尝试用fiddler抓包。 下班前，我打开腾讯微博首页和fiddler，看着一行行记录，我知道，第二天肯定会有些异常出现。 第二天，打开电脑，先查看腾讯微博首页。果然，广告位空白了。 我把fiddler的url记录导出成一个txt文件。有几千条记录，于是写了个小程序，把url分下类。 &lt;?php $handle = @fopen(&quot;path/to/urls.txt&quot;, &quot;r&quot;); $urls = array(); if ($handle) { while (!feof($handle)) { $buffer = fgets($handle, 4096); $url = getUrl($buffer); if(in_array($url,$urls) || empty($url)){ continue; }else{ array_push($urls,$url); } } fclose($handle); } function getUrl($fullurl){ $array = parse_url($fullurl); if(isset($array['host']) &amp;&amp; isset($array['path'])){ return $array['host'].$array['path']; }else{ return ''; } } foreach($urls as $url){ echo $url.&quot;\\n&quot;; } ?&gt; 结果如下： ---------- PHP ---------- pyhelp.qq.com/cgi-bin/getloc.so__ message.t.qq.com/newMsgCount.php t.web2.qq.com/channel/poll api.t.qq.com/side/get.json clients2.google.com/service/update2/crx www.gstatic.com/chrome/crlset/1444/crl-set-10054177008346624430.crx.data__ qos.report.qq.com/collect www.google.com/dl/chrome/components/recovery/recovery/win/101.3.21.140/install.crx__ **api.t.qq.com/asyn/mysidebar_n_new.php** www.gstatic.com/chrome/crlset/1445/crl-set-14121586793162109453.crx.data__ Output completed (0 sec consumed) - Normal Termination *号标记的url恰好是控制广告出现的文件。看了下请求时间，00：01。 这个结果，印证了以前的判断。 原因分析mysidebar_n_new.php是一个JSONP，获取数据后将值传递给回调函数mySidebar，该函数负责将数据中的dom结构加载到相应的位置。 广告位是app_ad和app_ad3_1。这里广告的逻辑是这样的，腾讯微博这边只输出dom结构，然后引用广告平台的一段js来填入内容。 而这里的app_ad和app_ad3_1，不管页面逻辑如下，强行将dom结构替换，替换的同时没有去请求广告平台的js，导致有位置没内容，就出现了空白。 解决方案确定了原因是晚上0点加载了mysidebar_n_new.php但没有进行相应处理，那么可能的解决方案有以下几种： 如果非必要，可以不加载吗？ 不可。跟开发该功能的同事确认，是为了解决特定问题而加载的。 能否不进行ajax方式更新，而改为reload？ 不可。如果用户0点正在看，强制刷新会影响用户体验 更改mysidebar_n_new.php 因为页面加载时也会请求这个文件，要改的话，需要判断加载的时间。不是一个优雅的解决方式。 js在加载mysidebar_n_new.php时进行相应处理 这种方式较好，解铃还需系铃人。 总结这件事情很简单，就是晚上0点加载了一个文件，但没有进行相应处理，引起了广告位空白。这使我想起一段话： “蛮多复杂的事情呢，他又蛮简单，就象你在新兵连学的立正、稍息，那是最标准的。” “但是蛮多简单的事情后面呢，又蛮复杂，就象我刚才跟你一说，你连立正都找不到怎么回事了。” 这段话来自《士兵突击》团长王庆瑞。","link":"/2015/04/10/ad-position-disappear-in-midnight.html"},{"title":"利用中文分词正向最大匹配法解决自动提取标签问题","text":"写文章时，会有自动提取标签的需求；写新闻时，会有查找主题或关键字的需求。如下图，就是分析新闻页面的内容，匹配相关车型。 图1 解决的方法很多，最常见的是基于字典的中文分词。 为了解决自动提取标签的问题，我们需要以下几个步骤： 1.制作字典。字典保存着奥迪、奔驰、宝马等车型信息 2.分析新闻的正文，查看是否有字典中的词。如果有，就提取出来。 中文分词也有很多种方法，基于词典的算法有最大匹配、最小匹配、逐词匹配和最佳匹配，高大上的NLP算法有隐马尔科夫。这里采用最大匹配法。 最大匹配法，顾名思义，就是先匹配最长的词，再匹配较短的词。例如图1，假如字典中有“奥迪Q7”和“奥迪”两个词，最大匹配法会优先匹配“奥迪Q7”，而不会先匹配“奥迪”。 全流程如下图所示： 图2 代码这里定义MaxWordSegmentation类，其中的run方法返回匹配到的结果。 class MaxWordSegmentation{ private $dict = array();//保存字典的list function __construct($pathToDict){ $this-&gt;dict = $this-&gt;loadDict($pathToDict); } //读入词典文件到内存 function loadDict($pathToDict){ if(!file_exists($pathToDict)){ die('Can not find dict file!'); } $dicts = array(); $handle = @fopen($pathToDict, &quot;r&quot;); if ($handle) { while (!feof($handle)) { $word = fgets($handle, 4096); //这里的词需要用trim去掉换行符 $word = trim($word); $dicts[$word] = 1; } fclose($handle); } return $dicts; } //查看词是否在字典中 function inDict($word){ return array_key_exists($word,$this-&gt;dict); } //按照词典进行分词。正向最大匹配法 function run($text,$encode = 'utf-8'){ $minLen = 0; $maxLen = 0; //找出最长的单词长度及最短的单词长度 foreach($this-&gt;dict as $key=&gt;$value){ $iLen = mb_strlen($key,$encode); if($minLen &gt; $iLen || $minLen == 0 ){ $minLen = $iLen; } if($maxLen &lt; $iLen){ $maxLen = $iLen; } } $sLen = mb_strlen($text, $encode); $result = array(); for($start = 0;$start &lt; $sLen;$start ++){//外层正文循环 for($maxLoop = $maxLen;$maxLoop &gt;= $minLen;$maxLoop --){//内层字典循环 $word = mb_substr ($text , $start, $maxLoop , $encode); //是否匹配成功 if($this-&gt;inDict($word,$this-&gt;dict)){//字典查找 //添加到输出列表 if(!in_array($word,$result)){ $result[] = $word; } break; } } } return $result; } } 使用示例： $text = '日前，奥迪全新SQ7运动版车型的无伪装谍照出人意料的被曝光出来，该车预计今年底或者明年初发布。可以看到，新车已经有一些专属设计明显的区别于普通版的车型。'; $file = './dict.txt'; $obj = new MaxWordSegmentation($file); $ret = $obj-&gt;run($text); var_dump($ret); /* 结果如下： array(1) { [0]=&gt; string(15) &quot;奥迪全新SQ7&quot; } */ 改进之处由于提取标签时，标签通常不会只有一个字，比如“奥迪”、“奔驰”、“宝马”等，子串长度都为2。如果字典中最短的词长为S，那么对对剩下的字符串重新进行匹配处理，不必等到剩余字串的长度为零才结束，只要匹配到S个字符就行。 算法分析假设字典中最短的词长为S，最长的词长为L，正文的长度为N。 外层正文循环：N 内层字典循环：L-S 内层的inDict字典查找最关键，这里采用数组模拟hashtable实现了个O(1)的实现。 所以总的循环次数是(L-S)*N，而(L-S)是个常数，所以时间复杂度是O(N) 代码下载点此下载","link":"/2015/05/05/use-max-word-segmentation-to-extract-tags.html"},{"title":"Restful接口设计：如何做版本兼容","text":"现在App大多采用Restful接口，与服务器完成资源交换。这种实现方式隐藏了服务器端的具体实现，做到了对App端的透明服务。服务器端如何变更，甚至换一种语言实现，只要对外接口不变，App依然不受影响。 App的升级过程中，版本兼容是一个值得关注的问题，因为App要升级，服务器端的Restful接口也要进化，于是版本兼容要同时兼顾App和服务器端。 一个例子版本兼容是向下兼容。接口升级后要保证低版本App能正常工作。 例如“App 1.0”使用了下面接口完成修改用户信息功能。 GET api.demo.com/modify_userinfo 采用GET方式提交用户输入。后来发现该接口可被CSRF攻击。 这时要做两件事： 第一，服务器接口要为新的“App 1.1”提供升级，需要将GET方式改为POST，并且增加Token验证。这是CSRF的一般防范方法，具体请参看 《CSRF简单介绍及利用方法》 第二，服务器接口要为旧的“App 1.0”版本修复漏洞。但这时App已发布，不可能修改了。这时只能服务器端来改。例如限制user-agent为”App 1.0”的请求才有效，用户通过浏览器提交的请求无效。这样在一定程度上减少了CSRF的危害。 版本兼容的两种情况从上面的例子来看，版本兼容可分为两种： 大版本，指版本升级，接口的输入输出可能发生变化。 小版本，指版本修复，只针对某个版本修改接口，其它接口保持不变。 这两种情况都很常见。App未发布时，服务器接口从1.0升级到2.0，这时App开发者可以配合一下，使用2.0的接口；但对于已发布的App版本，只能由服务器端针对特定的App版本来修复漏洞。 如何做App开发者需要配合，在请求接口时传入以下两项信息： App版本标识，一般包括App名称、版本、平台（Android、IOS等）、SDK版本等。一般放在user-agent里，例如微信的user-agent如下： Mozilla/5.0 (iPhone; CPU iPhone OS 5_1 like Mac OS X) AppleWebKit/534.46 (KHTML, like Gecko) Mobile/9B176 MicroMessenger/6.2.2 接口版本标识，指采用接口的版本，例如可以在Request Header里加入majarversion=2.0，表明使用的是2.0的接口。 服务器端开发者需要根据App版本和接口版本标识，来提供向下兼容","link":"/2015/06/01/how-to-deal-with-version-compatibility.html"},{"title":"日本东京之旅","text":"出发的那天，北京天气不错 首都国际机场 吃了碗虾仁面，35元 日航的小飞机 看建筑都成方块了 飞在云彩上面 东京海关 坐火车去上野 车上人很少 初到上野 街道 大和寿司排队中 等了半个小时，终于可以开动了 好吃的寿司 擦栏杆的工人，很用心的样子 RKK LINE 一个公园 优衣库1 优衣库2 优衣库3 门们 大桥 大桥夜景 自由女神 银座 好看的杯子 美食－扇贝 美食－寿司，生鱼片 美食－寿司 美食汇 干一杯 开吃前先拍照 早稻田大学－大门太小 早稻田大学－校园 近世禅画海报 手写海报 博物馆？ 图书馆 教堂 为什么不是红十字？ 草坪 前面就是日本天皇的宫殿 铺石子的广场 宫殿不让进 明治神宫 公共电动车 武道馆 时钟 东京大学 挖掘机 上野动物园 秋叶原 手办","link":"/2015/06/20/japan-tokyo-journey.html"},{"title":"Nginx Sticky的使用及踩过的坑","text":"#什么是Sticky？#为了理解Sticky的工作原理，我们可以先考虑一个问题：负载均衡怎么做？ DNS解析，在域名解析时分配给不同的服务器IP； IP Hash，根据客户端的IP，将请求分配到不同的服务器上； cookie，服务器给客户端下发一个cookie，具有特定cookie的请求会分配给它的发行者。 Sticky就是基于cookie的一种负载均衡解决方案，通过cookie实现客户端与后端服务器的会话保持, 在一定条件下可以保证同一个客户端访问的都是同一个后端服务器。请求来了，服务器发个cookie，并说：下次来带上，直接来找我。 为方便叙述，文中的cookie都指sticky使用的cookie。 #Sticky工作原理Sticky是nginx的一个模块,通过分发和识别cookie，来使同一个客户端的请求落在同一台服务器上。sticky的处理过程如下（假设cookie名称为route）： 1.客户端首次发起请求，请求头未带route的cookie。nginx接收请求，发现请求头没有route，则以轮询方式将请求分配给后端服务器。 2.后端服务器处理完请求，将响应头和内容返回给nginx。 3.nginx生成route的cookie，返回给客户端。route的值与后端服务器对应，可能是明文，也可能是md5、sha1等Hash值。 4.客户端接收请求，并创建route的cookie。 5.客户端再次发送请求时，带上route。 6.nginx接收到route，直接转给对应的后端服务器。 关于sticky的详细的配置过程在这里。 #参数解析 这里引用淘宝Tengine的文档： 语法：session_sticky [cookie=name] [domain=your_domain] [path=your_path] [maxage=time] [mode=insert|rewrite|prefix] [option=indirect] [maxidle=time] [maxlife=time] [fallback=on|off] [hash=plain|md5] 默认值：session_sticky cookie=route mode=insert fallback=on 上下文：upstream 说明: 本指令可以打开会话保持的功能，下面是具体的参数： cookie设置用来记录会话的cookie名称 domain设置cookie作用的域名，默认不设置 path设置cookie作用的URL路径，默认不设置 maxage设置cookie的生存期，默认不设置，即为session cookie，浏览器关闭即失效 mode设置cookie的模式: &nbsp;&nbsp;&nbsp;&nbsp;insert: 在回复中本模块通过Set-Cookie头直接插入相应名称的cookie。 &nbsp;&nbsp;&nbsp;&nbsp;prefix: 不会生成新的cookie，但会在响应的cookie值前面加上特定的前缀，当浏览器带着这个有特定标识的cookie再次请求时，模块在传给后端服务前先删除加入的前缀，后端服务拿到的还是原来的cookie值，这些动作对后端透明。如：”Cookie: NAME=SRV~VALUE”。 &nbsp;&nbsp;&nbsp;&nbsp;rewrite: 使用服务端标识覆盖后端设置的用于session sticky的cookie。如果后端服务在响应头中没有设置该cookie，则认为该请求不需要进行session sticky，使用这种模式，后端服务可以控制哪些请求需要sesstion sticky，哪些请求不需要。 option 设置用于session sticky的cookie的选项，可设置成indirect或direct。indirect不会将session sticky的cookie传送给后端服务，该cookie对后端应用完全透明。direct则与indirect相反。 maxidle设置session cookie的最长空闲的超时时间 maxlife设置session cookie的最长生存期 fallback设置是否重试其他机器，当sticky的后端机器挂了以后，是否需要尝试其他机器 hash 设置cookie中server标识是用明文还是使用md5值，默认使用md5 maxage是cookie的生存期。不设置时，浏览器或App关闭后就失效。下次启动时，又会随机分配后端服务器。所以如果希望该客户端的请求长期落在同一台后端服务器上，可以设置maxage。 hash不论是明文还是hash值，都有固定的数目。因为hash是server的标识，所以有多少个server，就有等同数量的hash值。 #一些例外# ##同一客户端的请求，有可能落在不同的后端服务器上##如果客户端启动时同时发起多个请求。由于这些请求都没带cookie，所以服务器会随机选择后端服务器，返回不同的cookie。当这些请求中的最后一个请求返回时，客户端的cookie才会稳定下来，值以最后返回的cookie为准。 ##cookie不一定生效##由于cookie最初由服务器端下发，如果客户端禁用cookie，则cookie不会生效。 ##客户端可能不带cookie##Android客户端发送请求时，一般不会带上所有的cookie，需要明确指定哪些cookie会带上。如果希望用sticky做负载均衡，请对Android开发说加上cookie。 #注意事项 cookie名称不要和业务使用的cookie重名。Sticky默认的cookie名称是route，可以改成任何值。但切记，不可以与业务中使用的cookie重名。 客户端发的第一个请求是不带cookie的。服务器下发的cookie，在客户端下一次请求时才能生效。","link":"/2015/08/01/nginx-sticky-problem.html"},{"title":"Redis技巧:分片技术和Hash Tag","text":"twitter的 twemproxy 是一个Redis的代理服务程序,能够实现key的分片。分片能使key均匀地分布到集群的机器上去，能保证数据的一致性，有着众多的优点。 但从Redis单实例切换到twemproxy集群时，还是有些需要注意的地方： 不支持的方法： KEYS,MIGRATE,SCAN等 支持但需特殊处理的方法： MSET,SINTERSTORE,SUNIONSTORE,ZINTERSTORE,ZUNIONSTORE等 全部请查看 Redis命令列表. 对于不支持的方法，在使用时需要寻找替代方案。本文主要解决一下需特殊处理的方法。 MSET单实例上的MSET是一个原子性(atomic)操作，所有给定 key 都会在同一时间内被设置，某些给定 key 被更新而另一些给定 key 没有改变的情况，不可能发生。 而集群上虽然也支持同时设置多个key，但不再是原子性操作。会存在某些给定 key 被更新而另外一些给定 key 没有改变的情况。其原因是需要设置的多个key可能分配到不同的机器上。 ##SINTERSTORE,SUNIONSTORE,ZINTERSTORE,ZUNIONSTORE 这四个命令属于同一类型。它们的共同之处是都需要对一组key进行运算或操作，但要求这些key都被分配到相同机器上。 这就是分片技术的矛盾之处： 即要求key尽可能地分散到不同机器，又要求某些相关联的key分配到相同机器。 Hash Tags解铃还需系铃人。解决方法还是从分片技术的原理上找。 分片，就是一个hash的过程：对key做md5，sha1等hash算法，根据hash值分配到不同的机器上。 为了实现将key分到相同机器，就需要相同的hash值，即相同的key（改变hash算法也行，但不简单）。 但key相同是不现实的，因为key都有不同的用途。例如user:user1:ids保存用户的tweets ID，user:user1:tweets保存tweet的具体内容，两个key不可能同名。 仔细观察user:user1:ids和user:user1:tweets，两个key其实有相同的地方，即user1。能不能拿这一部分去计算hash呢？ 这就是 Hash Tag 。允许用key的部分字符串来计算hash。 当一个key包含 {} 的时候，就不对整个key做hash，而仅对 {} 包括的字符串做hash。 假设hash算法为sha1。对user:{user1}:ids和user:{user1}:tweets，其hash值都等同于sha1(user1)。 Hash Tag 配置Hash Tag是用于hash的部分字符串开始和结束的标记，例如”{}”、”$$”等。配置时，只需更改hash_tag字段即可 beta: listen: 127.0.0.1:22122 hash: fnv1a_64 hash_tag: &quot;{}&quot; distribution: ketama auto_eject_hosts: false timeout: 400 redis: true servers: - 127.0.0.1:6380:1 server1 - 127.0.0.1:6381:1 server2 - 127.0.0.1:6382:1 server3 - 127.0.0.1:6383:1 server4","link":"/2015/09/20/redis-hash-tag.html"},{"title":"Swift知识点纪录","text":"po主初学Swift，遇到不少细节知识的缺失，只好请教Google大神。这篇文就是Google的结果，纪录一下。 ##获取UNIX时间戳即获得到某个时间点到1970年1月1日0点0分0秒的秒数。 返回字符串格式： var Timestamp: String { return &quot;\\(NSDate().timeIntervalSince1970)&quot; } 输出时，可以这样调用： println(&quot;Timestamp: \\(Timestamp)&quot;) 如果希望返回NSTimeInterval，则： var Timestamp: NSTimeInterval { return NSDate().timeIntervalSince1970 } ##创建alert创建Alert的代码。原文链接 func showAlert(){ var createAccountErrorAlert: UIAlertView = UIAlertView() createAccountErrorAlert.delegate = self createAccountErrorAlert.title = \"Oops\" createAccountErrorAlert.message = \"Could not create account!\" createAccountErrorAlert.addButtonWithTitle(\"Dismiss\") createAccountErrorAlert.addButtonWithTitle(\"Retry\") createAccountErrorAlert.show() } func alertView(View: UIAlertView!, clickedButtonAtIndex buttonIndex: Int){ switch buttonIndex{ case 1: NSLog(\"Retry\"); break; case 0: NSLog(\"Dismiss\"); break; default: NSLog(\"Default\"); break; //Some code here.. } }","link":"/2015/09/12/swift-records.html"},{"title":"Redis技巧:Sorted Set使用","text":"有序集合(Sorted Set)是Redis一个很重要的数据结构，它用来保存需要排序的数据。例如排行榜，一个班的语文成绩，一个公司的员工工资，一个论坛的帖子等。有序集合中，每个元素都带有score（权重），以此来对元素进行排序。它有三个元素：key、member和score。以语文成绩为例，key是考试名称（期中考试、期末考试等），member是学生名字，score是成绩。 有序集合有两大基本用途：排序和聚合，以下用几个例子分别说明。 排序假设老师需要处理期中考试的语文成绩，他做的第一件事是将学生成绩录入系统。 Li Lei成绩70分 127.0.0.1:6379&gt; ZADD mid_test 70 &quot;Li Lei&quot; (integer) 1 Han Meimei成绩70分 127.0.0.1:6379&gt; ZADD mid_test 70 &quot;Han Meimei&quot; (integer) 1 tom成绩99.5分 127.0.0.1:6379&gt; ZADD mid_test 99.5 &quot;Tom&quot; (integer) 1 排行榜有序集合天然就是做排行榜的利器。只需将带score的member塞到有序集合里，就可以正序或倒序取出数据。这要用到ZREVRANGE（倒序）和ZRANGE（正序）。 分数排行榜 127.0.0.1:6379&gt; ZREVRANGE mid_test 0 -1 WITHSCORES 1) &quot;Tom&quot; 2) &quot;99.5&quot; 3) &quot;Li Lei&quot; 4) &quot;70&quot; 5) &quot;Han Meimei&quot; 6) &quot;70&quot; 分段统计有序集合还支持按score区间来查询：ZREVRANGEBYSCORE为倒序查询，ZRANGEBYSCORE为正序。例如要知道90分以上的学霸： 127.0.0.1:6379&gt; ZREVRANGEBYSCORE mid_test 100 90 WITHSCORES 1) &quot;Tom&quot; 2) &quot;99.5&quot; 聚合有序集合，其本质是集合，当然会有交集（ZINTERSTORE）和并集（ZUNIONSTORE）运算。 交集ZINTERSTORE取所有集合的交集。以两个集合A和B为例，要取交集C，是这样的逻辑： A和B中共有的member，会加入到C中，其score等于A、B中score之和。 不同时在A和B的member，不会加到C中。 某班又进行了期末考试，同时来了个新同学Jerry。 127.0.0.1:6379&gt; ZADD fin_test 88 &quot;Li Lei&quot; (integer) 1 127.0.0.1:6379&gt; ZADD fin_test 75 &quot;Han Meimei&quot; (integer) 1 127.0.0.1:6379&gt; ZADD fin_test 99.5 &quot;Tom&quot; (integer) 1 127.0.0.1:6379&gt; ZADD fin_test 100 &quot;Jerry&quot; (integer) 1 老师要按期中考试和期末考试的总成绩来排座位，就对mid_test和fin_test做了个交集。 127.0.0.1:6379&gt; ZINTERSTORE sum_point 2 mid_test fin_test (integer) 3 127.0.0.1:6379&gt; ZREVRANGE sum_point 0 -1 WITHSCORES 1) &quot;Tom&quot; 2) &quot;199&quot; 3) &quot;Li Lei&quot; 4) &quot;158&quot; 5) &quot;Han Meimei&quot; 6) &quot;145&quot; 结果显示了学生的总成绩。但结果中没有新来的Jerry同学（尽管TA考了100分）。这是坑一。 并集ZUNIONSTORE计算所有集合的并集。以两个集合A和B为例，要取并集C，是这样的逻辑： A的所有member会加到C中，其score与A中相等 B的所有member会加到C中，其score与B中相等 A和B中共有的member，其score等于A、B中score之和。 假设某公司要核算工资总支出，先由各部门独自核算，再由财务统一处理。 程序员工资 127.0.0.1:6379&gt; zadd programmer 2000 peter (integer) 1 127.0.0.1:6379&gt; zadd programmer 3500 jack (integer) 1 127.0.0.1:6379&gt; zadd programmer 5000 tom (integer) 1 经理工资 127.0.0.1:6379&gt; zadd manager 2000 herry (integer) 1 127.0.0.1:6379&gt; zadd manager 3500 mary (integer) 1 127.0.0.1:6379&gt; zadd manager 4000 tom (integer) 1 财务统一处理。 127.0.0.1:6379&gt; zunionstore salary 2 programmer manager (integer) 5 127.0.0.1:6379&gt; zrange salary 0 -1 withscores 1) &quot;herry&quot; 2) &quot;2000&quot; 3) &quot;peter&quot; 4) &quot;2000&quot; 5) &quot;jack&quot; 6) &quot;3500&quot; 7) &quot;mary&quot; 8) &quot;3500&quot; 9) &quot;tom&quot; 10) &quot;9000&quot; 结果显示了总工资支出情况。 但结果中程序员tom和经理tom是两个人，但工资算在了一起。这是坑二。 避免踩坑还记得上面说的坑一和坑二吗？ 坑一： 当进行ZINTERSTORE操作时，如果进行聚合操作的源集合中元素不同，则聚合后的结果集仅为交集。如果发现聚合后少了一些元素，请查看源集合元素是否相同。 坑二： 当进行ZUNIONSTORE操作时，如果进行聚合操作的源集合中有相同元素，则聚合后的结果集中，相同元素的score等于源集合元素的score之和。如果发现聚合后某些元素的score异常，请查看源集合是否有相同元素。 我踩过的坑： 做用户的feed（timeline）时，需要将我关注的人和我自己发表的信息聚合起来。 应该用ZUNIONSTORE将所有信息聚合到一起。 后来有用户反馈说timeline排序错误，自己发表发布的信息永远在最上面。后来查明原因，由于早期的bug，自己竟然可以关注自己，导致关注人和自己重复聚合。踩到了坑二。 为什么踩坑以坑二为例，为什么有相同元素时，score就会变成原来元素的和？ 因为ZINTERSTORE和ZUNIONSTORE有个参数为AGGREGATE，表示结果集的聚合方式，可取SUM、MIN、MAX其中之一。默认值为SUM。 所以不指定聚合方式时，缺省值为SUM，即求和。 默认使用的参数 SUM ，可以将所有集合中某个成员的 score 值之 和 作为结果集中该成员的 score 值；使用参数 MIN ，可以将所有集合中某个成员的 最小 score 值作为结果集中该成员的 score 值；而参数 MAX 则是将所有集合中某个成员的 最大 score 值作为结果集中该成员的 score 值。 文档如上。 有序集合之总结使用场景：排行榜，有序列表，聚合； 算法复杂度： 增删：O(M*log(N))， N 为有序集的基数， M 为被成功操作（新增、移除）的成员的数量。 查询：O(log(N)+M)， N 为有序集的基数，而 M 为结果集的基数。 聚合：O(N)+O(M log(M))， N 为给定有序集基数的总和， M 为结果集的基数。 总数：O(1) 注意事项： ZINTERSTORE操作时,如果发现聚合后少了一些元素，请查看源集合元素是否相同。 ZUNIONSTORE操作时,如果发现聚合后某些元素的score异常，请查看源集合是否有相同元素。","link":"/2015/11/01/redis-zunionstore-tip.html"},{"title":"Redis技巧:phpredis扩展安装与升级","text":"为了使用zscan来处理有序集合（Sorted Set）按模式获取数据，需要将phpredis扩展从2.2.4升级到2.2.5以上（最新版本为2.2.7）。 安装和升级方法： 1.下载安装扩展 网址：https://pecl.php.net/package/redis wget https://pecl.php.net/get/redis-2.2.7.tgz tar zxvf redis-2.2.7.tgz cd redis-2.2.7 phpize ./configure make &amp;&amp; make install 2.检查： php -i | grep Redis Redis Support =&gt; enabled Redis Version =&gt; 2.2.7 安装或升级成功。 3.重启php5-fpm service php5-fpm restart","link":"/2016/01/29/install-and-upgrade-phpredis-extions.html"},{"title":"社交网站:热门内容排名算法浅议","text":"热门内容对任何网站都很重要，对社交网站更为重要。如何让用户第一眼就看到感兴趣的内容而留下来，正是各种排名算法的任务。 设计一个好的排名算法，需要考虑的因素很多，其中最重要的是以下两点： 社区属性及内容属性 网站本身内容的量级与更新频率 前者决定采用何种排名算法，后者确定排名的上升与下降速度。 影响因素社区属性及内容属性社交网站产生许多用户上传的内容，对这些内容进行排名时，需要考虑内容本身的信息。 从类型上，UGC内容主要是图片、文章、音频还是视频？一般来说，图片扫一眼，文章读一遍，音频听一次，视频看一遍,占用用户的时间依次增加,留住用户的成本也随之增加。 从特征上，单图或多图？段子还是长文？一首歌还是一集Podcast？短视频还是长视频？越复杂的形式，用户付出的时间成本越大。 用户付出了时间，那么会要求回报。网站回报用户的东西，无外乎兴趣和价值。而兴趣和价值，是两条不同方向的岔路，通向不同的排名算法。 网站本身内容的量级与更新频率内容的量级，像大象与蚂蚁，体量越大，系统越复杂。从海量内容中挑选出热门内容，需要更多的参数和指标。 更新频率，像流水线与手工作坊，频率越快，产出越多。新内容产生出来后，排名需要很快地上升，才能排到前列，引起注意；旧内容存在一段时间后，排名需要很快地下降，才能让位给新内容。 所以，需要根据网站本身的内容，确定排名算法需要考虑的参数，以及确定参数的取值。 兴趣与价值用户留下来，无外乎几种情况：这个东西很有趣（有趣型），或这个东西对我有用（有用型），或者兼而有之。 有趣的东西，用户第一眼看到就喜欢，但内容本身没有收藏价值。用户的态度是：见之我喜，不见也有其它东西让我喜。例如各种段子、笑话、gif图等。 有用的内容，用户看后念念不忘，收藏以备以后查看、回味与参考。例如旅游攻略、喜欢的文章、好听的歌曲等。 排名如何上升新内容一产生出来就具有上升趋势。 用户浏览、like（点赞、顶，喜欢）、评论、高评分（星级），编辑推荐等，都会使排名上升。 有趣型的内容，排名迅速上升，如病毒般地传播开来。 有用型的内容，排名逐渐上升，越到后来越多人知道它的价值。 排名如何下降旧内容渐渐呈现下降趋势。 少有访问、dislike（讨厌、踩，反对）、低评分（星级）、编辑降权，都会使排名下降。 有趣型的内容，排名维持了一段新鲜期后，大部分用户就失去兴趣了。 有用型的内容，排名虽然下降了，但其价值还在，还会不断有人关注。 没有银弹知名博主阮一峰发表了6篇基于用户投票的排名算法的系列文章。这6篇文章剖析了几个网站的排名算法。 如Delicious采用”过去60分钟内被收藏的次数”进行排名。每过60分钟，就统计一次。它的上升和下降趋势就是阶梯函数式的。 Hacker News采用票数计算排名，并考虑时间因素，分数随着时间逐渐减小。看下降曲线，Hacker News属于有趣型的网站（用户当然可以获得价值，这里指排名算法的类型）。 Stack Overflow属于有用型的网站。一个问题提出后，需要网友的回答、对答案投赞成或反对票、评论。这些内容的完善都需要时间。所以，随着时间的推移，一些问题和答案的价值开始慢慢显露，当你搜索时，Statck Overflow就会给你展示最有用的内容。 redit也属于有趣型的网站。它的时间因素占主导，就是说任何人发的新内容都比较容易靠前；排名算法中利用了赞成票与反对票的差额，说明有争议（很多人赞成，同时很多人反对）的内容，排名靠后；很多人赞成，很少人反对的内容，排名靠前。阮一峰得出结论： 这决定了Reddit是一个符合大众口味的社区，不是一个很激进、可以展示少数派想法的地方。 这也说明了，没有银弹，要建设什么样的社区，就要采用能体现社区特征的排名算法。","link":"/2016/02/15/sns-hot-feed-ranking-algorithm.html"},{"title":"PHP性能监控问题记录之一－安装配置php-apm","text":"php-apm是什么php-apm是一个PHP的性能检测工具，它能方便地捕捉到错误信息，并提供错误追踪回溯，获取请求的统计内存、CPU、响应时间的统计信息，并提供可视化的展示。 安装apm安装apm有两种方式：从PECL安装，或从源码编译。这里采用PECL安装。源码编译见官方文档。 sudo pecl install apm WARNING: channel &quot;pecl.php.net&quot; has updated its protocols, use &quot;pecl channel-update pecl.php.net&quot; to update downloading APM-2.0.5.tgz ... Starting to download APM-2.0.5.tgz (31,484 bytes) .........done: 31,484 bytes 14 source files, building running: phpize Configuring for: PHP Api Version: 20131106 Zend Module Api No: 20131226 Zend Extension Api No: 220131226 Enable Sqlite3 support [yes] : no Enable MariaDB/MySQL support [yes] : yes Enable Socket support [yes] : yes Enable Statsd support [yes] : yes 出现了如下错误： configure: error: Cannot find MySQL header files ERROR: `/tmp/pear/temp/APM/configure --with-php-config=/usr/bin/php-config --with-sqlite3=no --with-mysql --enable-socket=yes --enable-statsd=yes' failed 需要安装下mysql dev sudo apt-get install mysql-client libmysqlclient-dev 配置apm在php.ini上配置apm。apm.so依赖于json.so，如果之前没有加载，则应先加载。 extension=json.so extension=apm.so ;;;;;;;;;;;;;;;;;;; ; Module Settings ; ;;;;;;;;;;;;;;;;;;; ;apm apm.mysql_enabled=1 ; Error reporting level specific to the MariaDB/MySQL driver, same level as for PHP's *error_reporting* apm.mysql_error_reporting=E_ALL|E_STRICT apm.mysql_stats_enabled=1 apm.mysql_host=localhost apm.mysql_port=3306 apm.mysql_user=apm apm.mysql_pass=apm apm.mysql_db=apm apm.socket_enabled=0 apm.socket_stats_enabled=0 然后重启php，例如： service php5-fpm restart 部署apm-webapm提供了一个web端查看的项目，地址为：https://github.com/patrickallaert/php-apm-web。 将https://github.com/patrickallaert/php-apm-web/archive/master.zip解压后上传到网站根目录，或者如果使用git，可以clone到网站根目录下： git clone https://github.com/patrickallaert/php-apm-web.git apm-web 然后编辑config/db.php文件。如果选用mysql，可以如下配置： return new PDO(&quot;mysql:host=localhost;dbname=apm&quot;, &quot;apm&quot;, &quot;apm&quot;); 最后的效果如下： 返回Error的请求列表 点击每一列，都会出现请求的URL、时间、内存、CPU使用情况，以及错误信息。 点击错误信息，会出现该错误的Stacktrace。 还有详细的访问统计日志。 常见问题问题1 部署后无数据，且扩展未加载。检测php.ini文件，看是否有apm扩展。如果出现类似如下的信息，则说明扩展已加载。 问题2 部署后无数据，扩展已加载。mysql_stats_enabled默认值为0，这时是不收集的。可以设置apm.mysql_stats_enabled=1。","link":"/2016/06/15/problem-tips.html"},{"title":"PHP性能监控问题记录之二－session与gc过程","text":"session_start与gc垃圾回收过程在调性能时，偶然发现有个session函数（ThinkPHP/Common/functions.php）耗时很大。查了一下，ThinkPHP框架默认开启了session，就是每次请求都会调用session_start。 这里面存在问题。因为在PHP中, 如果使用file_handler作为Session的save handler, 那么就有概率在每次session_start的时候运行Session的Gc过程。详见鸟哥的分析。这样就造成每隔一段时间，session函数这里触发了Gc过程，就变慢了。 解决方法是将session_start默认关闭，需要时再打开。 'SESSION_AUTO_START'=&gt;false","link":"/2016/06/16/session-and-gc.html"},{"title":"Leetcode 258. Add Digits - 非负整数各位相加（另外一个解法）","text":"题目名称Add Digits （非负整数各位相加） 题目地址https://leetcode.com/problems/add-digits 题目描述英文：Given a non-negative integer num, repeatedly add all its digits until the result has only one digit. 中文：有一个非负整数num，重复这样的操作：对该数字的各位数字求和，对这个和的各位数字再求和……直到最后得到一个仅1位的数字（即小于10的数字）。 例如：num=38，3+8=11,1+1=2。因为2小于10，因此返回2。 解法1该算法有一个O(1)复杂度的《解法》。其中的公式为 (num - 1) % 9 + 1。这里引用一下这个公式的推导过程： 假设输入的数字是一个5位数字num，则num的各位分别为a、b、c、d、e。有如下关系：num = a * 10000 + b * 1000 + c * 100 + d * 10 + e 即：num = (a + b + c + d + e) + (a * 9999 + b * 999 + c * 99 + d * 9) 因为 a * 9999 + b * 999 + c * 99 + d * 9 一定可以被9整除，因此num模除9的结果与 a + b + c + d + e 模除9的结果是一样的。 对数字 a + b + c + d + e 反复执行同类操作，最后的结果就是一个 1-9 的数字加上一串数字，最左边的数字是 1-9 之间的，右侧的数字永远都是可以被9整除的。 这道题最后的目标，就是不断将各位相加，相加到最后，当结果小于10时返回。因为最后结果在1-9之间，得到9之后将不会再对各位进行相加，因此不会出现结果为0的情况。因为 (x + y) % z = (x % z + y % z) % z，又因为 x % z % z = x % z，因此结果为 (num - 1) % 9 + 1，只模除9一次，并将模除后的结果加一返回。 这个公式是正确的，但这个推导过程有待商榷。为什么要num要先减1，最后再加上1？ 解法2其实还有一个更容易的理解的解法。推导过程如下： 假设输入的数字是一个5位数字num，则num的各位分别为a、b、c、d、e。有如下关系：num = a * 10000 + b * 1000 + c * 100 + d * 10 + e 即：num = (a + b + c + d + e) + (a * 9999 + b * 999 + c * 99 + d * 9) 因为 a * 9999 + b * 999 + c * 99 + d * 9 一定可以被9整除，因此num模除9的结果与 a + b + c + d + e 模除9的结果是一样的。 所以最后的结果是：num % 9. 但是这个结果需要修正一下：如果num能被9整除，则结果为0，这时应该修正为9. 最后整个算法代码（swift）如下： class Solution { func addDigits(num: Int) -&gt; Int { if num == 0 { return 0 } var mod = num % 9 if mod == 0 { return 9 }else{ return mod } } } 算法复杂度也为O(1)。","link":"/2016/07/22/leetcode-258-add-digits.html"},{"title":"Leetcode 21. Merge Two Sorted Lists - 合并有序链表（递归与非递归解法）","text":"题目名称Merge Two Sorted Lists （合并有序链表） 题目地址https://leetcode.com/problems/merge-two-sorted-lists/ 题目描述英文：Merge two sorted linked lists and return it as a new list. The new list should be made by splicing together the nodes of the first two lists. 中文：有两个有序链表，将之合并为一个有序链表。 解法1 递归合并过程是这样的：先比较两个链表的头节点，节点值小的先摘到新链表中；然后再处理剩下的链表，直到两个链表都为空。递归解法比较容易实现。 算法代码（swift）如下： class Solution { func mergeTwoLists(l1: ListNode?, _ l2: ListNode?) -&gt; ListNode? { if l1 == nil{ return l2 } if l2 == nil{ return l1 } var ret: ListNode? if l1!.val &gt; l2!.val{ ret = l2 ret!.next = mergeTwoLists(l1,l2!.next) } else { ret = l1 ret!.next = mergeTwoLists(l1!.next,l2) } return ret } } 解法2 非递归也可以用while循环来做。就是用一个tail指针遍历两个链表，每次走到值较小的节点上，直到其中一个链表走到头。最后拼上另一个链表即可。 算法代码（swift）如下： class Solution { func mergeTwoLists(l1: ListNode?, _ l2: ListNode?) -&gt; ListNode? { if l1 == nil{ return l2 } if l2 == nil{ return l1 } var head: ListNode? var tail: ListNode? var ll1 = l1 var ll2 = l2 if l1!.val &gt; l2!.val{ head = l2 ll2 = l2!.next }else{ head = l1 ll1 = l1!.next } tail = head head?.next = tail while ll1 != nil &amp;&amp; ll2 != nil{ if ll1!.val &gt; ll2!.val{ tail?.next = ll2 tail = ll2 ll2 = ll2?.next }else{ tail?.next = ll1 tail = ll1 ll1 = ll1?.next } } if ll1 == nil{ tail?.next = ll2 } if ll2 == nil{ tail?.next = ll1 } return head } } 假设l1长度为M，l2长度为N，则算法复杂度为O(M+N)。","link":"/2016/07/23/merge-two-sorted-lists.html"},{"title":"Leetcode 172. Factorial Trailing Zeroes - 阶乘后缀0的数目（O(N)与O(logN)解法）","text":"题目名称Factorial Trailing Zeroes （阶乘后缀0的数目） 题目地址https://leetcode.com/problems/factorial-trailing-zeroes/ 题目描述英文：Given an integer n, return the number of trailing zeroes in n!.Note: Your solution should be in logarithmic time complexity. 中文：给一个整型数字n，返回n!后缀0的数目。需要对数级别的时间复杂度。 例如：10! = 3628800，后缀有两个0，返回2。 解法1 O(N)首先是不太可能直接计算n!，然后看后缀0的数目，因为n!太大了！我们先用笨办法，先算出前25个数字的结果，看能否找到规律。可以看到，后缀0数目发生变化的点，都是5的整数倍，如5，10，15，20，25…这容易理解，产生后缀0，只能是与10相乘，而10 ＝ 2 * 5，2广泛存在于偶数中，所以不用关注。只需要知道5的倍数的整数即可。 给定整数n，要找到以下数字，其中x*5^y &lt;= n： 5,10,15,...,x*5^y 然后看每个数字能分解出y个5: 1,1,1,...,y 最后的结果就是如下： Sum(1,1,1,...,y) 算法代码（swift）如下： class Solution { func trailingZeroes(n: Int) -&gt; Int { if n &lt; 5{ return 0 } var i = 5 var ret = 0 while i &lt;= n{ ret += num_of_five(i) i += 5 } return ret } func num_of_five(n: Int) -&gt;(Int) { var i = n var ret = 0 while i &gt; 0{ if i % 5 != 0 { break } i = i / 5 if i &gt;= 1{ ret++ } } return ret } } 这个算法需要循环(n/5)次，时间复杂度为O(N)，尽管“政治正确”，但不是最好的解法。 解法2 O(logN)将解法1的过程再回顾下，计算5出现的次数，可以分解看： 出现5，计数加1； 出现5*5，计数加2； 出现5*5*5，计数加3； 换个角度看， 除以5，计数加1； 除以5，计数加1；除以5*5，计数加1； 除以5，计数加1；除以5*5，计数加1；除以5*5*5，计数加1； 那么最后结果 floor(n/5) + floor(n/25) + floor(n/125) + .... 算法代码（swift）如下： class Solution1 { func trailingZeroes(n: Int) -&gt; Int { var i = 5 var ret = 0 while i &lt;= n{ ret += n / i i *= 5 } return ret } } 算法复杂度为O(logN)。","link":"/2016/07/24/factorial-trailing-zeroes.html"},{"title":"Leetcode 110. Balanced Binary Tree - 判断是否为平衡二叉树（递归解法）","text":"题目名称Balanced Binary Tree （判断是否为平衡二叉树） 题目地址https://leetcode.com/problems/balanced-binary-tree/ 题目描述英文：Given a binary tree, determine if it is height-balanced. For this problem, a height-balanced binary tree is defined as a binary tree in which the depth of the two subtrees of every node never differ by more than 1. 中文：给一个二叉树，判断是否为平衡二叉树。 解法算法代码（swift）如下： class Solution { func getHeight(root: TreeNode?) -&gt; Int{ if root == nil { return 0 } var leftCount = 0; var rightCount = 0; if root!.left != nil{ leftCount = getHeight(root!.left) + 1 } if root!.right != nil{ rightCount = getHeight(root!.right) + 1 } return leftCount &gt; rightCount ? leftCount : rightCount } func isBalanced(root: TreeNode?) -&gt; Bool { if root == nil { return true } var leftCount = root!.left == nil ? 0:getHeight(root!.left)+1 var rightCount = root!.right == nil ? 0:getHeight(root!.right)+1 if abs(leftCount-rightCount) &gt; 1{ return false }else if !isBalanced(root!.left) || !isBalanced(root!.right){ return false }else{ return true } } } 测试用例示例： public class TreeNode { public var val: Int public var left: TreeNode? public var right: TreeNode? public init(_ val: Int) { self.val = val self.left = nil self.right = nil } } //[1,null,2,null,3] var r1 = TreeNode(1) var r2 = TreeNode(2) var r3 = TreeNode(3) r1.right = r2 r2.right = r3 var s = Solution() s.isBalanced(r1)//false","link":"/2016/07/25/balanced-tree.html"},{"title":"Leetcode 344. Reverse String - 反转字符串","text":"题目名称Reverse String （反转字符串） 题目地址https://leetcode.com/problems/reverse-string/ 题目描述英文：Write a function that takes a string as input and returns the string reversed. Example:Given s = “hello”, return “olleh”. 中文：反转一个字符串。例如”hello”，转化为”olleh”。 解法算法代码（swift）如下： class Solution { func reverseString(s: String) -&gt; String { var len = s.characters.count var sArr = [Character](count:len, repeatedValue:&quot; &quot;) for i in s.characters{ sArr[--len] = i } return String(sArr) } } 算法复杂度为O(N)。 还有一种时间复杂度为O(N/2)的解法，即交换与中心对称位置的字符，这样可以只循环N/2次。","link":"/2016/07/25/reverse-string.html"},{"title":"Git使用Tips","text":"git clone 重命名文件夹1git clone https://github.com/user/userApp.git name_you_want git 多账户下 push出现403错误执行git push origin master时出现如下错误： remote: Permission to spetacular/asyntask.git denied to someone.fatal: unable to access ‘https://github.com/spetacular/asyntask.git/‘: The requested URL returned error: 403 这是由于.git/config的[remote “origin”]配置错误。可以更改此处代码。格式可以为以下两种情况： 12url=https://spetacular@github.com/spetacular/asyntask.giturl=ssh://git@github.com/spetacular/asyntask.git 如果不想更改全局配置，而只想临时使用： 1git push https://spetacular@github.com/spetacular/asyntask.git master git 设置和取消网络代理可设置为shadowsocks代理： 1234567git config --global https.proxy http://127.0.0.1:1080git config --global https.proxy https://127.0.0.1:1080git config --global --unset http.proxygit config --global --unset https.proxy git 更新特定tag已经打过tag，但又发现需要微调，再次git tag 1.0.0时，会发生如下错误 fatal: tag ‘1.0.0’ already exists 1.删除本地tag: 1git tag -d 1.0.0 2.重新打tag： 1git tag 1.0.03.push到服务器 1git push --force origin refs/tags/1.0.0:refs/tags/1.0.0","link":"/2016/07/31/git-sample.html"},{"title":"Restful API中的JSON模板解析－JSONOut库的介绍","text":"项目地址Restful API中的JSON模板解析 更新在2020-12-14:增加嵌套结构的支持。 为什么需要JSON模板解析Restful API的两个重要因素是输入和输出，一个好的API要求输入和输出都清晰可见。输入包括用户提交的方式、参数等，都是可预期的。尽量不要采用数组接收。例如以下的例子， $data = $_POST; $User-&gt;save($data); 这样的坏处一是无法过滤用户非法、多余的参数提交；二是参数不明，后续无法维护。 输出则要求仅提供满足需要的最小的数据集。例如如下的例子， $user = User::get(1); echo $user-&gt;toJson(); 这样的坏处一是输出的内容不清晰，只有知道数据表的结构才能了解输出的内容；二是可能一不小心把用户表敏感字段（密码，手机号）输出，造成信息泄漏。 JSONOut库介绍解决输出的方法，是采用JSON模板解析的方法。例如用户表有id,name,pass,avatar,phone字段，其中只有name,avatar是需要输出的。那么模板可以这样定义： { &quot;name&quot;: &quot;&quot;, &quot;avatar&quot;: &quot;&quot; } 当输出用户数据时，仅保留name和avatar，其它字段自动过滤掉。 优点是： 输出结构一目了然 仅保留满足需要的最小的数据集 敏感字段自动过滤，减少信息泄露 算法代码如下： class JSONOut { /** *按照JSON模板输出JSON数据 *@param data 源数据 *@param tplContent JSON模板内容 *@param multi 是否为多条内容，默认单条 *@return string */ public static function toJSON($data, $tplContent,$multi = false){ if(empty($data)){ return array(); } $tplData = json_decode($tplContent,true); if(!$tplData){ return false; } $array = array(); if (!$multi) { // 一维数组 $array = self::array_intersect_key_recursive($data,$tplData); } else { // 多维数组 foreach($data as $key =&gt; $value){ $array[$key] = self::array_intersect_key_recursive($value,$tplData); } } return json_encode($array); } private static function array_intersect_key_recursive(array $array1, array $array2) { if(self::is_indexed_array($array1)){//如果array1为索引数组，说明有多个元素，需要把array补充为相同数目 $diff = count($array1)-count($array2); for($i=0;$i&lt;$diff;$i++){ array_push($array2,$array2[0]); } } $array1 = array_intersect_key($array1, $array2); foreach ($array1 as $key =&gt; &amp;$value) { if (is_array($value) &amp;&amp; is_array($array2[$key])) { $value = self::array_intersect_key_recursive($value, $array2[$key]); } } return $array1; } /** * 判断数组是否为索引数组 */ private static function is_indexed_array($arr){ if (is_array($arr)) { return count(array_filter(array_keys($arr), 'is_string')) === 0; } return false; } } 测试代码测试代码如下： $tplContent = &lt;&lt;&lt;EOT { &quot;name&quot;: &quot;&quot;, &quot;avatar&quot;: &quot;&quot; } EOT; //单条数据 $user = array('id'=&gt;1,'name' =&gt; 'david','pass' =&gt; '123456','phone'=&gt;'13888888888','avatar' =&gt; 'http://spetacular.github.io/images/favicon.ico'); echo JSONOut::toJSON($user,$tplContent,false); echo &quot;\\n&quot;; //多条数据 $users = [array('id'=&gt;1,'name' =&gt; 'david','pass' =&gt; '123456','phone'=&gt;'13888888888','avatar' =&gt; 'http://spetacular.github.io/images/favicon.ico'), array('id'=&gt;2,'name' =&gt; 'john','pass' =&gt; '654321','phone'=&gt;'13888888889','avatar' =&gt; 'http://spetacular.github.io/images/favicon.png')]; echo JSONOut::toJSON($users,$tplContent,true); 测试结果如下： 单条数据： { &quot;name&quot;: &quot;david&quot;, &quot;avatar&quot;: &quot;http://spetacular.github.io/images/favicon.ico&quot; } 多条数据： [ { &quot;name&quot;: &quot;david&quot;, &quot;avatar&quot;: &quot;http://spetacular.github.io/images/favicon.ico&quot; }, { &quot;name&quot;: &quot;john&quot;, &quot;avatar&quot;: &quot;http://spetacular.github.io/images/favicon.png&quot; } ] 未来扩展注意到JSON模板仅有key，没有value，未来可以在value上扩展，比如默认值，比如： {&quot;gender&quot;:0} 如果数据里没有gender的数据，则默认为0； 或者在value上加过滤函数，比如： {&quot;age&quot;:&quot;int&quot;} 表示age要转化为整型数字。","link":"/2016/08/01/json-template-to-json.html"},{"title":"PHP异步任务队列管理器asyntask介绍","text":"PHP异步任务队列管理器asyntaskasyntask是一个轻量级异步任务队列管理器，支持实时，定时，长时和周期任务。 项目由来本项目最初用于通知推送。例如用户发布评论，需要推送一条push给原作者。而到苹果的服务器的请求时间较长，如果等待苹果服务器的返回结果，则整个发布评论的接口的响应时间就太长了。因为推送push早1秒晚1秒对用户基本没影响，所以当用户发布评论时，只要数据到数据库，即可返回。与此同时创建一条异步任务，在1秒内给用户推送push。这样既保证了接口的响应速度，又不影响用户体验。该项目已经在线上环境运行1年多，执行了累计8千万条命令，运行稳定。 优点 异步执行 集成管理后台，可视化操作 代码集成，可编程 缺点并非真正实时，秒级误差。 安装下载源码直接使用： 1git clone https://github.com/spetacular/asyntask.git 命令下载到本地。 也可以点击 https://github.com/spetacular/asyntask/archive/master.zip 下载最新内容的压缩包，然后解压。 通过 composer 来安装 在你的 composer 项目中的 composer.json 文件中，添加这部分： 12345{ &quot;require&quot;: { &quot;davidyan/asyntask&quot;: &quot;&gt;=1.0&quot; }} 然后执行composer install。调用示例如下： 12345678910include './vendor/autoload.php';$task = new AsynTask\\Task();//添加单次任务$name = '单次任务';$cmd = 'php abcd.php';$params = array( 'params'=&gt;1);$task-&gt;add_once_task($name,$cmd,$params); 配置1.asyntask的数据默认存储在Mysql数据库里，因此需要更改config.php里的配置： 123456'DB_HOST'=&gt;'127.0.0.1','DB_NAME' =&gt; 'asyntask','DB_USER' =&gt; 'root','DB_PWD' =&gt; '','DB_PORT' =&gt; '3306','DB_CHARSET' =&gt; 'utf8mb4', 2.导入数据表将resource文件夹里的db.sql导入数据库中。 3.配置健康检查脚本run.sh定期检查异步任务的运行状况，如果挂了，cron_asyn_task.php脚本。 1chmod +x run.sh 然后配置CronTab。运行crontab -e，然后添加一行： 1* * * * * path-to/run.sh &gt; /dev/null 2&gt;&amp;1 使用方式管理后台自带管理后台，可以轻松添加、编辑、删除、搜索任务。代码在https://github.com/spetacular/asynadmin，请自行部署。 编程方式可以集成到项目中，完整使用示例见test.php。例如添加周期任务： 1234567891011$name = '周期任务';$cmd = 'php abc.php';$params = array( 'params'=&gt;1);$timeOptions = array( 'day'=&gt;1, 'hour'=&gt;2, 'minute'=&gt;3);$task-&gt;add_loop_task($name,$cmd,$params,$timeOptions); 周期任务示例每天执行：day 1 hour 0 minute 0 每天零点执行 每小时执行：day 0 hour 1 minute 5 每小时的5分执行 每隔若干分钟执行：day 0 hour 0 minute 5 每隔5分钟执行","link":"/2017/03/09/asyntask-desc.html"},{"title":"开发知识点记录","text":"cron 任务没执行的原因查看 cron 日志，日志路径一般在： 1/var/log/cron.log 或者 1/var/log/syslog 这样就能看到类似这样的错误： Error: bad username; while reading /etc/crontab(system) ERROR (Syntax error, this crontab file will be ignored) 然后按图索骥，查找失败原因。 cron 启动、关闭、重启、重载、查看服务状态12345service cron start //启动服务 service cron stop //关闭服务 service cron restart //重启服务 service cron reload //重新载入配置 service cron status //查看服务状态 命令行输错命令如何快速撤销？最笨的方法可以按删除键，一个一个删除；最快的办法是Ctrl + U，一键清理。 docker显示容器命令(container CMD)详情 php生成形容’HH:MM:SS’格式的时间1gmstrftime('%H:%M:%S',$time) 例如 12echo gmstrftime('%H:%M:%S',3786);//Output: 01:03:06 chrome 下上传图片/文件慢的解决办法最近用flow.js做的上传图片功能，突然变得很慢，要10秒左右才能打开上传对话框。查了一下，是mac chrome的一个bug。设置图片格式时，如果采用如下写法： 1&lt;input type=&quot;file&quot; accept=&quot;image/*&quot;&gt; 则会重现。有网友反映，windows 10, chrome 53下也会有此问题。解决办法是，将image/*换为具体的格式： 1&lt;input type=&quot;file&quot; accept=&quot;image/png, image/jpeg, image/gif&quot;&gt; 参考：open dialog so slow with Chrome redis 按前缀/模式批量删除keys有两种情况：1.在redis外部，可以用 xargs；2.在redis内部，可以用 eval 执行脚本示例：先创建一些key。 1234567127.0.0.1:6379&gt; set david1 1OK127.0.0.1:6379&gt; set david2 2OK127.0.0.1:6379&gt; keys david*1) &quot;david2&quot;2) &quot;david1&quot; 方法1: 1redis-cli keys david* | xargs redis-cli del 方法2:进入 redis 命令行。 12127.0.0.1:6379&gt; eval &quot;local keys = redis.call('keys',KEYS[1]) for i,v in ipairs(keys) do redis.call('del',v) end&quot; 1 david*(nil) 两种方法的结果如下： 12127.0.0.1:6379&gt; keys david*(empty list or set)","link":"/2017/01/01/develop-tips.html"},{"title":"Docker:基本概念","text":"镜像Docker镜像Image是一个就像具有Time Machine功能的虚拟机，保存着特定时刻的一个快照。这个快照包含了已安装的程序、共享库、配置文件、环境变量、用户组等信息。构建Image时，以上内容不会改变。这意味着可以利用Image快速复制出多个相同的运行实体（容器）。 容器Docker容器container是镜像Image的运行实体。一个镜像可以生成多个相同的容器，每个容器的代码、运行环境、系统工具、系统库都完全相同。这在环境部署、服务扩容等方面具有重大作用。 仓库Docker仓库Registry是一个镜像存储和分发的服务。一个 Docker Registry中可以包含多个仓库（Repository）；每个仓库可以包含多个标签（Tag）；每个标签对应一个镜像。 构建利用 Dockerfile 定制镜像时，可以用docker build命令，将指令构建为文件系统。 宿主机宿主机就是运行Docker容器的机器。例如我在mac上用Docker for Mac，执行docker pull ubuntu来拉取一个ubuntu镜像然后运行，那么宿主机指的是mac。","link":"/2017/03/10/docker-basic-concept.html"},{"title":"Docker:网络配置","text":"Docker运行容器时，一个常见的问题是：localhost(127.0.0.1)指向哪里？这涉及到Docker的网络配置问题。 Docker支持的网络模式如下：bridge（默认）、host、container、network-name、none。 桥接 bridgeDocker 默认的网络模式是 bridge 。在该模式下，docker 创建了一个 bridge，名称通常为 docker0 。可以用ifconfig来查看： 1234567docker0 Link encap:Ethernet HWaddr **:**:**:**:**:** inet addr:192.168.0.1 Bcast:0.0.0.0 Mask:255.255.240.0 UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:305002 errors:0 dropped:0 overruns:0 frame:0 TX packets:407006 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:30331811 (30.3 MB) TX bytes:752905683 (752.9 MB) 宿主机和容器通过bridge进行通信，如图所示： 容器内的localhost指向容器内部。 本机 host当使用 --net=host启动容器时，网络配置为 host 模式。该模式下，容器和宿主机共享网络。形象地讲，容器和宿主机共享一个网卡，在容器内的网络访问如同直接在宿主机上操作一样。例如： 1docker run --rm -it --net=host ubuntu:trusty bash 容器内的localhost指向宿主机。 containerDocker可以指定一个容器复用另一个容器的网络设置。这种模式适用于由多个容器搭建整套系统的情况，例如搭建PHP开发环境时，希望Mysql、Redis都使用PHP容器的网络配置，即可以使用该模式。 network-nameDocker允许用户使用Docker network driver或[第三方network driver插件]（https://docs.docker.com/engine/extend/legacy_plugins/#network-plugins）创建自定义网络，然后多个容器都可以使用相同的网络。常见的第三方etwork driver插件有：Contiv Networking、Kuryr Network Plugin、Weave Network Plugin。 none顾名思义，该模式下没有网络连接。例如我的宿主机ip为192.168.0.59，在容器内访问宿主机就提示无网络。 123docker run -it --net=none my_ubuntu bashroot@6533968160dd:/# ping 192.168.0.59connect: Network is unreachable","link":"/2017/03/20/docker-network-settings.html"},{"title":"教程：配置 Let&#39;s Encrypt 免费 HTTPS 证书","text":"最新更新Certbot 更新了自动更新脚本，现在配置更简单了,基本实现全自动化。 以 Nginx 和 Ubuntu 16.04 为例： 1. 安装 certbot 软件12345$ sudo apt-get update$ sudo apt-get install software-properties-common$ sudo add-apt-repository ppa:certbot/certbot$ sudo apt-get update$ sudo apt-get install python-certbot-nginx 2. 一键安装脚本certbot 附带一个 nginx 插件，可以直接生成证书和写入nginx配置文件中。 1sudo certbot --nginx 3. 自动更新将自动更新脚本加入crontab中，每月自动更新，crontab -e 加入以下内容： 10 0 1 * * certbot renew 目录定义 存放验证域名的文件，即acme-dir，默认存储在/home/xxx/www/challenges/ 存放最终结果证书的目录，如本文中的/data/ssl 存放验证域名文件的目录12345 mkdir ~/www/challenges/``` 然后配置一个 HTTP 服务，以 Nginx 为例： NGINX server { server_name www.yoursite.com yoursite.com; location ^~ /.well-known/acme-challenge/ { alias /home/xxx/www/challenges/; try_files $uri =404; } location / { rewrite ^/(.*)$ https://yoursite.com/$1 permanent; } } 1234 然后重启nginx。``` sudo service nginx reload 创建存放证书的目录12mkdir /data/sslcd /data/ssl 创建一个 RSA 私钥用于 Let’s Encrypt 识别你的身份1openssl genrsa 4096 &gt; account.key 创建 RSA 私钥（兼容性好）1openssl genrsa 4096 &gt; domain.key Common Name 必须为你的域名，其它的随便填1openssl req -new -sha256 -key domain.key -out domain.csr 获取脚本1wget https://raw.githubusercontent.com/diafygi/acme-tiny/master/acme_tiny.py 指定账户私钥、CSR 以及验证目录，执行脚本：1python acme_tiny.py --account-key ./account.key --csr ./domain.csr --acme-dir /home/xxx/www/challenges/ &gt; ./signed.crt 如果一切正常，当前目录下就会生成一个 signed.crt，这就是申请好的证书文件。 证书融合1234 wget -O - https://letsencrypt.org/certs/lets-encrypt-x3-cross-signed.pem &gt; intermediate.pemcat signed.crt intermediate.pem &gt; chained.pem wget -O - https://letsencrypt.org/certs/isrgrootx1.pem &gt; root.pemcat intermediate.pem root.pem &gt; full_chained.pem ssl配置12345678910111213141516171819202122server { listen 443; server_name www.text.wiki; root /data/htdocs/textwiki; autoindex on; index index.php index.html; ssl on; ssl_certificate /data/ssl/chained.pem; ssl_certificate_key /data/ssl/domain.key; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_trusted_certificate /data/ssl/full_chained.pem; location / { try_files $uri $uri/ /index.php; location ~ \\.php$ { fastcgi_pass unix:/var/run/php5-fpm.sock; fastcgi_param SCRIPT_FILENAME /data/htdocs/textwiki$fastcgi_script_name; include fastcgi_params; } } } 自动更新脚本1.新建文件vi renew_cert.sh 2.填写如下内容 12345678 #!/bin/bash cd /data/ssl/ python acme_tiny.py --account-key account.key --csr domain.csr --acme-dir /home/xxx/www/challenges/ &gt; signed.crt || exit wget -O - https://letsencrypt.org/certs/lets-encrypt-x3-cross-signed.pem &gt; intermediate.pem cat signed.crt intermediate.pem &gt; chained.pem wget -O - https://letsencrypt.org/certs/isrgrootx1.pem &gt; root.pemcat intermediate.pem root.pem &gt; full_chained.pem service nginx reload 3.chmod +x renew_cert.sh 加入crontabcrontab 中加入自动更新脚本，每月自动更新，crontab -e 加入以下内容： 10 0 1 * * /data/ssl/renew_cert.sh &gt;/dev/null 2&gt;&amp;1 最关键的是，如果服务器有防火墙，请放开服务器的443入口说明本文注重实战，跟着教程一步一步配置即可，想了解原理的同学，可以参考如下网址：Let’s Encrypt官网Let’s Encrypt，免费好用的 HTTPS 证书Let’s Encrypt 给网站加 HTTPS 完全指南","link":"/2017/03/29/lets-encrypt-deploy-step-by-step.html"},{"title":"Nginx缓存最佳实践","text":"与Varnish类似，Nginx非常适合做网页缓存。许多管理员转向Varnish，因为Varnish确实有用。但是，Nginx也有如下优点： Nginx能非常有效地直接处理静态内容。在静态文件和Nginx在同一主机的情况下，这种特性尤为有用。 当放置在应用服务器前端时，Nginx确实能够担当缓存服务器的角色。 虽然Varnish作为网页缓存服务器拥有比Nginx更丰富的缓存相关的特性，但是Nginx仍然是一个不错的选择。 如果您的流量需要为缓存添加一层基础设施，但不需要引入学习和维护的新技术的开销，Nginx可能更适合。 如果您碰巧使用Nginx Plus，它具有支持和额外的功能，这一点尤其如此。 用例Nginx自己处理静态内容。这是Web服务器的典型用例，而不是缓存服务器。然而，由于Nginx可以向其他Web服务器或应用程序（通过HTTP，FastCGI和uWSGI）代理请求，因此通常用于在向其他进程代理应用程序请求时提高服务静态文件的性能。 这是一个常见的架构，但是PHP的用户可能会使用Apache + Apache的mod_php，它将PHP“内置”到Apache中，使得它似乎一切都在一个神奇（但效率较低）的地方。除了直接提供静态文件的能力外，Nginx还可以作为缓存服务器，这意味着Nginx可以缓存从其他服务器接收的内容。 以下用例是使用Nginx作为缓存服务器的一些常见的用例： Nginx可以位于Web服务器的前端，这可能是其他Nginx实例或Web应用程序。这是缓存服务器的典型用例 - 它作为其他Web或应用程序服务器的网关，起到了负载平衡器的作用。 Nginx缓存可以与负载均衡器一起使用。 实际上，Nginx可以充当负载平衡器和缓存服务器！ 除了其他HTTP服务器/监听器之外，Nginx还可以缓存代理到FastCGI和uWSGI进程的请求结果！一个很好的用例是缓存内容管理系统（CMS）的结果，大多数用户不需要网站的动态方面 - 他们只是想看到内容。 缓存服务器的主要优点是我们在我们的应用服务器上放置的负载较少。对缓存的静态或动态资产的请求无需甚至到达应用程序（或静态内容）服务器 - 我们的缓存服务器本身可以处理许多请求！ 在这里的例子中，我们使用Nginx处理静态站点，再在前面放置Nginx缓存服务器。 它如何工作首先你需要知道Origin Server是什么。 源服务器是拥有真正的静态文件或动态生成的HTML的服务器。他们有两个责任： 请求时提供动态和静态内容 通过HTTP缓存头决定如何缓存文件（和潜在的动态内容） 接下来说下缓存服务器。 缓存服务器（通常）是“前端”;它从客户端收到初始HTTP请求。然后它会处理请求本身（如果它具有所请求资源的新缓存副本），或者将请求传递给Origin Server来实现。 如果请求发送到Origin Server，则由Cache Server读取源服务器的响应头，以确定响应是缓存还是简单传递。 一些较大的Web应用程序除了缓存服务器之外还使用负载平衡器，从而导致高度分层的基础架构。 缓存服务器的职责： 确定HTTP请求是否接受缓存响应，并且缓存中有一个新项目可以响应 如果请求不应被缓存，或者缓存的项目是否过期，则向源服务器发送HTTP请求 响应来自其缓存或源服务器的HTTP响应为适当的。 最后说下客户端。客户端可以拥有自己的本地（私有）缓存 - 例如每个浏览器都有一个本地缓存。我们的浏览器可能会缓存一个响应本身（通常是图像，CSS和JS文件），因此，如果静态文件在其本地缓存中已经有新版本，那么浏览器根本不会向缓存服务器发送请求。 实现本地缓存的客户端具有以下职责： 发送请求 缓存响应 决定从本地缓存中提取请求或发出HTTP请求以检索它们 源服务器源服务器最终负责提供文件和控制如何缓存文件。 客户端可以请求不被缓存的资源。缓存服务器“必须”遵守HTTP规范。 此外，客户端请求可缓存资源时，必须遵循从源服务器返回的缓存参数，这可能包括不缓存结果的指令！ 这意味着我们需要确定文件如何缓存在我们的源服务器上。 为此，我通常将H5BP配置目录复制到/etc/nginx/h5bp,作为Nginx服务器配置。 复制H5BP文件之后，我可以在源服务器的Nginx虚拟主机中包含basic.conf配置： 123456789101112131415server { # Note that it's listening on port 9000 listen 9000 default_server; root /var/www/; index index.html index.htm; server_name example.com www.example.com; charset utf-8; include h5bp/basic.conf; location / { try_files $uri $uri/ =404; }} 与缓存最相关的H5BP配置文件是expires.conf，它确定了常见文件的缓存行为： 123456789101112131415161718192021222324252627# Expire rules for static content# cache.appcache, your document html and datalocation ~* \\.(?:manifest|appcache|html?|xml|json)$ { expires -1; # access_log logs/static.log; # I don't usually include a static log}# Feedlocation ~* \\.(?:rss|atom)$ { expires 1h; add_header Cache-Control &quot;public&quot;;}# Media: images, icons, video, audio, HTClocation ~* \\.(?:jpg|jpeg|gif|png|ico|cur|gz|svg|svgz|mp4|ogg|ogv|webm|htc)$ { expires 1M; access_log off; add_header Cache-Control &quot;public&quot;;}# CSS and Javascriptlocation ~* \\.(?:css|js)$ { expires 1y; access_log off; add_header Cache-Control &quot;public&quot;;} 上述配置禁用manifest，appcache，html，xml和json文件的缓存。 它将RSS和ATOM订阅文件缓存1小时，Javascript和CSS文件1年，以及其他静态文件（图像和媒体）1个月。 缓存全部设置为“public”，所以任何系统都可以缓存它们。 将它们设置为私有将限制它们被私有缓存（例如我们的浏览器）缓存。 所以源服务器本身没有进行任何缓存，只是说文件应该根据文件扩展名进行缓存。 H5BP为设置缓存规则提供了不错的参考。 如果直接向源服务器发出请求，我们可以看到这些规则生效。 原始服务器我设置来测试这个恰好是运行在127.17.0.18:9000 以.html结尾的文件不会被缓存。 我们可以看到响应如下： 123456789# GET curl request and select response headers$ curl -X GET -I 127.17.0.18:9000/index.htmlHTTP/1.1 200 OKServer: nginx/1.4.6 (Ubuntu)Date: Fri, 05 Sep 2014 23:24:52 GMTContent-Type: text/htmlLast-Modified: Fri, 05 Sep 2014 22:16:24 GMTExpires: Fri, 05 Sep 2014 23:24:52 GMTCache-Control: no-cache 请注意，Expires与Date相同，表示该请求立即到期，即告诉客户端不要缓存。 响应还返回头信息Cache-Control：no-cache来表明不缓存响应内容。 这完全遵循H5BP expires.conf配置设置的.html文件的规则。 接下来，我们可以尝试获取一个需要缓存的文件： 12345678910# GET curl request and select response headers$ curl -X GET -I 127.17.0.18:9000/css/style.cssHTTP/1.1 200 OKServer: nginx/1.4.6 (Ubuntu)Date: Fri, 05 Sep 2014 23:25:04 GMTContent-Type: text/cssLast-Modified: Fri, 05 Sep 2014 22:46:39 GMTExpires: Sat, 05 Sep 2015 23:25:04 GMTCache-Control: max-age=31536000Cache-Control: public 我们可以看到这个css文件在当前日期的1年后过期！ 缓存规则的设置max-age(过期时间）大约为1年（以秒为单位），并允许公共缓存。 这也遵循了H5BP expires.conf里设置的.css文件的规则。 至此，源服务器设置好了。 缓存服务器源服务器已经设置完毕，但是我们需要在源服务器前面放置一个缓存服务器。 在我们的场景中，缓存服务器将作为Web服务器来接收请求。 如果缓存服务器不能直接从缓存里命中文件，则它会将HTTP请求传递给源服务器。 在开始安装新的Nginx之前，我们可以先看一下反向代理的“标准”设置。以下设置尚未配置任何缓存，它只是实现将请求代理到源服务器的功能： 123456789101112131415server { # Note that it's listening on port 80 listen 80 default_server; root /var/www/; index index.html index.htm; server_name example.com www.example.com; charset utf-8; location / { include proxy_params; proxy_pass http://172.17.0.18:9000; }} 只是简单地将请求代理到源服务器。 这里为测试而设置的缓存服务器监听在172.17.0.13:80 如果我们在缓存服务器上发出请求，就像直接发送请求到源服务器一样。 这是因为缓存服务器当前没有缓存：它只是将请求传递给源服务器： 123456789$ curl -X GET -I 172.17.0.13/css/style.cssHTTP/1.1 200 OKServer: nginx/1.4.6 (Ubuntu)Date: Fri, 05 Sep 2014 23:30:07 GMTContent-Type: text/cssLast-Modified: Fri, 05 Sep 2014 22:46:39 GMTExpires: Sat, 05 Sep 2015 23:30:07 GMTCache-Control: max-age=31536000Cache-Control: public 现在，我们添加必要的指令，实现从源服务器中获取Nginx缓存响应。 我们在上面定义的配置中，增加额外的缓存指令： 1234567891011121314151617181920212223# Note that these are defined outside of the server block,# altho they don't necessarily need to beproxy_cache_path /tmp/nginx levels=1:2 keys_zone=my_zone:10m inactive=60m;proxy_cache_key &quot;$scheme$request_method$host$request_uri&quot;;server { # Note that it's listening on port 80 listen 80 default_server; root /var/www/; index index.html index.htm; server_name example.com www.example.com; charset utf-8; location / { proxy_cache my_zone; add_header X-Proxy-Cache $upstream_cache_status; include proxy_params; proxy_pass http://172.17.0.18:9000; }} 以下解释一下缓存指令。 proxy_cache_path这是保存缓存文件的路径。 levels指令设置缓存文件如何保存到文件系统。如果没有定义，缓存文件直接保存在指定的路径中。如果这样定义（1：2），缓存文件将根据其md5哈希值保存在缓存路径的子目录中。 keys_zone是缓存区域的名称。这里它被命名为my_zone，并为缓存key和其他元数据提供了10MB的存储空间，尽管这并不限制可以缓存的文件数量！它只是设置元数据的存储空间。文档声称1MB区可以存储约8000个key和元数据。 最后，我们设置了inactive指令，它告诉Nginx在60分钟内清除任何没有访问的缓存。请注意，这里60m是60分钟，而key_zone的10m是10兆字节。如果未显式设置，则inactive指令默认为10分钟。 使用inactive使Nginx有机会“忘记”关于不常被请求的缓存资源。这样一来，Nginx缓存可以让您最大程度的降低成本 - 最需要的资源会保留在缓存中（并遵循愿服务器所指示的缓存规则）。 proxy_cache_key这是用来区分缓存文件的key。 默认值为$scheme$proxy_host$uri$is_args$args，但是我们可以根据需要进行更改。 proxy_cache_key也可以设置为类似&quot;$host$request_uri $cookie_user&quot;（带引号）这样的形式，也可以包括cookies信息。 Cookie确实会影响缓存，所以请谨慎设置！ 如果Cookie被并入缓存密钥，您可能会意外地Nginx为每个独立cookie（每个站点访问者）都创建了重复缓存的文件。 这意味着将Cookie并入key确实会降低缓存的有效性。 针对每个用户的缓存，是私有缓存（Web浏览器）的目的，而不是我们正在构建的“公共”缓存服务器的目的。 但是，在某种情景下确实需要引入Cookie，那么proxy_cache_key的这个选项就很有用了。 proxy_cache在location块内，Nginx以proxy_cache my_zone指令定义缓存区域。 我们还添加了一个有用的header（头信息）来通知我们资源是否从缓存提供。这可以通过add_header X-Proxy-Cache $ upstream_cache_status指令完成。 这将设置一个名为X-Proxy-Cache的响应头，值为HIT，MISS 或 BYPASS。 保存配置文件后，重新加载Nginx的配置（sudo service nginx reload），并再次尝试HTTP请求。 首次获取CSS文件： 123456789# GET curl request and selected headers$ curl -X GET -I 172.17.0.13/css/style.cssDate: Fri, 05 Sep 2014 23:50:12 GMTContent-Type: text/cssLast-Modified: Fri, 05 Sep 2014 22:46:39 GMTExpires: Sat, 05 Sep 2015 23:50:12 GMTCache-Control: max-age=31536000Cache-Control: publicX-Proxy-Cache: MISS 因为之前没有请求过该文件，所以这里返回的cache状态为MISS。缓存服务器需要向源服务器请求资源。再试一次： 123456789# GET curl request and selected headers$ curl -X GET -I 172.17.0.13/css/style.cssDate: Fri, 05 Sep 2014 23:50:48 GMTContent-Type: text/cssLast-Modified: Fri, 05 Sep 2014 22:46:39 GMTExpires: Sat, 05 Sep 2015 23:50:12 GMTCache-Control: max-age=31536000Cache-Control: publicX-Proxy-Cache: HIT 我们可以看到，第二个请求和第一个请求相差不到30秒。 我们可以通过X-Proxy-Cache头信息看到缓存状态是HIT。 Expires头信息保持不变，因为Nginx只是从缓存中返回资源。 当缓存服务器返回到源服务器获取新文件时，那些头信息将会更新。 安装现在的设置，Nginx将忽略客户端的Cache-Control请求头。 但是，有些Web客户端不想使用缓存项目，我们希望缓存服务器能够支持这种要求。 例如，使用浏览器打开网页时，按住SHIFT，同时单击重新加载按钮，这时浏览器将发送一个Cache-Control: no-cache。 这要求缓存服务器不提供资源的缓存版本。 但目前我们的设置会将之忽略。 为了在请求时适当地绕过缓存，我们可以在location块中将proxy_cache_bypass $http_cache_control指令添加到缓存服务器中： 12345678location / { proxy_cache my_zone; proxy_cache_bypass $http_cache_control; add_header X-Proxy-Cache $upstream_cache_status; include proxy_params; proxy_pass http://172.17.0.18:9000;} nginx reload 之后，可以看到设置生效了： 1234567$ curl -X GET -I 172.17.0.13/css/style.css...X-Proxy-Cache: HIT # A regular request which is normally a cache HIT ...$ curl -X GET -I -H &quot;Cache-Control: no-cache&quot; 172.17.0.13/css/style.css...X-Proxy-Cache: BYPASS # ... is now bypassed when told to proxy_cache_bypass指令告知Nginx遵守HTTP请求中的Cache-Control请求头。 代理缓存Nginx的缓存功能强大！ 我们刚刚看到它可以缓存代理的HTTP请求，但是它也可以缓存FastCGI，uWSGI代理请求的结果，甚至缓存负载平衡请求（“upstream”）的结果。 这意味着我们可以缓存到动态应用程序的请求结果。 如果我们使用Nginx来缓存FastCGI进程的结果，我们可以将FastCGI进程视为源服务器，将Nginx作为缓存服务器。 例如，在fideloper.com上，我缓存了从PHP-FPM返回的HTML结果。 这是一个使用fastcgi_cache的示例： 1234567891011121314151617181920212223242526272829303132fastcgi_cache_path /tmp/cache levels=1:2 keys_zone=fideloper:100m inactive=60m;fastcgi_cache_key &quot;$scheme$request_method$host$request_uri&quot;;server { # Boilerplay omitted set $no_cache 0; # Example: Don't cache admin area # Note: Conditionals are typically frowned upon :/ if ($request_uri ~* &quot;/(admin/)&quot;) { set $no_cache 1; } location ~ ^/(index)\\.php(/|$) { fastcgi_cache fideloper; fastcgi_cache_valid 200 60m; # Only cache 200 responses, cache for 60 minutes fastcgi_cache_methods GET HEAD; # Only GET and HEAD methods apply add_header X-Fastcgi-Cache $upstream_cache_status; fastcgi_cache_bypass $no_cache; # Don't pull from cache based on $no_cache fastcgi_no_cache $no_cache; # Don't save to cache based on $no_cache # Regular PHP-FPM stuff: include fastcgi.conf; # fastcgi_params for nginx &lt; 1.6.1 fastcgi_split_path_info ^(.+\\.php)(/.+)$; fastcgi_pass unix:/var/run/php5-fpm.sock; fastcgi_index index.php; fastcgi_param LARA_ENV production; }} 这里的缓存设置选项很多。对于使用FastCGI缓存，请注意以下两点 用fastcgi_cache替换proxy_cache的所有实例 用fastcgi_cache_valid 200 60m来设置PHP请求响应的到期时间。 你可以看到实际效果： 12345678910111213$ curl -X GET -I fideloper.com/index.php...Cache-Control: max-age=86400, publicX-Fastcgi-Cache: MISS$ curl -X GET -I fideloper.com/index.php...X-Fastcgi-Cache: HIT# If this URL existed, you'd see a BYPASS$ curl -X GET -I fideloper.com/admin...X-Fastcgi-Cache: BYPASS 相关资源您可以使用缓存进行更多的操作，例如设置不进行缓存的情况（例如管理区域）以及清除缓存的方法。 Learn about Web Caches , 作者是Mark Nottingham，IETF组织HTTP工作组主席，W3C技术架构组的成员。 建议所有网站开发者都阅读。还有 HTTP Specification 也很有参考价值。 Nginx Admin Guide on Caching HTTP Cache Docs FastCGI Cache docs uWSGI Cache Docs 本文翻译自Nginx Caching。 感谢谷歌翻译，它出色的效果减少了博主很多工作量。","link":"/2017/04/10/nginx-cache.html"},{"title":"在线编辑器增加本地自动保存功能[Editor.md,UEditor]","text":"UEditor 增加本地自动保存功能，更新于2017年11月17日。 Editor.mdEditor.md 是一款开源 Markdown 在线编辑器。在编辑较长文章时，为防止疏忽造成内容丢失，可以添加自动保存功能。 预览地址： https://www.text.wiki/md/ 利用localStorage来暂存数据。当文本框有改动时，将内容存到localStorage里；当页面再次加载时，读取localStorage的值。 假设页面创建的编辑器命名为 mdEditor ,则只需将以下js加入页面即可： 12345678910111213141516&lt;script&gt; var key = 'default_md_key'; mdEditor.on(&quot;load&quot;, function(){ var content = localStorage.getItem(key); if(content){ var f = confirm(&quot;您上次编辑的文章未提交，是否恢复？内容：\\n&quot;+content); if(f == true){ testEditor.setValue(content) } } }) mdEditor.on(&quot;change&quot;, function(){ localStorage.setItem(key,this.getValue()) })&lt;/script&gt; 用户向服务器提交数据并保存成功后，可以删掉key。 1localStorage.removeItem(key); UEditorUEditor 本身有自动保存功能，但并不好用，这里也说明一下配置方法。 12345678910111213141516var key = 'default_ue_key';var ue = UE.getEditor('editor'); ue.addListener( 'ready', function( editor ) { var content = localStorage.getItem(key); if(content){ var f = confirm(&quot;您上次编辑的文章未提交，是否恢复？内容：\\n&quot;+content); if(f == true){ UE.getEditor('editor').execCommand('insertHtml', content); } } } ); ue.addListener('contentchange',function(){ var content = UE.getEditor('editor').getContent(); localStorage.setItem(key,content); }); 用户向服务器提交数据并保存成功后，可以删掉key。 1localStorage.removeItem(key);","link":"/2017/04/07/editor-md-auto-save-markdown.html"},{"title":"小程序新能力：公众号自定义菜单点击打开相关小程序","text":"微信在2017年3月27日发布了《小程序新能力》的重要更新，主要新增了以下功能： 个人开发者可申请小程序 公众号自定义菜单点击可打开相关小程序 公众号模版消息可打开相关小程序 公众号关联小程序时，可选择给粉丝下发通知 移动App可分享小程序页面 扫描普通链接二维码可打开小程序 本文主要说明实现公众号自定义菜单点击打开相关小程序。 公众号可将已关联的小程序页面放置到自定义菜单中，用户点击后可打开该小程序页面。公众号运营者可在公众平台进行设置，也可以通过自定义菜单接口进行设置。 1.公众平台设置方式1.1关联小程序当读者使用公众平台设置方式时，首先要关联公众号和小程序。打开公众平台https://mp.weixin.qq.com，打开“设置” –&gt; “公众号设置”，可以看到如下图： 点击“添加”按钮，即进入小程序添加页面。 扫码验证身份后，即可关联小程序。再输入框输入小程序的AppID，点击搜索按钮，即可完成关联。 注意，只能关联公众号主体一致的小程序。就是说，公共号和小程序的主体如果是个人，则必须是同一个人；如果是企业，则必须是同一企业。 公众号关联小程序时，可勾选“关联后给已关注公众号的用户发送通知”，给粉丝下发通知消息，粉丝点击该通知消息可以打开小程序。该消息不占用原有群发条数。 1.2设置小程序自定义菜单可以在“功能管理”下“自定义菜单”下编辑，如下图所示： 填写要素有： 1.菜单名称：底部自定义菜单展示的文字； 2.跳转类型：选择“跳转小程序”Tab； 3.小程序路径：填入小程序路径 4.备用网页：旧版兼容选项。旧版微信客户端无法支持小程序时，用户点击菜单时将会打开备用网页。 2.开发模式自定义菜单创建在开发模式下，可以通过自定义菜单创建接口来创建小程序的菜单。文档地址为https://mp.weixin.qq.com/wiki下的“自定义菜单”页面。 接口地址如下： 1https://api.weixin.qq.com/cgi-bin/menu/create?access_token=ACCESS_TOKEN 请求内容示例如下： 1234567891011{ &quot;button&quot;: [ { &quot;type&quot;: &quot;miniprogram&quot;, &quot;name&quot;: &quot;小程序&quot;, &quot;appid&quot;: &quot;wx1dda1f639e823874&quot;, &quot;url&quot;: &quot;http://www.qq.com&quot;, &quot;pagepath&quot;: &quot;page/index&quot; } ]} 完整程序如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758&lt;?phpdefine('APPID', '公众号APPID ');//公众号APPIDdefine('APPSECRET', '公众号APPSECRET ');//公众号APPSECRET//$menu变量为存放菜单项的json字符串$menu = '{&quot;button&quot;: [{&quot;type&quot;: &quot;miniprogram&quot;,&quot;name&quot;: &quot;小程序&quot;,&quot;appid&quot;: &quot;请填写小程序ID&quot;,&quot;url&quot;: &quot;http://www.qq.com&quot;,&quot;pagepath&quot;: &quot;page/index&quot;}]}';$url = &quot;https://api.weixin.qq.com/cgi-bin/menu/create?access_token=&quot; . get_token();$content = curl_post($url, $menu);$ret = json_decode($content, true);if ($ret['errcode'] == 0) {//创建成功 echo 'create menu ok';} else {//创建失败 echo 'create menu fail,msg:' . $ret['errmsg'];}function curl_post($url, $post_string) { $ch = curl_init(); curl_setopt($ch, CURLOPT_URL, $url); curl_setopt($ch, CURLOPT_POSTFIELDS, $post_string); curl_setopt($ch, CURLOPT_RETURNTRANSFER, true); $data = curl_exec($ch); curl_close($ch); return $data;}function curl_get($url) { $ch = curl_init(); curl_setopt($ch, CURLOPT_URL, $url);; curl_setopt($ch, CURLOPT_RETURNTRANSFER, true); curl_setopt($ch, CURLOPT_SSL_VERIFYPEER, FALSE); curl_setopt($ch, CURLOPT_SSL_VERIFYHOST, FALSE); if (!curl_exec($ch)) { $data = ''; } else { $data = curl_multi_getcontent($ch); } curl_close($ch); return $data;}function get_token() { $url = &quot;https://api.weixin.qq.com/cgi-bin/token?grant_type=client_credential&amp;appid=&quot; . APPID . &quot;&amp;secret=&quot; . APPSECRET; $content = curl_get($url); $ret = json_decode($content, true);//{&quot;access_token&quot;:&quot;ACCESS_TOKEN&quot;,&quot;expires_in&quot;:7200} if (array_key_exists('errcode', $ret) &amp;&amp; $ret['errcode'] != 0) { return false; } else { return $ret['access_token']; }}?&gt; 欢迎加入微信开发者小密圈 微信开放平台拥有订阅号、服务号、小程序和企业号，吸引了一大批开发者，也形成了独特的生态圈。希望我们能吸收能量，交换信息，自由生长。 PC端点击链接：微信生态圈小密圈 微信扫码：","link":"/2017/03/31/weixin-mp-open-minapp-in-menu.html"},{"title":"Postman 及 Newman 使用开发指南（二）：Postman 环境变量与全局变量","text":"使用API的常见场景是开发人员有自己的本机开发（dev）环境，团队之间共用测试（staging）环境，对外提供生产（production）环境。 本文假设各种环境的链接如下： 123dev http://localhoststaging http://stagingproduction https://production 开发人员对/users接口进行调试时，如果没有环境变量，则可能要不断地输入以下链接进行替换： 123http://localhost/usershttp://staging/usershttps://production/users Postman 提供了环境变量（Environment Variable），来解决这个问题。 环境变量管理Postman 环境变量相关按钮放置在Builder页面的右上角。 点击最右的 ⚙️ 图标，可以添加、编辑、查看环境。首先，我们添加一个staging的环境，下属变量url设置为http://staging 。 再添加dev和production环境。 使用示例我们将环境切换为dev，在地址栏输入/users，查看“Code”代码： 1234GET /users HTTP/1.1Host: localhostCache-Control: no-cachePostman-Token: 9a0ae49e-8983-55f2-7585-a4ea721863da 可见，Postman 自动将解析为http://localhost。 全局变量API测试时，也常有这样的场景：首先要进行登录，登录后获取用户名，而用户名将在后续请求中使用。 Postman 提供了全局变量（Global Variable），来解决类似问题。 设置全局变量至少有三种方式来设置全局变量： Postman 全局变量的按钮放置在Builder页面的右上角，👁️ 图标。 点击“Global”对应的“Edit”即可进入管理页面： Pre-request Script Tests 其中方式2和方式3都可以用以下命令设置全局变量： 1postman.setGlobalVariable(&quot;variable_key&quot;, &quot;variable_value&quot;); 示例我们首先通过上节的方式1添加一个全局变量：leap_year_2016 我们访问 Postman 官方示例中判断年份是否为闰年的链接： 1https://postman-echo.com/time/leap?timestamp=2016-10-10 然后在Tests里添加如下代码： 12var jsonData = JSON.parse(responseBody);postman.setGlobalVariable('leap_year_2016',jsonData.leap) ; 发送请求后，由于2016年是闰年，所以全局变量leap_year_2016值应变为true。 查看一下： leap_year_2016=true，符合预期。","link":"/2017/04/21/postman-environment-variable-and-global-variable.html"},{"title":"Postman 及 Newman 使用开发指南（四）：Postman Collection 与 Runner","text":"在 Postman 里，集合（Collection）是一个很重要的概念，它可以将多个请求进行分组。在每个集合内部，又可向下细分为不同的文件夹。结构化的优点是利于后续查看和维护。 集合 Collection创建创建集合有两种方法： 在左边栏处点击新建。 保存或另存请求时，直接新建。 管理点击每个集合右侧的“&lt;”和“…”，会分别展开集合详情和集合操作页面。 集合详情介绍该集合的名称、简介（支持 Markdown 语法），并可分享、编辑、复制、导出、删除等。点击蓝色的 “Run” 按钮，会进入 Runner 页面。 集合操作则列出对集合的常用操作：分享、重命名、编辑、新增文件夹、复制、导出、删除。Pro 版还有监控和发布文档功能。 导出功能导出功能是一个非常重要的功能。 导出文件后，你可以分享给同事，在其它电脑上导入，而且可以在命令行中使用。推荐使用v2版本。 RunnerRunner 是一个对集合中请求进行测试的工具。它会检测每个请求的测试用例，最终返回结果汇总和统计数据。 有两种方式进入 Runner： 集合详情页，点击蓝色的 “Run” 按钮。 Postman 左上角 “Runner” 按钮。 进入后，可以选择一个集合，或集合中的某一文件夹进行测试。 在 Start Run 之前，可以选择环境，确定循环次数，确定发送请求的时间间隔，日志保存选项，并可以上传相关的文件。 点击 Start Run，Runner工具开始按照设置运行。 全部完成后，展示测试数据。 在此页面，可以查看汇总信息，导出结果，重试该测试。 Runner提供了批量测试的功能，Run 一下，就知道接口响应是否符合预期。更重要的是，Runner 可以在命令行运行，即可编程。借助此工具，我们就可以完成定时脚本、健康检测、冒烟测试等功能。下节，我将介绍 ”Run in Command Line“ 的工具：Newman 。","link":"/2017/04/23/postman-collection-and-runner.html"},{"title":"Postman 及 Newman 使用开发指南（一）：Postman 简介","text":"Postman 是一款帮助开发者分享、测试、文档化、监控API接口的工具。它最开始是一个 Chrome插件 ，但现在提供了 Mac、Windwos、Linux、Chrome 全平台的支持。 简介下载安装Postman 目前提供了全平台的版本，读者可以到以下地址进行下载安装。 https://www.getpostman.com/ 安装启动后，界面如下： 界面介绍顶部Show/Hide Sidebar: 显示或隐藏边栏 Runner: 打开接口测试工具Runner； Import: 导入postman数据，包括postman集合、环境变量、curl命令等文件； Open: 打开新窗口 Builder: 请求构造器，为主工作区域 Team Library：团队工作区域 SYNC：同步功能，登录后可将本地数据集上传到云端，以便备份、分享或异处使用。 Sign In：登录功能。 设置：设置，文档，官方支持中心 边栏Filter：搜索框，提供关键字检索 History：请求历史，按时间倒序 Collections：postman集合，包含多个API。这是一个很重要的概念，后续的Runner就是基于集合进行的。 Builder请求构造器，使用方式类似于浏览器。最简单的使用方式就是在地址栏输入地址，点击“Send”按钮，稍后响应窗口会显示响应结果。 postman可指定如下请求构造部分： 请求方式：GET、POST、PUT、PATCH、DELETE、COPY、HEAD、OPTIONS、LINK、UNLINK、PURGE、LOCK、UNLOCK、PROPFIND、VIEW 参数：支持可视化编辑、批量编辑 认证方式：Authorization方式，支持No Auth、Basic Auth、Digest Auth、OAuth 1.0、OAuth 2.0、Hawk Authentication、AWS Signature。其中No Auth是利用Cookie认证。 headers：设置请求头 Body：当请求为POST方式时，可以设置Body。共有四种情况： form-data：模拟表单提交 x-www-form-urlencoded：模拟ajax请求 raw：原始格式，支持多种格式，包括Text、JSON、Javascript、XML、HTML等 binary：上传文件 Pre-request Script: 提交请求之前执行的脚本。 Tests：对响应结果做测试。 postman可以查看响应结果： Body： 响应内容，支持JSON、XML美化高亮方式。 Cookies：返回Cookies Headers：响应头信息。 Tests：对响应结果的单元测试。 功能生成CodeBuilder页面的右上角，有“Cookie”、”Code“字样，点击”Code“，可以查看各种语言生成的请求代码。 目前支持大部分常用语言，包括HTTP、C、cURL、C#、Go、Java、Javascript、NodeJS、Object-C、OCaml、PHP、Python、Ruby、Shell、Swift等。 管理CookieBuilder页面的右上角，有“Cookie”、”Code“字样，点击”Cookie“，可以进入管理Cookie页面。「注意：某些版本的postman还未集成此功能，如Chrome插件」 单元测试请求一个接口后，我们可能希望这个接口能满足一定的条件，如： HTTP返回码为200 符合一定的JSON格式 响应时间少于200ms postman的tests功能可以满足此需求。 代码： 123tests[&quot;Status code is 200&quot;] = responseCode.code === 200;var jsonData = JSON.parse(responseBody);tests[&quot;2016 is a leap year&quot;] = jsonData.leap === true; 我们希望HTTP返回码为200。2016年是闰年，所以我们希望返回leap=true。 而响应窗口的Tests选项卡，显示2条测试都通过，说明接口一切正常。","link":"/2017/04/20/postman-and-newman-development.html"},{"title":"Postman 及 Newman 使用开发指南（三）：Postman Test 与接口测试","text":"API 开发、测试、维护的过程中，我们希望接口各司其职，并且一贯如此。做到这一点，我们需要检查下列选项： 网络连接是否正常 响应时间是否控制在1秒以下 HTTP 返回状态码是否为200 符合一定的JSON或XML格式 返回内容是否包含特定字符串 只有所有的选项都符合预期，我们才会认为接口正常工作。 Postman Builder页面提供了接口测试的功能。 书写测试用例的方法是，在tests数组中增加一个元素，key为测试用例的名称，value为表达式。如果执行过后表达式的值为 true ，表明通过测试；反之，未通过测试。 1tests[&quot;Body matches string&quot;] = responseBody.has(&quot;string_you_want_to_search&quot;); Postman 常见的测试用例如下： 设置环境变量 12postman.setEnvironmentVariable(&quot;key&quot;, &quot;value&quot;); 获取环境变量 12postman.getEnvironmentVariable(&quot;key&quot;); 设置全局变量 12postman.setGlobalVariable(&quot;key&quot;, &quot;value&quot;); 获取全局变量 12postman.getGlobalVariable(&quot;key&quot;); 返回内容是否包含特定字符串 1tests[&quot;Body matches string&quot;] = responseBody.has(&quot;string_you_want_to_search&quot;); 将 XML 转化为 JSON 1var jsonObject = xml2Json(responseBody); 返回内容是否为特定字符串 1tests[&quot;Body is correct&quot;] = responseBody === &quot;response_body_string&quot;; 检查 JSON 数据的某个字段是否等于特定值 123var data = JSON.parse(responseBody);tests[&quot;Your test name&quot;] = data.value === 100; 检查是否存在 Content-Type（区分大小写） 1tests[&quot;Content-Type is present&quot;] = postman.getResponseHeader(&quot;Content-Type&quot;); //Note: the getResponseHeader() method returns the header value, if it exists. 检查是否存在 Content-Type（区分大小写） 1tests[&quot;Content-Type is present&quot;] = responseHeaders.hasOwnProperty(&quot;Content-Type&quot;); 响应时间小于 200ms 12tests[&quot;Response time is less than 200ms&quot;] = responseTime &lt; 200; HTTP 返回状态码为 200 12tests[&quot;Status code is 200&quot;] = responseCode.code === 200; HTTP 返回状态码包含特定字符串 1tests[&quot;Status code name has string&quot;] = responseCode.name.has(&quot;Created&quot;); POST 请求成功的状态码 1tests[&quot;Successful POST request&quot;] = responseCode.code === 201 || responseCode.code === 202; 利用 TinyValidator 来检查 JSON 是否符合特定格式 123456789101112var schema = { &quot;items&quot;: { &quot;type&quot;: &quot;boolean&quot; }};var data1 = [true, false];var data2 = [true, 123];tests[&quot;Valid Data1&quot;] = tv4.validate(data1, schema);tests[&quot;Valid Data2&quot;] = tv4.validate(data2, schema);console.log(&quot;Validation failed: &quot;, tv4.error); 该例子的样本文件如下： JSON 文件：Download JSON file CSV 文件：Download CSV file","link":"/2017/04/22/postman-api-test.html"},{"title":"Postman 及 Newman 使用开发指南（五）：Newman 简介","text":"Newman 是 Postman 推出的一个 nodejs 库。利用 Newman，我们可以很方便地运行和测试集合，并用之构造自动化测试和持续集成。 安装运行可以使用 npm 来 newman。如果还未安装 npm，可以 点此安装 。 1npm install newman --global; 上一节中，我们将 Echo 集合导出，命名为 postman_echo.postman_collection.json。 在命令行运行如下命令即可，即可启动测试。 1newman run postman_echo.postman_collection.json 运行界面如下： 编程实现新建文件 test_echo.js，内容如下： 12345678910var newman = require('newman'); // require newman in your project// call newman.run to pass `options` object and wait for callbacknewman.run({ collection: require('./postman_echo.postman_collection.json'), reporters: 'cli'}, function (err) { if (err) { throw err; } console.log('collection run complete!');}); 然后在命令行运行如下命令： 1node test_echo.js 执行效果和直接运行 newman 完全一致。 结果导出方式Postman 提供了5种导出方法： cli：命令行 html：网页 json junit：以 xml 格式导出 teamcity：导出到持续集成（Continuous Integration）工具 TeamCity 另外 Postman 的 html 导出方式支持自定义导出模板。如果不指定模板，将采用 默认模板。 API参考Newman 的核心方法是 newman.run(options: *object* , callback: *function*) =&gt; run: EventEmitter。其中： options 为Newman运行时的设置项。 callback 为回调函数，会回传 error 和 summary 信息。 run 为函数主体，可监听事件。 options 设置项 参数 是否必需 类型 解释 options 是 object 包含 run 一个集合所需的所有信息。 options.collection 是 object|string|PostmanCollection 指定 collection。可以传递collection 对象，也可以传递 URL 或本地 JSON 文件路径。 options.environment 否 object|string 指定环境变量。可以传递“键=&gt;值“对的对象，也可以传递 URL 或本地文件路径。 options.globals 否 object|string 指定全局变量。可以传递对象，URL 或本地文件路径。 options.iterationCount 否 number，默认值为1 指定循环的次数。 options.iterationData 否 string 当循环多次时，指定作为数据源的 JSON、CSV、URL的路径。 options.folder 否 string 集合中文件夹的名称或 ID。设置此项后，只有该文件夹才会执行。 options.timeoutRequest 否 number，默认值无穷大 请求超时时间，以毫秒为单位。默认不超时。 options.delayRequest 否 number，默认值为 0 请求之间的时间间隔，以毫秒为单位。 options.ignoreRedirects 否 boolean，默认值为 false 是否允许 3xx 跳转。 options.insecure 否 boolean，默认值为 false 是否禁止 SSL 验证和允许自签名证书。 options.bail 否 boolean，默认值为 false 遇到错误时是否终止运行。 options.suppressExitCode 否 boolean，默认值为 false 是否覆盖退出码（Exit Code）。 options.reporters 否 string|array 指定 reporter。可选 cli、json、html 和 junit。指定一种report时，用 string 即可，如 cli；指定多种时，用array，如['cli','html']。 options.reporter 否 object 指定特定 reporter 的属性。例如 reporter : { junit : { export : './xmlResults.xml' } } 或 reporter : { html : { export : './htmlResults.html', template: './customTemplate.hbs' } }。 options.color 否 boolean 强制在 CLI 里使用彩色输出。 options.noColor 否 boolean 强制在 CLI 里禁止彩色输出。 options.sslClientCert 否 string public client certificate file 路径 options.sslClientKey 否 string private client key file 路径 options.sslClientPassphrase 否 string secret client key passphrase callback 是 function run 完成之后，会调用 callback 函数，会回传 error 和 summary 信息。 callback 回调 参数 类型 解释 error object 致命错误信息，如 Newman 无法处理的异常。 summary object 本次 run 的所有汇总信息 summary.error object 错误信息 summary.collection object 集合信息 summary.environment object 环境变量 summary.globals object 全局变量 summary.run object 本次 run 的信息 summary.run.stats object 统计信息 summary.run.failures array.&lt;object&gt; 失败信息 summary.run.executions array.&lt;object&gt; 每个请求的信息 run 事件 事件 描述 start 开始run collection beforeIteration 一次循环开始之前 beforeItem 一个条目开始处理之前 beforePrerequest pre-request 脚本开始执行之前 prerequest pre-request 脚本执行完毕 beforeRequest HTTP请求发送之前 request HTTP响应接收之后 beforeTest 测试脚本执行之前 test 测试脚本执行之后 beforeScript 任意脚本（测试脚本或pre-request 脚本）开始执行之前 script 任意脚本（测试脚本或pre-request 脚本）执行之后 item 一个条目执行完毕之后 iteration 一个循环执行完毕之后 assertion 测试脚本里每个测试用例被执行之后 console console方法被调用时触发 exception 脚本错误出现时触发 beforeDone run结束之前 done run结束，无论有没有错误都触发 完整参考请查看 https://github.com/postmanlabs/newman 。","link":"/2017/04/25/newman-develop-tutorial.html"},{"title":"Postman 及 Newman 使用开发指南（六）：利用 Newman 进行冒烟测试","text":"大家提到“冒烟测试”，大部分人会援引微软的定义： 在软件中，“冒烟测试”这一术语描述的是在将代码更改签入到产品的源树中之前对这些更改进行验证的过程。在检查了代码后，冒烟测试是确定和修复软件缺陷的最经济有效的方法。冒烟测试设计用于确认代码中的更改会按预期运行，且不会破坏整个版本的稳定性。 “冒烟测试”这一术语源自硬件行业。该术语源于此做法：对一个硬件或硬件组件进行更改或修复后，直接给设备加电。如果没有冒烟，则该组件就通过了测试。 对于 API 接口来说，冒烟测试就是跑一遍测试，看看所有的返回结果是否符合预期。判断一次冒烟测试是否通过的标准，可以简化一下两条： 网络请求正常 返回内容符合预期 本节我们用 Newman 编写冒烟测试案例，并生成自定义报表，附带邮件发送功能。本节的例子完整代码参考 https://github.com/spetacular/newman-smoking-test nodejs 脚本脚本用到了两个插件：Newman 和 Nodemailer 。 12npm install newman --savenpm install nodemailer --save 脚本 smoke.js: 将 --- 包含的部分换成正确的配置即可。 自定义 HTML 模板由于大部分邮件客户端只支持 inline 的 CSS 样式，所以我们将样式写到元素里。 template.hbs 文件： 最后发送邮件的效果如下： 完整代码参考 https://github.com/spetacular/newman-smoking-test","link":"/2017/04/27/postman-use-newman-for-smoking-test.html"},{"title":"PHP从数组中删除元素的四种方法","text":"茴香豆的“茴”字有四种写法，PHP从数组中删除元素也有四种方法 ^_^。 删除一个元素，且保持原有索引不变使用 unset 函数，示例如下： 12345&lt;?php $array = array(0 =&gt; &quot;a&quot;, 1 =&gt; &quot;b&quot;, 2 =&gt; &quot;c&quot;); unset($array[1]); //↑ 你想删除的key?&gt; 输出： 1234Array ( [0] =&gt; a [2] =&gt; c) 使用 unset 并未改变数组的原有索引。如果打算重排索引（让索引从0开始，并且连续），可以使用 array_values 函数： 12345678910$array = array_values($array);/*输出array(2) { [0]=&gt; string(1) &quot;a&quot; [1]=&gt; string(1) &quot;c&quot;}*/ 删除一个元素，不保持索引使用 array_splice 函数，示例如下： 12345&lt;?php $array = array(0 =&gt; &quot;a&quot;, 1 =&gt; &quot;b&quot;, 2 =&gt; &quot;c&quot;); array_splice($array, 1, 1); //↑ 你想删除的元素的Offset?&gt; 输出： 1234Array ( [0] =&gt; a [1] =&gt; c) 按值删除多个元素，保持索引使用 array_diff 函数，示例如下： 12345&lt;?php $array = array(0 =&gt; &quot;a&quot;, 1 =&gt; &quot;b&quot;, 2 =&gt; &quot;c&quot;); $array = array_diff($array, [&quot;a&quot;, &quot;c&quot;]); //└────────┘→ 你想删除的数组元素值values?&gt; 输出： 123Array ( [1] =&gt; b) 与 unset 类似，array_diff 也将保持索引。 按键删除多个元素，保持索引使用 array_diff_key 函数，示例如下： 123456&lt;?php $array = array(0 =&gt; &quot;a&quot;, 1 =&gt; &quot;b&quot;, 2 =&gt; &quot;c&quot;); $array = array_diff_key($array, [0 =&gt; &quot;xy&quot;, &quot;2&quot; =&gt; &quot;xy&quot;]); //↑ ↑ 你想删除的数组键keys?&gt; 输出： 123Array ( [1] =&gt; b) 与 unset 类似，array_diff_key 也将保持索引。","link":"/2017/05/12/delete-element-from-array-php.html"},{"title":"MySQL 1045 Access denied 和 1449 The user specified as a definer does not exist 错误处理","text":"MySQL 使用新建用户查询时，如果数据库中有 view，可能会出现这样的错误： 1SQLSTATE[28000]: Invalid authorization specification: 1045 Access denied for user 'user'@'10.174.68.21' (using password: YES) 或者 1SQLSTATE[HY000]: General error: 1449 The user specified as a definer ('db_prod'@'%') does not exist 其表现是： 涉及到 table 的查询都正常； 涉及到 view 的查询都报错。 大多数情况下，出现此问题的根源是view definer设置不当。 查看所有的 definer可以先检查下所有 view 的 definer。 123456789select DEFINER from information_schema.VIEWS; +---------------------------+| DEFINER |+---------------------------+| db_prod@% || db_prod@% || db_prod@10.174.68.21 || root@127.0.0.1 |+---------------------------+4 rows in set (0.00 sec) 可以发现，某些 view 的 definer 为db_prod@% 。 查看view的definer进而可以查看单个 view 的 definer。 12SHOW CREATE VIEW viewnameCREATE ALGORITHM=UNDEFINED DEFINER=`db_prod`@`%` SQL SECURITY DEFINER VIEW `viewname` AS select * from tablename; 创建用户时需要注意的事项如果 view 的 definer 与当前的用户不一致，可以修改或删除用户。 1234删除用户DROP USER 'db_prod'@'10.174.68.21';修改 definerUPDATE `mysql`.`proc` p SET definer = 'root@localhost' WHERE definer='root@foobar' AND db='dbname'; 在创建用户时，可以用如下 SQL. 1234创建用户CREATE USER 'db_prod'@'%' IDENTIFIED BY '123456';授权GRANT ALL ON dbname.* TO 'db_prod'@'%'; 关键是保持 DEFINER 和创建用户时的用户名完全一致。 参考资料MySql: Some queries produce: SQLSTATE28000: Invalid authorization specification: 1045 Access denied for user mysql如何修改所有的definer","link":"/2017/05/31/mysql-1045-access-denied-view-definer.html"},{"title":"Phabricator 入门教程","text":"Phabricator 是一款用于敏捷开发的项目管理软件，它集成了众多实用功能，包括： 代码管理：添加 Git, Mercurial 和 SVN 仓库，查看源码，review 代码 bug 追踪：测试人员、开发人员协同工作 项目管理：项目的启动、进展、完成 工作板：所有任务一目了然 wiki：构建文档 任务系统：创建任务、指派任务、完成任务、增加或降低优先级 博客系统：甚至可以写博客 这里介绍如何从零开始，完成 Phabricator 的安装配置，设置数据库和发送邮件配置，仅供参考。 前提条件Phabricator 的运行环境是 php，数据库采用 mysql，web服务器可选 nginx、apache、lighttpd等。 如果操作系统为 ubuntu ，希望快速安装，可以拷贝 install_ubuntu.sh 脚本，一键安装 php5、apache、mysql-server。 这里安装 php5 (包括所需扩展)、nginx、mysql-server。 12sudo apt-get updatesudo apt-get install php5 php5-mysql php5-gd php5-dev php5-curl php-apc php5-cli php5-json nginx mysql-server Tips：PHP 7 的兼容性如果你的环境是PHP 7.0，那么恭喜你，Phabricator 无法运行在 PHP 7.0 下。 Phabricator 支持 5.x 版本以及 7.1 以上版本，唯独不支持 7.0。 据官方声明，Phabricator 依赖的异步信号处理特性，在 7.0 版本被移除了，但是在7.1 版本又加上了。 使用 PHP 7.0 的用户，可以降级到 PHP 5 或升级到 PHP 7.1，或者用 Docker。 安装Phabricator 需要 3 个仓库的代码：arcanist、libphutil、phabricator。其中phabricator是项目的主体，arcanist 是 Phabricator 的命令行工具，libphutil 是Phabricator 的实用工具集，包括文件系统、markdown解析、守护进程等。 假设 webroot 目录为 /var/www，则执行如下命令： 12345cd /var/wwwmkdir phabricator &amp;&amp; cd $_git clone https://github.com/phacility/libphutil.gitgit clone https://github.com/phacility/arcanist.gitgit clone https://github.com/phacility/phabricator.git 保证 arcanist、libphutil、phabricator 目录为并列关系。 12root@localhost:/var/www/phabricator# lsarcanist libphutil phabricator 配置 nginx： 其中 root 指向 phabricator 目录的 webroot。 12345678910111213141516171819202122232425262728293031server { server_name phabricator.example.com; root /var/www/phabricator/phabricator/webroot; location / { index index.php; rewrite ^/(.*)$ /index.php?__path__=/$1 last; } location /index.php { fastcgi_pass localhost:9000; fastcgi_index index.php; #required if PHP was built with --enable-force-cgi-redirect fastcgi_param REDIRECT_STATUS 200; #variables to make the $_SERVER populate in PHP fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; fastcgi_param QUERY_STRING $query_string; fastcgi_param REQUEST_METHOD $request_method; fastcgi_param CONTENT_TYPE $content_type; fastcgi_param CONTENT_LENGTH $content_length; fastcgi_param SCRIPT_NAME $fastcgi_script_name; fastcgi_param GATEWAY_INTERFACE CGI/1.1; fastcgi_param SERVER_SOFTWARE nginx/$nginx_version; fastcgi_param REMOTE_ADDR $remote_addr; }} 配置数据库首先要设置用于 Phabricator 连接的数据库配置。 1234cd /var/www/phabricator/phabricatorphabricator/ $ ./bin/config set mysql.host localhostphabricator/ $ ./bin/config set mysql.user rootphabricator/ $ ./bin/config set mysql.pass 123456 然后执行： 1phabricator/ $ ./bin/storage upgrade 邮件Phabricator 支持的邮件系统很多，如Mailgun、Amazon SES、SendGrid、SMTP。这里以 SMTP 为例。 有两种方式配置邮件发送。 1. 配置文件修改 conf/local/local.json ，加入如下配置： 1234567{ &quot;phpmailer.smtp-protocol&quot;: &quot;SSL&quot;, &quot;phpmailer.smtp-password&quot;: &quot;密码&quot;, &quot;phpmailer.smtp-user&quot;: &quot;邮箱地址&quot;, &quot;phpmailer.smtp-port&quot;: 端口号, &quot;phpmailer.smtp-host&quot;: &quot;邮件服务器&quot;} 2. 命令行用 bin/config 命令进行设置，如： 12345phabricator/ $ ./bin/config set phpmailer.smtp-protocol sslphabricator/ $ phabricator/ $ ./bin/config set phpmailer.smtp-password 密码phabricator/ $ ./bin/config set phpmailer.smtp-user 邮箱地址phabricator/ $ ./bin/config set phpmailer.smtp-port 端口号phabricator/ $ ./bin/config set phpmailer.smtp-host 邮件服务器 验证配置完成后，可以用命令行测试邮件发送配置是否正确： 123phabricator/ $ ./bin/mail list-outbound # 列出所有邮件phabricator/ $ ./bin/mail show-outbound # 显示某条邮件phabricator/ $ ./bin/mail send-test # 发送测试邮件 例如给 david 发送 主题为 hello 的邮件，正文在 body.txt 里： 12345phabricator/ $ ./bin/mail send-test --to david --subject 'hello' &lt; ./body.txtReading message body from stdin...Mail sent! You can view details by running this command: phabricator/ $ ./bin/mail show-outbound --id 37773 执行 ./bin/mail show-outbound --id 37773 可以看到此处发送的调试信息。 12345678910111213PROPERTIESID: 37773Status: sentRelated PHID:Message:PARAMETERSsensitive: 1cc: []subject: hellois-bulk:mailtags: []............... 这些命令对调试邮件配置和开发新的邮件适配器 (Adapter) 很有用处。 #结语 至此，Phabricator 可以使用了。更详细的功能和配置请参考 Phabricator User Documentation","link":"/2017/10/19/phabricator-install-and-config.html"},{"title":"Git webhook 实现自动部署教程","text":"Git Hook(钩子) 是 Git 在代码提交、推送、合并等工作流程中引入的事件触发器，其中最常用的场景是代码检查，持续集成，自动部署等。本文主要讲解一下利用 Git webhook 实现自动部署。 一、Git hookGit 的 hook 分为本地仓库 hook 和服务器仓库 hook。 本地 hook本地 hook 通常在代码的 .git/hooks 目录下，如下所示： 12345$ hooks git:(master) lsapplypatch-msg.sample pre-commit.sample prepare-commit-msg.samplecommit-msg.sample update.samplepost-update.sample pre-push.samplepre-applypatch.sample pre-rebase.sample 默认情况下，这些脚本不会生效。使用时，只需将 .sample 后缀去掉，然后赋予脚本执行权限即可。 本地 hook 主要用于代码静态分析、REVIEW、代码规范、命名规范等。 pre-commit 提交之前的代码检查，包括是否通过单元测试，静态代码分析 prepare-commit-msg 提交信息之前，可用来生成默认的提交信息 commit-msg 提交信息之后，可用来检查提交信息是否符合特定的格式 post-commit 提交代码之后，一般用来通知代码已提交。 post-checkout checkout 代码之后，可用来设置工作目录、生成文档、生成静态资源等工作 post-merge 合并代码之后，可用来保存 merge 操作中，git 没有保存的信息。 pre-push push 代码之前，可用来检查本次 push 的 commits 是否符合特定的标准。 服务器 hook服务器 hook 指代码传输到服务器时，在服务器端所做的一系列操作。 pre-receive 处理 push 操作之前，可以检查本次 push 的 commits 和文件是否符合特定的标准。 update update 与 pre-receive 操作类似，不同的是 pre-receive 只执行一次，而 update 可能执行多次。 post-receive 整个提交过程完成之后，可用于更新其它服务或通知用户，比如发邮件告诉开发人员已提交代码，通知持续集成 (Continuous Integration) 服务器部署代码 webhook如果 Git 服务部署在自己的服务器上，如用 GitLab 搭建一套 Git 服务，则可以使用服务器 hook。如果使用了 GitHub、Bitbucket 等云端平台，那么只能使用 webhook 来完成脚本。 webhook 本质上属于服务器 hook，因为发送通知的方式是网络请求，因此得名。使用 webhook 的步骤如下： 设置用于接收请求的 URL。 服务器收到 push、pull request、merge、tag 等操作时，会将相应信息发送给步骤 1 里的 URL。 URL 对应的程序收到网络请求后，执行自动部署、邮件通知等操作。 二、部署 webhook环境假设为叙述方便，我们做如下假设。 项目 值 域名 www.weixinbook.net 接收请求的URL https://www.weixinbook.net/webhook.php 项目部署目录 /var/www/weixinbook build.sh路径 /var/www/hooks/build.sh webhook.php /var/www/weixinbook/webhook.php 环境 nginx php 用户组 www-data Git 项目地址Git 项目需要使用 SSH 地址，如 git@github.com:xxx/xxx.git 。如果之前采用了 HTTPS 链接，需要修改 .git/config 文件里的 url 字段： 12345vi .git/config[remote &quot;origin&quot;] url = git@github.com:xxx/xxx.git fetch = +refs/heads/*:refs/remotes/origin/* 部署 SSH 无密码登录首先需要生成 SSH key。 如果使用 nginx: 1sudo -u www-data ssh-keygen -t rsa -C &quot;nginx&quot; 如果使用 apache: 1sudo -u apache ssh-keygen -t rsa -C &quot;apache&quot; 然后复制你的 public key，粘贴到项目的“SSH 公钥”设置里。 1cat ~/.ssh/id_rsa.pub 以 GitHub 为例，在 Settings -&gt; Deploy keys，选择 Add deploy key. 添加完毕后，可以运行 ssh -T git@github.com 来验证是否设置成功。首次运行时会看到一条 RSA key 指纹的连接确认信息，输入 yes 回车即可。 如果看到下面的信息，就说明设置生效了。 12Hi username! You've successfully authenticated, but GitHub does notprovide shell access. webhook.phpwebhook.php 用来接收 webhook 请求，一般要做以下操作： 监测请求来源合法性。检查方法：密码，AES等。 检查是否为特定分支，如 master 分支。 启动自动部署脚本，如完成拉取最新代码，重新设置目录权限等。 12345678910111213&lt;?php$raw_data = file_get_contents(&quot;php://input&quot;);$pay_load = json_decode($raw_data,true);//监测请求来源合法性，if($pay_load['password'] != 'pass'){ exit('ok 400');}//检查是否为master分支if($pay_load[&quot;ref&quot;] != &quot;refs/heads/master&quot;){ exit('ok 401');}exec('sh /var/www/hooks/build.sh'); build.shbuild.sh 用来执行自动部署的具体操作。一般情况下，不要放在外网可访问的 webroot 目录下。 12345678910#! /bin/bashSITE_PATH='/var/www/weixinbook'USER='www-data'USERGROUP='www-data'cd $SITE_PATHgit reset --hard origin/mastergit clean -fgit pullgit checkout masterchown -R $USER:$USERGROUP $SITE_PATH 这样每次在客户端 push 代码，服务器会发请求给 webhook.php，webhook.php 检查通过后，启动 build.sh 进行自动部署。 三、结语自动部署并不难，如果失败，大部分是权限问题。解决的方法很简单： 保证 ssh key 的用户为 www-data 保证 build.sh , webhook.php 以及代码目录的用户为 www-data. 权限问题解决不了，不要草率地全用 root，这样会带来一定的安全风险。","link":"/2017/10/31/git-webhooks-for-php.html"},{"title":"贝叶斯定理、拼写纠正、生日悖论","text":"本周天气非常好。 一、贝叶斯定理贝叶斯定理解决的是，在事件B已经发生的情况下，事件A发生的概率。 最重要的公式有两个: 贝叶斯公式 全概率公式 直观的解释就是，无论是事件B发生情况下，事件A发生，或者\b事件A发生情况下，事件B发生，其\b结果都是事件A和事件B同时发生了。反映到文氏图上，就是 A∩B 。 二、拼写纠正拼写纠正是一个很常见的\b功能，比如把 config 拼写成 conf，软件提示\b输错了，并推荐正确\b写法。 12345678910111213141516171819git confgit: 'conf' is not a git command. See 'git --help'.Did you mean this? config``` 此类问题有很多解决方法，贝叶斯是其中之一。贝叶斯方法的思路是，在输入conf的情况下，计算正确值是config或其它\b单词的概率是多少，并找出概率最大的单词。其中的数学原理并不能，但是要解决两个问题：1.\b寻找备选项。如果conf可能是错误的，那么如何找到与conf\b类似的词如confs、config、confidence等 2.如何确认备选项的概率\b对于问题1，简单的方法是\b通过编辑距离找出相近的单词，如增加、减少、替换、更换位置等，找到所有的备选项。对于问题2，简单的方法是找一本英文书，分离出所有的单词，计算每个单词出现的次数。我做了个简单的demo。首先有一个单词表的文件 `words.txt`，列出一本书中出现的所有单词。示例如下： thethathe 1另外用于检查拼写的文件： package main import ( “bufio” “fmt” “log” “os”) var model = make(map[string]int)var alphalet = “abcdefghijklmnopqrstuvwxyz”//找到所有编辑距离为1的单词集合func edit1(word string) []string { s := make([]string, 1) wordLen := len(word) //add 1 word for i := range word { for _, c := range alphalet { s = append(s, word[0:i]+string(c)+word[i:]) } } //delete 1 word for i := range word { s = append(s, word[0:i]+word[i+1:]) } //change 1 word for i := range word { for _, c := range alphalet { s = append(s, word[0:i]+string(c)+word[i+1:]) } } // change position for i := range word[0 : wordLen-1] { s = append(s, word[0:i]+string(word[i+1])+string(word[i])+word[i+2:]) } return s } //计算所有的候选单词func candidates(word string) []string { s := make([]string, 1) s = append(s, word) s1 := append(s, edit1(word)…) return s1 } //计算最优解func calc(word string) string { words := candidates(word) var maxProb = 1 var correctWord = word for _, w := range words { prob, prs := model[w] if prs == true { if prob &gt; maxProb { maxProb = prob correctWord = w } } } return correctWord } func main() { file, err := os.Open(“./words.txt”) if err != nil { log.Fatal(err) } defer file.Close() scanner := bufio.NewScanner(file) for scanner.Scan() { word := scanner.Text() model[word]++ } fmt.Println(calc(&quot;tae&quot;)) if err := scanner.Err(); err != nil { log.Fatal(err) } } 1234567891011当输入 `tha` 时，会自动纠正为 `the`。# 三、生日悖论在23人中，有2人生日相同的概率大于50%；若有50人，则概率增至97。这可能与人们的直觉相悖，因为一年有365天，两人\b同天生日，通常就是人们常说的“太巧了”。但经过[计算](https://zh.wikipedia.org/wiki/%E7%94%9F%E6%97%A5%E5%95%8F%E9%A1%8C#%E6%A6%82%E7%8E%87%E4%BC%B0%E8%AE%A1)，发现其概率要高得多。![](/images/2018/birthday-probability.svg)我写了\b一段代码来模拟一下，\b从1到365的数字中，随机取出2个数，\b算出2个数相同的概率。 package mainimport “fmt”import “time”import “math/rand” //return a random number of 1~365func generateBirth() int{ rand.Seed(time.Now().UnixNano()) return 1+rand.Intn(364)} //check if at least 2 persons have same birthdayfunc checkSameBirthDay(num int) bool{ brithDays := make(map[int]int) for i:=0;i&lt;num;i++{ day := generateBirth() brithDays[day]++ if(brithDays[day] &gt;1){ return true } } return false} func main(){ personNum := 50 loopNum := 10000 sameNum := 0 for i:=0;i&lt;loopNum;i++{ if true == checkSameBirthDay(personNum){ sameNum++ } } fmt.Printf(“The probability of same birthday in %d persons is %f\\n”,personNum,float64(sameNum)/float64(loopNum))} 123当人数为50人，结果如下： go run birthday.goThe probability of same birthday in 50 persons is 0.973000 与计算的值相差无几。 # 四、参考链接 [贝叶斯推断及其互联网应用（一）：定理简介](http://www.ruanyifeng.com/blog/2011/08/bayesian_inference_part_one.html) [哈希碰撞与生日攻击](http://www.ruanyifeng.com/blog/2018/09/hash-collision-and-birthday-attack.html) [维基百科：生日问题](https://zh.wikipedia.org/wiki/%E7%94%9F%E6%97%A5%E5%95%8F%E9%A1%8C) [ 朴素贝叶斯案例2：拼写纠错（python实现）](https://blog.csdn.net/PbGc396Dwxjb77F2je/article/details/78786980)","link":"/2018/09/08/bayesian-spelling-check-birthday-problem.html"},{"title":"01背包问题","text":"一、定义01背包问题是在资源有限的情况下，实现收益最大化的一类问题。经典的场景是探险家偶然进入一片宝藏，每一件宝贝都是独一无二的，具有自己的价值和重量。由于背包容量有限，只能选择某几样宝贝。此类问题归属于 动态规划。 二、实现直观来看，这是一个如何选择的问题。对一个宝贝i来说，是拿还是不拿，这是一个问题。01背包问题的公式如下： 选择i的收益是 123宝贝i的价值+为了选择i，背包容量减少为背包容量 - 宝贝i的重量，这时能装的宝贝的价值 不选择i的收益是 1上一次选完宝贝后总的价值。即就当做没看见 i。 代码实现见 01packege 三、一点感触01背包问题，归根结底是一个选择问题。选择了一个物品，获取价值的同时必然要承担它的负重，也会放弃一些选择其它物品的机会。程序里选错了可以重选，人生里选错了就不能重来。 \b四、参考链接动态规划：背包问题 动态规划 之 0-1背包问题 动态规划之01背包问题（最易理解的讲解）","link":"/2018/09/21/01-package-problem.html"},{"title":"欧几里得算法及其扩展,RSA 算法","text":"一、前言2018年9月24日，阿蒂亚爵士完成黎曼猜想证明的演讲（黎曼猜想证明现场：3分钟核心讲解 提问陷沉默）。证明正确与否自有专业人士来解读，但其中涉及到的一些数学知识很有意思。对整数做因数分解是很困难的事情，所以人们把两个大质数相乘的乘积公开作为加密密钥，即RSA算法的原理。而RSA算法又使用到欧几里得算法的扩展，记一下。 二、欧几里得算法欧几里得算法，又名辗转相除法，此算法可以求两个整数的公约数。例如 a = 1251 和 b = 390 两个数，计算过程如下： 12345678a b mod1251 390 81390 81 6681 66 1566 15 615 6 36 3 0The highest common divissor between 1251 and 390 is : 3 代码实现见 辗转相除法 三、扩展欧几里德算法扩展欧几里得算法 是这样一个问题： 其中 gcd(a,b) 是a和b的最大公约数，求解x和y，使等式成立。 如对 43x+17y=1 的求解过程如下： 12343 = 17 x 2 + 917 = 9 x 1 + 89 = 8 x 1 + 1 将上述等式倒过来： 1239 = 8 x 1 + 117 = 9 x 1 + 843 = 17 x 2 + 9 为实现 ax + by = 1，取值如下： 1234a b x y8 1 0 19 8 1 -117 9 -1 2 将等式写为与原等式类似的格式： 1239 + 8 x (-1) = 1 等式 117 + 9 x (-1) = 8 等式 243 + 17 x (-2) = 9 等式 3 将等式 2 和等式 3 代入到等式 1 中： 12343 + 17 x (-2) + (17 + 9 x (-1)) x (-1) = 143 + 17 x (-2) + (17 + (43 + 17 x (-2)) x (-1)) x (-1) = 143 x 2 + 17 x (-5) = 1 可以得到最终的结果如下： 1243 17 2 -5Result is : x=2 , y=-5 代码实现见 扩展欧几里得算法 四、RSA算法RSA算法 是一种非对称加密算法，主要有3个作用：加密、解密和签名。得到公钥和私钥的过程，在 RSA算法原理（二） 里有详细的阐述。我用go实现的demo如下： 123456789101112131415161718func rsa() (int, int, int) { //第一步，随机选择两个不相等的质数p和q。 p := 3 q := 11 //第二步，计算p和q的乘积n。 n := p * q //第三步，计算n的欧拉函数φ(n)。 phi := (p - 1) * (q - 1) //第四步，随机选择一个整数e，条件是1&lt; e &lt; φ(n)，且e与φ(n) 互质。 e := 3 //第五步，计算e对于φ(n)的模反元素d。 d, _ := exgcd(e, phi) if d &lt; 0 { d = phi + d } //第六步，将n和e封装成公钥，n和d封装成私钥。 return n, e, d} 代码实现见 RSA算法示例。 输出结果类似于如下： 1234public key = 3,private key = 7,length = 33Origin Message = 24Encoded Message = 30Decoded Message = 24 以上是原理实现的demo，这里的公钥和私钥都是很小的质数。如果试着把p和q放大，会发现加密和解密时的复杂度快速提高，甚至int64类型的数字溢出了。应用到实际上，公钥和私钥都是很大的质数，一般是1024位，重要场合则为2048位。 五、参考链接世界上最早的算法：辗转相除法（求两个自然数最大公约数 扩展欧几里得算法视频 轻松学习RSA加密算法原理 RSA算法原理（一） RSA算法原理（二）","link":"/2018/09/26/rsa.html"},{"title":"Javascript整型数字精度限制","text":"在做一个项目时，需要生成一个长整数Id，生成函数如下： `$Id = date('YmdHis') . rand(1, 100000);` 按照时间加随机数的形式，可以直观地看到这个Id是何时生成的。 这样，我用PHP生成了这样一个数 $id = '2014112010185143423'; 然后将值传给JS变量 var id = &lt;?php echo $id;?&gt;; 但是在使用时，发现id竟然变了。 猜想可能是溢出，把dialogid给换成字符串，一切就OK了。 var id = '&lt;?php echo $id;?&gt;'; 为啥动了我的数？原来JS采用IEEE 754定义的双精度浮点数。当有效位数超过 52 位时，会存在精度丢失。 IEEE 754表示的64位双精度数据的格式如下，其中52位的Significand是无符号整数。你可以将Significand左移或右移，移动的位数保存在Exponent里，11位可以表示+(2^10 -1) ~ -(2^10 -1)的范围。Sign表示正负。 TotalbitsExponentSignificand 6411152 这就是说，当Exponent起作用时，精度会丢失。即：左移时，数字末位补零；右移时，数字末位丢失。 回到前面提到的数字Id，数字大于2^53-1，表示该数时，相当于将Significand左移，只能将末位补零。 另外，js的Number提供了最大无误差的数字： Number.MAX_SAFE_INTEGER is 9007199254740991 (2^53−1). 参考资料： http://blog.vjeux.com/2010/javascript/javascript-max_int-number-limits.html http://lifesinger.wordpress.com/2011/03/07/js-precision/","link":"/2015/01/20/js-int-limit.html"},{"title":"HTTPS 简明教程","text":"Table of Contents 前言 第 1 节 什么是HTTPS SSL 介绍 HTTP 和 HTTPS 的比较 HTTPS 的优势 第 2 节 SSL 的工作原理 非对称密码学 对称密码学 SSL 上的数据传输 SSL 握手 数据传输 公共密钥基础架构 第 3 节 什么是 SSL 证书? X.509 SSL 证书的类型 基于验证级别的 SSL 证书类型 域验证证书（Domain Validated Certificates） 组织验证证书（Organization Validated Certificates） 扩展验证证书（Extended Validated Certificates） 基于域名数目的 SSL 证书类型 单域名证书（Single Domain Certificate） 通配符证书（Wildcard SSL Certificate） 统一SSL证书/多域SSL证书/ SAN证书（Unified SSL Certificate /Multi-Domain SSL Certificate/SAN Certificate） 第 4 节 SSL 证书格式 PEM 格式 PKCS#7 格式 DER 格式 PKCS#12 格式 结语 参考文献 前言越来越多的网站使用 HTTPS 来确保安全。HTTPS 使用 SSL/TSL 技术来加密两个系统之间的通信。这个教程将帮助读者逐步深入地理解 HTTPS 。本教材会分为几个章节，每个章节包含若干相关的主题。每个主题包含浅显易懂的解释和来自现实生活的例子。 本教材可供想了解 HTTPS 的原理、了解如何在网站使用 HTTPS 的初学者和专业人士参考。 第 1 节 什么是HTTPSHTTPS 代表 Hyper Text Transfer Protocol Secure（超文本传输安全协议），它是一种为两个系统（例如浏览器和 web 服务器）之间的通信提供保护的协议。 下面的图片表明了 HTTP 和 HTTPS 的通信过程的差别： HTTP 和 HTTPS 的通信过程 如上图所示，[HTTP](https://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol) 在浏览器和 web 服务器之间传输数据时，使用了明文的超文本格式；而 HTTPS 采用了加密的格式。HTTPS 阻止了攻击者在传输过程中来查看、修改数据的攻击行为。即使攻击者截获了通信消息，他们也无法使用，因为报文被加密了。 HTTPS 在浏览器和 web 服务器之间建立了加密连接，使用的技术是 Secure Socket Layer (SSL) 或 Transport Layer Security (TLS) 协议。TLS 是 SSL 的新版本。 SSL 介绍SSL 是两个系统之间建立加密连接的标准安全技术。适用范围包括浏览器到服务器、服务器到服务器、客户端到服务器等。SSL 保证了两个系统传输的数据保持加密和私密。 HTTPS 实质上是 HTTP over SSL。SSL 建立加密连接的方式，是使用一种电子证书 — SSL 证书。 SSL ## HTTP 和 HTTPS 的比较 http https 数据以超文本格式传输 数据以加密格式传输 默认端口 80 默认端口 443 不安全 使用 SSL 安全技术 以 http:// 开头 以 https:// 开头 HTTPS 的优势 安全通信: HTTPS 通过在两个系统之间建立加密连接来保证安全连接。 数据完整性: HTTPS 通过加密数据来保证数据完整性。即使攻击者捕获了数据，他们也无法查看或修改它。 隐私和安全: HTTPS 通过组织攻击者监听浏览器和服务器之间的通信过程，来保护网站用户的隐私和安全。 更快的性能: HTTPS 比 HTTP 具有更快的数据传输速率，因为数据进行了加密从而使体积减少。 搜索引擎优化: 使用 HTTPS 可以增加搜索引擎优化（SEO）的排名。在 Google Chrome 浏览器，如果使用 HTTP，网页会被标记为“不安全”。 未来: HTTPS 代表了 web 的未来，因为它使互联网更安全。 第 2 节 SSL 的工作原理HTTPS 使用 SSL 协议来传输加密数据。我们来看下 SSL 的工作原理。SSL 使用以下概念： 非对称密码学 对称密码学 非对称密码学非对称密码学（又称为非对称加密或公钥加密）使用数学相关的密钥对来加密和解密数据。在密钥对中，其中一个密钥可以分享给任何人，这个密钥称为公钥（Public Key）；另外一个需要保持私密，称为私钥（Private Key）。 在非对称加密中，可以使用私钥对数据进行签名，该私钥只能使用成对的相关公钥验证签名。 非对称加密原理图 SSL 使用非对称加密来发起通信，这被称为 SSL 握手。 最常用的非对称密钥加密算法包括EIGamal，RSA，DSA，椭圆曲线技术和 PKCS。 对称密码学在对称密码学里，只有一个密钥来加密和解密数据。发送者和接收者都拥有这个密钥，并且应该只限于他们拥有，不能泄漏给第三方。 发起握手之后，SSL 利用会话密钥（session key）来使用对称加密。使用最广泛的对称加密算法是 AES-128，AES-192 和 AES-256。 对称加密原理图 ## SSL 上的数据传输 SSL 使用非对称和对称加密来安全地传输数据。下图表明了SSL 通信的步骤: SSL 通信 如上图，SSL 通信被分为两个步骤：SSL 握手和实际的数据传输。 SSL 握手SSL 上的通信通常由 SSL 握手开始。SSL 握手是一种非对称加密，允许浏览器在开始实际数据传输之前验证 web 服务器，获得公钥，建立安全连接。 下图表明了 SSL 握手的步骤： SSL 握手 我们说明一下上图的步骤： 客户端发送 “client hello” 消息，其中包括客户端的 SSL 版本号，密码设置，会话专用数据以及服务器使用 SSL 与客户端通信需要的其它信息。 服务器以 “server hello” 消息响应，其中包括服务器的 SSL 版本号，密码设置，会话专用数据，带有公共密钥的 SSL 证书以及客户端使用 SSL 与服务器通信需要的其它信息。 客户端从 CA（证书颁发机构）验证服务器的 SSL 证书并鉴定服务器。如果身份鉴定失败，则客户端将拒绝 SSL 连接并抛出异常。如果认证成功，将继续执行步骤 4。 客户端创建会话密钥，并使用服务器的公共密钥对其进行加密，然后将其发送到服务器。如果服务器已请求客户端身份鉴定（主要是在服务器到服务器的通信中），则客户端会将自己的证书发送到服务器。 服务器使用其私钥解密会话密钥，并将确认消息发送给使用该会话密钥的客户端。 这样在 SSL 握手结束时，客户端和服务器都具有有效的会话密钥，它们将用于加密或解密实际数据。此后将不再需要公钥和私钥。 数据传输现在客户端和服务器使用一个共享的会话密钥来加密和解密实际的数据，并传输之。由于两端使用同样的会话密钥，所以这是一个对称加密。实际数据传输采用对称加密，是因为对称加密的 CPU 消耗比非对称加密更少。 SSL 数据传输 这样，SSL 使用了非对称加密和对称加密。在现实生活中，实现 SSL 通信的一些基础架构，称为“公共密钥基础架构”（Public Key Infrastructure）。 公共密钥基础架构公共密钥基础架构（public key infrastructure (PKI)）是创建、管理、分发、使用、储存、废弃电子证书和管理公钥加密的一系列角色、策略、程序的集合。 PKI 包括以下元素： 证书颁发机构: Certificate Authority，简称 CA。这个机构负责鉴定个人、计算机和其它实体的身份。 注册机构: Registration Authority。CA 的一个子机构，代表根证书颁发机构，颁发证书用于特殊用途。 SSL 证书: SSL Certificate。SSL 证书是一个包含公钥和其它信息的数据文件。 证书管理系统: Certificate Management System。这个系统负责存储、校验、废弃证书。 第 3 节 什么是 SSL 证书?SSL 证书，又叫作电子证书，在两系统的安全通信中扮演着重要的角色。 SSL 证书是由 CA 颁发的数据文件。如上所述，SSL 使用非对称加密，利用一个密钥对（公钥和私钥）来建立两个系统之间的加密连接。SSL 证书包含所有者的公钥和其它细节信息。web 服务器通过 SSL 证书将公钥发送给浏览器，而浏览器则使用 SSL 证书来验证和鉴定web 服务器。 读者可以打开任何 HTTPS 网站的证书。例如，在 Google Chrome 浏览器里打开 https://www.google.com 来检查 google.com 的SSL 证书。如下图所示，任何 HTTPS 网站在地址栏都有一个锁的图样。 浏览器中的 HTTPS 点击锁图样，然后在弹层点击证书，如下图所示： 打开 SSL 证书 打开的证书窗口如下图所示。 SSL 证书 在“通用（General）” Tab 下，显示了证书的签发对象、签发者和有效起止时间。“细节（Details）” Tab 下包括其它细节信息。“证书路径（Certificate Path）” 包含了所有中间证书和根 CA 证书的信息。 X.509X.509 是定义电子证书格式的标准。 SSL 使用 X.509 格式。换句话说，SSL 证书实际上是 X.509 证书。 X.509 使用 Abstract Syntax Notation One (ASN.1) 的格式化语言来表示证书的数据结构。 X.509 格式的 SSL 证书包括以下信息： 版本：Version。证书使用的数据格式采用 X.509 的哪个版本。 序列号：Serial number。CA 为每个证书指定的唯一标识符。 公钥：Public Key。所有者的公钥。 主题：Subject。所有者的姓名、地址、国家和域名。 签发者：Issuer。签发该证书的 CA 的名字。 生效时间：Valid-From。证书开始生效的日期，在此之前无效。 过期时间：Valid-To。证书过期的日期，在此之后无效。 签名算法：Signature Algorithm。创建签名所使用的算法。 指纹：Thumbprint。证书的哈希值。 指纹算法：Thumbprint Algorithm。创建证书哈希值所使用的算法。 SSL 证书的类型根据验证级别和域名数目，SSL 证书有多种类型可以选用。所有类型的证书，其加密级别都相同，但验证级别却不同。 基于验证级别的 SSL 证书类型网站使用 SSL 证书来与访客建立起信用等级（Trust Level）。不同的企业需要不同的信用等级。例如，收集用户重要信息的网站需要安全的传输。金融机构需要验证域真实性以及数据安全性。因此，CA 需要根据信用等级来验证网站所有者的信息。以下三种证书是基于验证级别的： 域验证证书（Domain Validated Certificates）域验证（Domain Validated ，简写DV ）证书要求最低的验证级别，因为 DV 证书的主要用途是保护域名对应的 web 服务器和浏览器之间的通信安全。CA 只验证所有者是否拥有域名的控制权。 组织验证证书（Organization Validated Certificates）组织验证（Organization Validated ，简写OV ）证书要求中等级别的验证等级。在此等级下，CA 不但检查使用该域名的组织的权限，而且会检查组织的信息。OV 证书增强了组织和域名的信用等级。 扩展验证证书（Extended Validated Certificates）扩展验证（Extended Validated ，简写EF ）证书要求高级别的验证等级。在此级别下，CA 根据准则对组织进行严格的背景检查。这包括对实体的法律、物理、经验状况的验证。 基于域名数目的 SSL 证书类型根据受保护的域名的数目，证书可以分为以下几类。 单域名证书（Single Domain Certificate）单域名证书只保护一个完整域名。例如针对 www.example.com 的单域名证书，不能用于 mail.example.com。 通配符证书（Wildcard SSL Certificate）通配符证书可以保护一个域名的无限数量的子域名。例如一个针对 example.com 的通配符证书，也可用于 mail.example.com、blog.example.com 等。 统一SSL证书/多域SSL证书/ SAN证书（Unified SSL Certificate /Multi-Domain SSL Certificate/SAN Certificate）统一 SSL 证书使用 SAN 扩展之后，可以至多保护 100 个使用相同证书的域名。它专门用于保护 Microsoft Exchange 和 Office Communication 的通信环境。 选择了合适的 SSL 证书类型之后，需要从知名 CA 购买证书。此操作与技术关系不大，不再赘述。如果读者想安装免费 SSL 证书，请参考笔者以前的博文 《教程：配置 Let’s Encrypt 免费 HTTPS 证书》。 第 4 节 SSL 证书格式前文讲过，SSL 证书使用 X.509 格式。X.509 证书有不同的格式，例如PEM, DER, PKCS#7 和 PKCS#12。PEM 和 PKCS#7 格式使用 Base64 ASCII 编码，DER 和 PKCS#12 使用二进制编码。根据不同的格式和编码方式，证书文件有不同的扩展名。 下图说明了 X.509 证书的编码格式和文件扩展名。 SSL 证书格式 ## PEM 格式 大多数 CA 提供格式为 PEM 、文件为 Base64 ASCII 编码的证书。证书文件类型有 .pem, .crt, .cer, 和 .key。 .pem 文件将服务器证书、中间证书和私钥包含在单个文件里。 .crt 和 .cer 文件分别保存服务器证书和中间证书。 .key 文件保存私钥。 PEM 文件使用 ASCII 编码，因为读者可以使用任意文本编辑器打开它们。 PEM 文件中的每个证书包含在 ---- BEGIN CERTIFICATE---- 和 ----END CERTIFICATE---- 之间。 私钥文件包含在 ---- BEGIN RSA PRIVATE KEY----- 和 -----END RSA PRIVATE KEY----- 之间。 CSR 包含在 -----BEGIN CERTIFICATE REQUEST----- 和 -----END CERTIFICATE REQUEST----- 之间。 PKCS#7 格式PKCS#7 格式是加密消息语法标准（Cryptographic Message Syntax Standard）。PKCS#7 证书使用 Base64 ASCII 编码，文件扩展名为 .p7b 或 .p7c。只有证书以此格式保存，不包括私钥。P7B 证书包含在 -----BEGIN PKCS7----- 和 -----END PKCS7----- 之间。 DER 格式DER 证书是二进制格式，存储在 .der 或 .cer 文件里。这些证书主要用于 Java 开发的 web 服务器中。 PKCS#12 格式PKCS#12 证书是二进制格式，存储在 .pfx 或 .p12 文件里。 PKCS#12 将服务器证书、中间证书和私钥包含在单个 .pfx 后缀、用密码加密保护的文件里。这些证书主要用于 Windows 平台。 CA 提供以上任何格式的证书。 结语本文主要讲解了 HTTPS 的原理、SSL 证书的工作原理、SSL 证书的分类和格式，可以作为入门教程。读完本文，相信读者会对 HTTPS 有一个概览式的了解。读者有兴趣的读者可以研究密码学，有需要安装证书的读者可以自行搜索安装方法。 参考文献本文为笔者处于学习目的而翻译，原文在 HTTPS Tutorials 。","link":"/2019/09/23/https-tutorials.html"},{"title":"Scala 简明教程","text":"Table of Contents Scala 简介 安装与配置 安装 Java 安装 Scala 安装 sbt Hello World 交互模式 编译模式 构建模式 基本语法 基本概念 数据类型 整数 浮点数 字符 布尔 其它类型 变量与常量 流程控制逻辑 条件控制 循环控制 循环控制的 break 和 continue 类和对象 一个例子 构造函数 主构造函数 辅助构造函数 singleton objects 伴生对象 Companion Object 继承 override 一个例子 模式匹配 简单例子（整型数值匹配、温度判断） case class apply unapply 并发编程 Akka Akka Hello World Akka Quickstart with Scala 下载 配置及运行 代码解读 思考 参考文献 Scala 简介Scala 集成了面向对象编程和函数式编程语言的各种特性。Scala 的静态类型帮助减少复杂系统中的 bug，并且支持 JVM 和 JavaScript 运行环境，同时还提供丰富的类库可供使用。 Scala 的官网地址为 https://www.scala-lang.org 安装与配置Scala 运行环境依赖于 JVM，所以要先安装 Java。之后再安装 Scala，最后安装 sbt。 安装 Java在 oracle 官网下载 JavaSE 1.5 以上的版本。下载地址如下： http://www.oracle.com/technetwork/java/javase/downloads/index.html 这里我选择 1.8 版本，根据自己的操作系统选择合适的下载包。 设置环境变量的教程在这里：https://www.runoob.com/java/java-environment-setup.html 设置完成后，我们可以在终端查看 java 和 javac 命令: 123456$ java -versionjava version &quot;1.8.0_181&quot;Java(TM) SE Runtime Environment (build 1.8.0_181-b13)Java HotSpot(TM) 64-Bit Server VM (build 25.181-b13, mixed mode)$ javac -versionjavac 1.8.0_181 安装 ScalaScala 的下载地址为 https://www.scala-lang.org/download/ 。可以根据自己的操作系统选择合适的下载包。 如果操作系统为 Windows，可以查找 msi 的安装文件。 如果操作系统为 Mac，可以使用 Homebrew 来安装： 12brew updatebrew install scala 如果操作系统为 Ubuntu，直接用 apt-get install scala 来进行安装。 安装完成后，可以运行如下命令来查看 scala 命令： 12$ scala -versionScala code runner version 2.13.0 -- Copyright 2002-2019, LAMP/EPFL and Lightbend, Inc. 安装 sbtsbt 的全称为 Simple Build Tool，用于管理 Scala 项目的依赖并进行构建，其地址就像 Maven 之于 Java。其下载地址为 https://www.scala-sbt.org/download.html。 Hello World我们演示三种输出 Hello World 的方式。 交互模式在命令行输入 scala 命令，进入交互模式。 123456$ scalaWelcome to Scala 2.13.0 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_181).Type in expressions for evaluation. Or try :help.scala&gt; println(&quot;Hello World&quot;);Hello World 编译模式创建一个文件 HelloWorldDemo.scala，文件内容如下： 12345object HelloWorldDemo { def main(args: Array[String]):Unit = { println(&quot;Hello World&quot;); }} 运行编译及运行命令： 123$ scalac HelloWorldDemo.scala$ scala HelloWorldDemoHello World 构建模式可以使用 sbt 来构建。如编译模式的文件为例，我们将其放到 helloworld 目录里。确保该目录只有一个文件。输入 sbt 命令，然后用 run 命令运行。 1234567891011121314151617$ sbtsbt:helloworld&gt; run[info] Updating ...[info] Done updating.[info] Compiling 1 Scala source to /Users/didi/code/scala/helloworld/target/scala-2.12/classes ...[info] Non-compiled module 'compiler-bridge_2.12' for Scala 2.12.7. Compiling...[info] Compilation completed in 8.672s.[info] Done compiling.[info] Packaging /Users/didi/code/scala/helloworld/target/scala-2.12/helloworld_2.12-0.1.0-SNAPSHOT.jar ...[info] Done packaging.[info] Running HelloWorldDemo Hello World[success] Total time: 18 s, completed 2019-6-25 15:19:19sbt:helloworld&gt; run[info] Running HelloWorldDemo Hello World[success] Total time: 0 s, completed 2019-6-25 15:19:23 可以看到，第一次 run 时，需要进行编译，耗时 18 s。第二次 run 时，不需再编译即可运行，耗时可以忽略不计。 基本语法基本概念 类：外部事物的抽象，具有方法和属性。 对象：对象是类的具体实例；类是对象的抽象。 方法：方法是类的某个行为动作。一个类可以包含多个方法。 属性：类的若干属性，又称为字段。 语句：语句是执行代码的单元。如果一行只有一个语句，那么结尾不需要加;分号。如果一行之内有多个语句的话，需要加分号。 注释：支持多行注释 /* */ 和单行注释 //。 包：使用 package 定义包，类似于命名空间。 引用：使用 import 引用包。注意 Scala 的包引用可以出现在任何地方，而不只是文件的顶部。 标识符：标识符可以由字母和下划线开头，后面可以连接数字、字母和下划线。区分大小写。 类名：类名采用驼峰写法，每个单词首字母大写。 方法名：方法名称的第一个字母用小写，其余每个单词的第一个字母大写。 程序文件名：程序文件名称和对象名称要完全匹配。虽然新版本不再要求，但为了保证代码能通过旧版本编译器，还是保持一致较好。 程序入口：程序入口为 main 方法。 数据类型Scala 的数据类型分为以下几类： 整数 类型 说明 Byte 8位有符号补码整数。数值区间为 -128 到 127 Short 16位有符号补码整数。数值区间为 -32768 到 32767 Int 32位有符号补码整数。数值区间为 -2147483648 到 2147483647 Long 64位有符号补码整数。数值区间为 -9223372036854775808 到 9223372036854775807 浮点数 类型 说明 Float 32 位, IEEE 754 标准的单精度浮点数 Double 64 位 IEEE 754 标准的双精度浮点数 字符 类型 说明 Char 16位无符号Unicode字符, 区间值为 U+0000 到 U+FFFF String 字符序列 布尔 类型 说明 Boolean true或false 其它类型 类型 说明 Unit 表示无值，和其他语言中void等同。用作不返回任何结果的方法的结果类型。Unit只有一个实例值，写成()。 Null null 或空引用 Nothing Nothing类型在Scala的类层级的最低端；它是任何其他类型的子类型。 Any Any是所有其他类的超类 AnyRef AnyRef类是Scala里所有引用类(reference class)的基类 变量与常量变量在程序运行过程中，其值可能发生改变，采用 var 声明变量； 常量在程序运行过程中，其值不会发生改变，采用 val 声明常量。 声明变量和常量的语法如下： 12345var VariableName : DataType [= Initial Value]或val VariableName : DataType [= Initial Value] 另外 DataType 是可选的。如果不指定数据类型，编译器会自动根据初始值推断出数据类型。 流程控制逻辑Scala 的流程控制主要包括条件控制和循环控制。 条件控制其使用方法和其它语言类似，示例如下： 123456789if(布尔表达式 1){ // 如果布尔表达式 1 为 true 则执行该语句块}else if(布尔表达式 2){ // 如果布尔表达式 2 为 true 则执行该语句块}else if(布尔表达式 3){ // 如果布尔表达式 3 为 true 则执行该语句块}else { // 如果以上条件都为 false 执行该语句块} 循环控制循环支持三种循环类型：while 循环、do while 循环和 for 循环。 while 循环 1234while(condition){ //语句块} do while循环 123do { //语句块} while( condition ); for 循环 123for( var x &lt;- Range ){ //语句块} 循环控制的 break 和 continue Scala 的 break 的语法和其它语言不同，它完全是基于类的一种实现。 文件：BreakDemo.scala 123456789101112131415161718import scala.util.control.Breaks;object BreakDemo { def main(args: Array[String]) { var i = 0; val numList = List(1,2,3,4); val loop = new Breaks; loop.breakable { for( i &lt;- numList){ println( &quot;第&quot; + i + &quot;次&quot; ); if( i == 3 ){ loop.break; } } } println( &quot;事不过三&quot; ); }} 输出结果： 1234第1次第2次第3次事不过三 遗憾的是，Scala 里并没有 continue 的语法，即跳过本次循环。但想一下 continue 的实质是什么？跳过满足特定条件的循环中的元素，即过滤掉某些元素，让其不参与循环。Scala 提供了循环过滤的机制。 我们看一下示例。 文件：ForFilterDemo.scala 123456789101112131415import java.util.Random;object ForFilterDemo { def main(args: Array[String]) { var r = new Random() var rand = r.nextInt(100) var i = 0 for(i &lt;- 0 to 10 if rand % 2 == 0//只保留所有的偶数 ){ println(rand) rand = r.nextInt(100) } }} 输出结果（每次都不相同）： 12369012 我们循环 10 次，每次取一个 100 以内的随机数，要求只保留偶数，所有的奇数都过滤掉。实现上可以用 for + if 的循环过滤机制来实现。 类和对象Scala 类的特征有如下几个： 单继承 无 static 静态属性和方法，但提供了单例对象（singleton objects）的概念 支持抽象类和抽象方法 支持重载（override） 一个例子首先看一个类的示例。Point 类有两个属性 x 和 y，并有一个方法 move。 文件：PointDemo.scala 12345678910111213141516171819202122import java.io._class Point(val xc: Int, val yc: Int) { var x: Int = xc var y: Int = yc def move(dx: Int, dy: Int) { x = x + dx y = y + dy println (&quot;Point x location : &quot; + x); println (&quot;Point y location : &quot; + y); }}object PointDemo { def main(args: Array[String]) { val pt = new Point(10, 20); // Move to a new location pt.move(10, 10); }} 运行命令如下： 1234&gt; scalac PointDemo.scala&gt; scala PointDemoPoint x location : 20Point y location : 30 在这个例子里，需要注意的是，如果文件中定义了不止一个类，则要使用 scalac 进行编译后再运行。 这个例子还可以学到构造函数和 singleton objects 的知识，我们分述如下。 构造函数Scala 的构造函数分为主构造函数和辅助构造函数。 主构造函数在上例中，定义 Point 类的语句如下： 1class Point(val xc: Int, val yc: Int) 其中 xc、yc 为构造函数的参数。那么其函数的定义在哪里呢？ 在类的主体里，除了 def 定义的函数之外，其余部分都是主构造函数。 辅助构造函数辅助构造函数用于创造不同类型、不同数目参数的构造函数。定义方法如下： 123def this(){ //function body} 我们改造一下PointDemo.scala文件，来演示主构造函数和辅助构造函数。 文件：PointDemo2.scala 123456789101112131415161718192021222324252627282930import java.io._class Point(val xc: Int, val yc: Int) { var x: Int = xc var y: Int = yc def this(){//if no params,set (0,0) this(0,0) println(&quot;Auxiliary constructor&quot;) } def move(dx: Int, dy: Int) { x = x + dx y = y + dy println (&quot;Point x location : &quot; + x); println (&quot;Point y location : &quot; + y); } println(&quot;Main Constructor&quot;);//this is main constructor too}object PointDemo2 { def main(args: Array[String]) { val pt1 = new Point(10, 20); // Move to a new location pt1.move(10, 10); val pt2 = new Point(); // Move to a new location pt2.move(10, 10); }} 输出结果： 1234567Main ConstructorPoint x location : 20Point y location : 30Main ConstructorAuxiliary constructorPoint x location : 10Point y location : 10 我们看以下问题： 主构造函数和辅助构造函数有什么区别？ 答案是没有区别。当我们实例化一个类时，编译器会自动匹配参数类型和参数个数相符的构造的函数。 能否将主构造函数和辅助构造函数的参数完全一致？ 答案是不能。我们定义辅助构造函数如下： 12345// demo for same main and auxiliary constructors def this(xc: Int, yc: Int){ this(xc,yc) println(&quot;Auxiliary constructor&quot;) } 结果编译通不过。 singleton objectssingleton object 有如下特征： singleton object 里的方法是全局可见的 不能为 singleton object 创建实例 singleton object 的主构造函数不接收参数 singleton object 可以继承 class 和 traits singleton object始终存在 main 方法 不需要创建实例即可调用 singleton object 的方法 我们查看一下实例。 文件：SingletonDemo.scala 123456789101112131415161718192021222324252627282930object Exampleofsingleton { // Varaibles of singleton object var str1 = &quot;Welcome ! MIS&quot;; var str2 = &quot;This is Scala language tutorial&quot;; // Method of singleton object def display() { println(&quot;Called By Display Method&quot;) println(str1); println(str2); } } // Singleton object with named as Main object SingletonDemo { def main(args: Array[String]) { // Calling method of singleton object Exampleofsingleton.display(); println(&quot;Called By Property&quot;) println(Exampleofsingleton.str1); println(Exampleofsingleton.str2); } } 输出结果如下： 123456Called By Display MethodWelcome ! MISThis is Scala language tutorialCalled By PropertyWelcome ! MISThis is Scala language tutorial 伴生对象 Companion Object我们经常会遇到这样的场景：定义一个类，实现常见的功能；再定义一个类来调用以完成特定的任务。这样带来命名问题，这有时也是让开发者头疼的一件事情。 Scala 的伴生对象完美解决了这个问题。我们先看下例子： 文件：SingletonDemo.scala 12345678910111213141516171819202122232425262728293031323334// Companion class class CompanionDemo { // Variables of Companion class var str1 = &quot;MIS&quot;; var str2 = &quot;Tutorial of Companion object&quot;; private var str3 = &quot;Hello MIS&quot; // Method of Companion class def show() { println(str1); println(str2); } private def mis() { println(&quot;In mis method&quot;) println(str3) }} // Companion object object CompanionDemo{ def main(args: Array[String]) { var obj = new CompanionDemo(); obj.show(); println(obj.str3); obj.mis(); } } 输出结果： 12345MISTutorial of Companion objectHello MISIn mis methodHello MIS 可以看到同时存在 class CompanionDemo 和 object CompanionDemo。 伴生对象的特点如下： companion object 和 companion class 的名称必须完全一致 companion object 必须和 companion class 定义在同一个文件中 companion object 可以访问 companion class 的私有属性和私有方法 伴生对象除了解决命名问题外，还可以对外提供接口，方便调用。 继承Scala 是单继承的语言，子类只有一个父类。 ###抽象类和抽象方法 定义一个抽象类，需要使用 abstract 关键字。例如： 12345abstract class Animal(iname:String){ //class body def eat } 定义抽象方法，执行保持方法的函数体为空即可。 子类继承抽象父类，必须实现父类的所有的抽象方法。 override override 关键字有以下特点： 子类重写父类的抽象方法，不需要加 override 关键字。 子类重新父类的普通方法，需要加 override 关键字。 只有主构造函数才可以往父类的构造函数里写参数。 一个例子我们看一个演示继承的例子。 文件：InheritDemo.scala 1234567891011121314151617181920212223242526272829303132333435abstract class Animal(iname:String){ val name:String = iname; def eat def move(){ println(&quot;All Animals Can Move&quot;); } def display(){ println(&quot;Hello &quot;+name) }}class Dog(iname:String) extends Animal(iname:String) { def eat(){ println(&quot;Dog Eats Bones&quot;); } override def move(){ println(&quot;Dog is An Animal, So Dog can move&quot;); } def display(msg:String){ println(&quot;Hello &quot;+name+&quot; &quot;+msg) }}object InheritDemo{ def main(args: Array[String]) { val dog = new Dog(&quot;Teddy&quot;); dog.eat(); dog.move(); dog.display(&quot;Welcome!&quot;) } } 输出结果： 123Dog Eats BonesDog is An Animal, So Dog can moveHello Teddy Welcome! Dog 类的 display 方法为什么没有加 override ？ 答案是 Dog 类的 display 方法的参数和 Animal 类的 display 方法不同，不属于重写，所以不需要加 override 。 模式匹配Scala 使用 case 关键字实现多条件逻辑匹配的功能。每个条件是一个备选项，称为模式。每个备选项包含一个模式和一到多个表达式。语法如下： 1case 模式 =&gt; 语句; 我们看几个例子，学习模式匹配。 简单例子（整型数值匹配、温度判断）文件：Temperature.scala 12345678910111213object Temperature { def main(args: Array[String]) { println(display(37)) println(display(100)) println(display(50)) } def display(x: Int): String = x match { case 0 =&gt; &quot;freezing point&quot; case 100 =&gt; &quot;boiling point&quot; case x if(x &gt;= 36.5 &amp;&amp; x &lt;= 37.5) =&gt; &quot;body point&quot; case _ =&gt; &quot;other point&quot; }} 输出结果： 123body pointboiling pointother point match 表达式通过以代码编写的先后次序尝试每个模式来完成计算，只要发现有一个匹配的case，剩下的case不会继续匹配。 case class文件：CaseClassDemo.scala 123456789101112131415161718object CaseClassDemo { def main(args: Array[String]) { val alice = new Person(&quot;Alice&quot;, 25) val bob = new Person(&quot;Bob&quot;, 32) val charlie = new Person(&quot;Charlie&quot;, 32) for (person &lt;- List(alice, bob, charlie)) { person match { case Person(&quot;Alice&quot;, 25) =&gt; println(&quot;Hi Alice!&quot;) case Person(&quot;Bob&quot;, 32) =&gt; println(&quot;Hi Bob!&quot;) case Person(name, age) =&gt; println(&quot;Age: &quot; + age + &quot; year, name: &quot; + name + &quot;?&quot;) } } } // 样例类 case class Person(name: String, age: Int)} 输出结果： 123Hi Alice!Hi Bob!Age: 32 year, name: Charlie? 在声明样例类时，下面的过程自动发生了： 构造器的每个参数都成为val，除非显式被声明为var，但是并不推荐这么做； 在伴生对象中提供了apply方法，所以可以不使用new关键字就可构建对象； 提供unapply方法使模式匹配可以工作； 生成toString、equals、hashCode和copy方法，除非显示给出这些方法的定义。 apply将对象以函数的方式进行调用时，scala会隐式地将调用改为在该对象上调用apply方法。例如XXX(“hello”)实际调用的是XXX.apply(“hello”), 因此apply方法又被称为注入方法。apply方法常用于创建类实例的工厂方法。示例如下： 1234567object Greeting{ def apply(name: String) = &quot;Hello &quot; + name}Greeting.apply(“Lucy”) //与下面的调用等价 Greeting(“Lucy”) //结果为 Hello Lucy unapply与 apply 相对的是 unapply 方法，它的用法与 apply 类似，但其作用是用来抽取部分参数，它也称为抽取方法，主要用于模式匹配时抽取某些参数 case XXX(str) =&gt; println(str) 并发编程 Akka从 Scala 的 2.11.0 版本开始，Scala 的 Actors 库已经过时了。早在 Scala 2.10.0 的时候，默认的 actor 库即是 Akka。所以我们重点讲述 Akka。 Akka 是构建高并发、分布式、弹性消息驱动的 Java 和 Scala 应用的工具集合。官网地址是https://akka.io/ 。 Akka Hello World示例代码我已经放到 github 上了，地址为 https://github.com/spetacular/scala-simple-lesson。 下载代码后，用命令行 cd akkademo 进入示例代码目录。 依次运行 stb 、compile、run，可以看到类似于如下输出： 1234567891011121314151617$ sbt[info] Loading project definition from /Users/didi/Documents/GitHub/scala-simple-lesson/akkademo/project[info] Loading settings for project akkademo from build.sbt ...[info] Set current project to akka-demo (in build file:/Users/didi/Documents/GitHub/scala-simple-lesson/akkademo/)[info] sbt server started at local:///Users/didi/.sbt/1.0/server/bc7cb24c19aa2ba1ff53/socksbt:akka-demo&gt; compile[info] Updating ...[info] Done updating.[info] Compiling 1 Scala source to /Users/didi/Documents/GitHub/scala-simple-lesson/akkademo/target/scala-2.12/classes ...[info] Done compiling.[success] Total time: 6 s, completed 2019-6-25 17:57:00sbt:akka-demo&gt; run[info] Packaging /Users/didi/Documents/GitHub/scala-simple-lesson/akkademo/target/scala-2.12/akka-demo_2.12-1.0.jar ...[info] Done packaging.[info] Running com.example.AkkaDemo fine thank youhuh? 我们看下代码： 12345678910111213141516171 package com.example;2 import akka.actor.{Actor,ActorSystem,Props}3 4 class HelloActor extends Actor {5 def receive = {6 case &quot;how are you&quot; =&gt; println(&quot;fine thank you&quot;)7 case _ =&gt; println(&quot;huh?&quot;)8 }9 }10 11 object AkkaDemo extends App {12 val system = ActorSystem(&quot;HelloSystem&quot;)13 // default Actor constructor14 val helloActor = system.actorOf(Props[HelloActor], name = &quot;helloactor&quot;)15 helloActor ! &quot;how are you&quot;16 helloActor ! &quot;Bonjour&quot;17 } 我们先看下调用示例图： 第 4 至 9 行，定义了 HelloActor 的类。HelloActor 继承在 Actor 类，实现了 receive 方法。该方法根据传递过来的字符串，做出不同的动作。 第 11 行，AkkaDemo 继承了 App 类，作为main class，是程序执行的入口。 第 12 行，定义了一个 ActorSystem，这是所有 Actor 的容器。 第 14 行，定义了一个 Actor。Actor 是一种类似于线程、goroutine的一个事物，是执行调度的一个单位。在创建 helloActor 的时候，并没有用 new 关键字来实例化，而是用 actorOf 来创建一个 Actor 的引用。actorOf 接收两个参数：配置类 Props 和名称。这种方式的优点是在分布式系统上，调用方不用关心实质的 Actor 在哪台机器上。 第 15、16行，将&quot;how are you&quot;和&quot;Bonjour&quot;的消息传递给 HelloActor。 这个例子演示了 akka 的 AkkaSystem、Actor、消息传递是如何交互的。但这个例子，仍然是串行处理的。我们将在下面的例子中，演示并行编程。 Akka Quickstart with Scala这个例子来自 Akka 官方示例，网址为 Akka Quickstart with Scala 。 下载下载方法有两种： 在官网页面 Lightbend Tech Hub 点击 CREATE A PROJECT FOR ME 嫌速度慢可以下载 Scala Simple Lesson ，解压后 akka-quickstart-scala 目录即是示例代码。 配置及运行解压后的代码，构建脚本可能没有运行权限，需要用 chmod 命令添加可运行权限。 123$ cd akka-quickstart-scala$ chmod u+x ./sbt$ chmod u+x ./sbt-dist/bin/sbt 然后输入 ./sbt 。这时会下载依赖的包，可能会持续一段时间。 然后在 stb 输入提示符下，输入 reStart 来启动示例。类似的输出如下： 12345678910111213sbt:akka-quickstart-scala&gt; reStart[info] Updating ...[info] Done updating.[info] Compiling 1 Scala source to /Users/didi/Documents/GitHub/scala-simple-lesson/akka-quickstart-scala/target/scala-2.12/classes ...[info] Done compiling.[info] Application akka-quickstart-scala not yet started[info] Starting application akka-quickstart-scala in the background ...akka-quickstart-scala Starting com.example.AkkaQuickstart.main()[success] Total time: 5 s, completed 2019-6-25 20:16:03sbt:akka-quickstart-scala&gt; akka-quickstart-scala [INFO] [06/25/2019 20:16:04.502] [helloAkka-akka.actor.default-dispatcher-5] [akka://helloAkka/user/printerActor] Greeting received (from Actor[akka://helloAkka/user/helloGreeter#266661191]): Hello, Scalaakka-quickstart-scala [INFO] [06/25/2019 20:16:04.502] [helloAkka-akka.actor.default-dispatcher-5] [akka://helloAkka/user/printerActor] Greeting received (from Actor[akka://helloAkka/user/howdyGreeter#1265860474]): Howdy, Akkaakka-quickstart-scala [INFO] [06/25/2019 20:16:04.502] [helloAkka-akka.actor.default-dispatcher-5] [akka://helloAkka/user/printerActor] Greeting received (from Actor[akka://helloAkka/user/howdyGreeter#1265860474]): Howdy, Lightbendakka-quickstart-scala [INFO] [06/25/2019 20:16:04.502] [helloAkka-akka.actor.default-dispatcher-5] [akka://helloAkka/user/printerActor] Greeting received (from Actor[akka://helloAkka/user/goodDayGreeter#1928998630]): Good day, Play 代码解读完整代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293941 //#full-example2 package com.example3 4 import akka.actor.{ Actor, ActorLogging, ActorRef, ActorSystem, Props }5 6 //#greeter-companion7 //#greeter-messages8 object Greeter {9 //#greeter-messages10 def props(message: String, printerActor: ActorRef): Props = Props(new Greeter(message, printerActor))11 //#greeter-messages12 final case class WhoToGreet(who: String)13 case object Greet14 }15 //#greeter-messages16 //#greeter-companion17 18 //#greeter-actor19 class Greeter(message: String, printerActor: ActorRef) extends Actor {20 import Greeter._21 import Printer._22 23 var greeting = &quot;&quot;24 25 def receive = {26 case WhoToGreet(who) =&gt;27 greeting = message + &quot;, &quot; + who28 case Greet =&gt;29 //#greeter-send-message30 printerActor ! Greeting(greeting)31 //#greeter-send-message32 }33 }34 //#greeter-actor35 36 //#printer-companion37 //#printer-messages38 object Printer {39 //#printer-messages40 def props: Props = Props[Printer]41 //#printer-messages42 final case class Greeting(greeting: String)43 }44 //#printer-messages45 //#printer-companion46 47 //#printer-actor48 class Printer extends Actor with ActorLogging {49 import Printer._50 51 def receive = {52 case Greeting(greeting) =&gt;53 log.info(&quot;Greeting received (from &quot; + sender() + &quot;): &quot; + greeting)54 }55 }56 //#printer-actor57 58 //#main-class59 object AkkaQuickstart extends App {60 import Greeter._61 62 // Create the 'helloAkka' actor system63 val system: ActorSystem = ActorSystem(&quot;helloAkka&quot;)64 65 //#create-actors66 // Create the printer actor67 val printer: ActorRef = system.actorOf(Printer.props, &quot;printerActor&quot;)68 69 // Create the 'greeter' actors70 val howdyGreeter: ActorRef =71 system.actorOf(Greeter.props(&quot;Howdy&quot;, printer), &quot;howdyGreeter&quot;)72 val helloGreeter: ActorRef =73 system.actorOf(Greeter.props(&quot;Hello&quot;, printer), &quot;helloGreeter&quot;)74 val goodDayGreeter: ActorRef =75 system.actorOf(Greeter.props(&quot;Good day&quot;, printer), &quot;goodDayGreeter&quot;)76 //#create-actors77 78 //#main-send-messages79 howdyGreeter ! WhoToGreet(&quot;Akka&quot;)80 howdyGreeter ! Greet81 82 howdyGreeter ! WhoToGreet(&quot;Lightbend&quot;)83 howdyGreeter ! Greet84 85 helloGreeter ! WhoToGreet(&quot;Scala&quot;)86 helloGreeter ! Greet87 88 goodDayGreeter ! WhoToGreet(&quot;Play&quot;)89 goodDayGreeter ! Greet90 91 //#main-send-messages92 }93 //#main-class94 //#full-example 我们先直观地看下这个文件的大体框架，如下图所示。 首先 main 类创建了ActorSystem 容器，然后创建了三个 Greeter Actor 实例和一个 Printer Actor 实例。 main 类首先将消息传递给三个 Greeter Actor，然后三个 Greeter Actor 分别再将消息传递给 Printer Actor，并由 Printer Actor 将消息打印出来。 下面是详细解读。 第 6 至 34 行，定义了 Greeter 的伴友类和伴友对象。注意 Greeter 是一个 Actor，其 receive 方法定义在第 25 至 32 行。 第 12 行定义了 WhoToGreet 的样例类：final case class WhoToGreet(who: String) 。 第 13 行定义了 Greet 的样例类：case object Greet 。 Greeter、WhoToGreet、Greet 的关系是怎样的呢？ Greeter 意思为礼仪人员。它接收的 case class 是 WhoToGreet 和 Greet。就是说礼仪人员有两个用途：一是组织根据人员组装合适的礼貌用语(WhoToGreet），另一个是将拼好的礼貌用语用嘴巴(Printer)说出去(Greet)。 第 36 至 56 行定义了 Printer 的伴友类和伴友对象。 第 41 行定义了 Greeting 样例类：case class Greeting(greeting: String) 。 第 51 至 54 行定义了 Printer 的 receive 方法。它接收 Greeting，并将 greeting 内容打印出来。 第 58 至 94 行为入口方法。 第 63 行定义了名为 helloAkka 的 Actor System。 第 69 至 76 行创建了三个 Greeter 的实例：howdyGreeter、helloGreeter、goodDayGreeter。他们会说不同的礼貌用语。 第 78 至 91 行调用 Actor 进行并发处理。我们以第一个调用为例，看一下处理流程。 12howdyGreeter ! WhoToGreet(&quot;Akka&quot;)howdyGreeter ! Greet 首先将 WhoToGreet(&quot;Akka&quot;) 的消息传递给 howdyGreeter，这时会激活如下语句： 1226 case WhoToGreet(who) =&gt;27 greeting = message + &quot;, &quot; + who 此时 123message = &quot;Howdy&quot;who = &quot;Akka&quot;greeting = message + &quot;, &quot; + who = &quot;Howdy, Akka&quot; 然后将 Greet 的消息传递给 howdyGreeter，这时会激活如下语句： 123428 case Greet =&gt;29 //#greeter-send-message30 printerActor ! Greeting(greeting)31 //#greeter-send-message 此时将 Greet 的消息通过 Greeting 传递给 PrinterActor。 最后 PrintActor 收到消息后，将消息打印在日志上。 123451 def receive = {52 case Greeting(greeting) =&gt;53 log.info(&quot;Greeting received (from &quot; + sender() + &quot;): &quot; + greeting)54 } 思考如果我们多次输入 reStrart 来运行该例子，会发现输出的礼貌用语的次序各不相同。这就是说程序是并发执行的。同时需要注意，无论怎么执行，howdyGreeter 的两次结果，Akka 始终在 Lightbend 之前。我们可以得出结论： 不同的 Actor 直接是并发执行的。 同一个 Actor 接收到的多个消息，是串行执行的。 参考文献https://www.geeksforgeeks.org/scala-singleton-and-companion-objects/ https://www.tutorialspoint.com/scala/index.htm https://www.runoob.com/scala/scala-tutorial.html https://blog.csdn.net/shenlei19911210/article/details/78538255 https://docs.scala-lang.org/zh-cn/tour/pattern-matching.html https://developer.lightbend.com/guides/akka-quickstart-scala/","link":"/2019/06/24/Scala-simple-lessons.html"},{"title":"OLAP 的 12 条规则","text":"Edgar F. Codd于1985年撰写了一篇论文，定义了关系数据库管理系统（RDBMS）的规则，这些规则彻底改变了IT行业。 1993年，Codd及其同事研究了以下 12 条规则，用于定义OLAP（在线分析处理）。这是一个可以在多维空间中整合和分析数据的行业。 Codd的12条规则是： Multidimensional conceptual viewUser-analysts would view an enterprise as being multidimensional in nature – for example, profits could be viewed by region, product, time period, or scenario (such as actual, budget, or forecast). Multi-dimensional data models enable more straightforward and intuitive manipulation of data by users, including “slicing and dicing“. 准则1 多维概念视图在用户分析师看来，企业天然是多维的。 例如，可以按地区，产品，时间段或方案（例如实际，预算或预测）查看利润。多维数据模型使用户能够更直接，更直观地处理数据，包括“分片和分块”。 TransparencyWhen OLAP forms part of the users’ customary spreadsheet or graphics package, this should be transparent to the user. OLAP should be part of an open systems architecture which can be embedded in any place desired by the user without adversely affecting the functionality of the host tool. The user should not be exposed to the source of the data supplied to the OLAP tool, which may be homogeneous or heterogeneous. 准则2 透明性准则当OLAP构成用户习惯电子表格或图形包的一部分时，这应该对用户透明。 OLAP应该是开放系统体系结构的一部分，该体系结构可以嵌入到用户期望的任何位置，而不会不利地影响宿主工具的功能。用户不应暴露于提供给OLAP工具的数据源，这可能是同构的或异构的。 AccessibilityThe OLAP tool should be capable of applying its own logical structure to access heterogeneous sources of data and perform any conversions necessary to present a coherent view to the user. The tool (and not the user) should be concerned with where the physical data comes from. 准则3 存取能力推测OLAP工具应该能够应用自己的逻辑结构来访问异构数据源，并执行向用户呈现连贯视图所需的任何转换。工具（而不是用户）应关注物理数据的来源。 Consistent reporting performancePerformance of the OLAP tool should not suffer significantly as the number of dimensions is increased. 准则4 稳定的报表性能随着维度数量的增加，OLAP工具的性能不会受到显著影响。 Client/server architectureThe server component of OLAP tools should be sufficiently intelligent that the various clients can be attached with minimum effort. The server should be capable of mapping and consolidating data between disparate databases. 准则5 客户/服务器架构OLAP工具的服务器组件应该足够智能，各种客户端可以轻松地连接它。服务器应该能够在不同的数据库之间映射和合并数据。 Generic DimensionalityEvery data dimension should be equivalent in its structure and operational capabilities. 准则6 维的等同性准则每个数据维度的结构和操作能力都应相同。 Dynamic sparse matrix handlingThe OLAP server’s physical structure should have optimal sparse matrix handling. 准则7 动态的稀疏矩阵处理准则OLAP服务器的物理结构应具有最佳的稀疏矩阵处理。 Multi-user supportOLAP tools must provide concurrent retrieval and update access, integrity and security. 准则8 多用户支持能力准则OLAP工具必须提供并发检索和更新访问，完整性和安全性。 Unrestricted cross-dimensional operationsComputational facilities must allow calculation and data manipulation across any number of data dimensions, and must not restrict any relationship between data cells. 准则9 非受限的跨维操作计算设施必须允许跨任意数量的数据维度进行计算和数据处理，并且不得限制数据单元之间的任何关系。 Intuitive data manipulationData manipulation inherent in the consolidation path, such as drilling down or zooming out, should be accomplished via direct action on the analytical model’s cells, and not require use of a menu or multiple trips across the user interface. 准则10 直观的数据操作合并路径中固有的数据操作，例如向下钻取或缩小，应通过对分析模型单元的直接操作来完成，而不需要使用菜单或跨用户界面多次行程。 Flexible reportingReporting facilities should present information in any way the user wants to view it. 准则11 灵活的报告生成报告工具应以用户想要查看的任何方式显示信息。 Unlimited Dimensions and aggregation levels. 准则12 不受限的维度和聚合层次 翻译自：http://olap.com/learn-bi-olap/codds-paper/","link":"/2018/12/01/olap-12-rules.html"},{"title":"重新思考 CAP 定理","text":"CAP Theorem: Revisited 译文 记录：两个月之前，我写了一篇博客来解释 CAP 定理。发表之后我意识到，我在这个主题上的思考已经非常过时，并且不适应于真实世界。我尝试在这篇博客上弥补该缺陷。 在当今的技术环境中，当需要额外的资源（计算，存储等）以在合理的时间范围内成功完成工作负载时，我们见证了对系统进行扩展的强烈且不断增长的需求。这可以通过向系统添加其他商品硬件来处理增加的负载来实现。 这种扩展策略的结果是系统增加了额外的复杂度。 这就是 CAP 定理起作用的地方。 CAP 定理指出，在一个分布式系统（指互相连接并共享数据的节点的集合）中，当涉及读写操作时，只能保证一致性（Consistence）、可用性（Availability）、分区容错性（Partition Tolerance）三者中的两个，另外一个必须被牺牲。 但是，正如下面看到的那样，这里没有太多选择。 一致性 - 对某个指定的客户端来说，读操作保证能够返回最新的写操作结果。 可用性 - 非故障的节点在合理的时间内返回合理的响应（不是错误和超时的响应）。 分区容错性 - 当出现网络分区后，系统能够继续“履行职责”。 在继续讲解之前，我们先纠正一个概念：面向对象编程 != 网络编程！ 在构建共享内存的应用程序时，一旦节点在空间和时间上分开，有一些我们认为理所当然的假设就会崩溃。 分布式计算的谬误之一是网络是可靠的。 但它们不是。 网络和网络的某些部分经常会意外崩溃。 网络故障发生在您的系统上，您无法选择何时发生。 鉴于网络并不完全可靠，您必须容忍分布式系统中的分区。 不过，幸运的是，您可以选择在发生分区时要做什么。 根据 CAP 定理，这意味着我们有两个选择：一致性和可用性。 CP-Consistency/Partition Tolerance-等待分区节点的响应，这可能导致超时错误。 系统还可以根据您想要的情况选择返回错误。 当您的业务需求要求原子读写时，请选择“可用性之上的一致性”。 如下图所示，为了保证一致性，当发生分区现象后，N1 节点上的数据已经更新到 y，但由于 N1 和 N2 之间的复制通道中断，数据 y 无法同步到 N2，N2 节点上的数据还是 x。这时客户端 C 访问 N2 时，N2 需要返回 Error，提示客户端 C“系统现在发生了错误”，这种处理方式违背了可用性（Availability）的要求，因此 CAP 三者只能满足 CP。 AP-Availability/Partition Tolerance-返回拥有数据的最新版本，这可能是过时的。 此系统状态还将接受在分区解析后可以稍后处理的写操作。 当您的业务需求允许系统中的数据同步时具有一定的灵活性时，请选择“可用性高于一致性”。 当系统尽管存在外部错误（购物车等）而仍需要继续运行时，可用性也是一个令人信服的选择. 如下图所示，为了保证可用性，当发生分区现象后，N1 节点上的数据已经更新到 y，但由于 N1 和 N2 之间的复制通道中断，数据 y 无法同步到 N2，N2 节点上的数据还是 x。这时客户端 C 访问 N2 时，N2 将当前自己拥有的数据 x 返回给客户端 C 了，而实际上当前最新的数据已经是 y 了，这就不满足一致性（Consistency）的要求了，因此 CAP 三者只能满足 AP。注意：这里 N2 节点返回 x，虽然不是一个“正确”的结果，但是一个“合理”的结果，因为 x 是旧的数据，并不是一个错乱的值，只是不是最新的数据而已。 一致性和可用性之间的决定是软件的权衡。 您可以选择面对网络分区时要做什么-控制权就在您手中。 网络中断（无论是永久性的还是永久性的）都是生活中不可或缺的事实，无论您是否希望发生，都会发生-这是软件外部存在的情况。 构建分布式系统具有许多优点，但同时也增加了复杂性。 了解面对网络错误时可以使用的折衷方法，并选择正确的路径对于应用程序的成功至关重要。 从一开始就无法正确执行此操作，可能会在首次部署之前使您的应用程序注定要失败。 原文:CAP Theorem: Revisited","link":"/2021/01/11/cap.html"},{"title":"Go 语言 Channel 简明教程","text":"channnel 中文译作通道。通道的核心是通信（signaling）。一个通道允许一个 goroutine 向另一个 goroutine 发送关于特定事件的信号。把通道看作一种信号机制（signaling mechanism），将帮助你写出更好的代码，和更精确的行为。为了理解信号是如何工作的，我们必须理解三个特性： 投递可靠性保障（Guarantee Of Delivery） 状态（State） 是否携带数据（With or Without Data） 这三个特性设计了一种信号机制。 投递可靠性保障投递可靠性保障基于一个问题：是否有必要保障一个特定 goroutine 发出的信号被接收？ 换句话说，就像 listing 1 的例子： Listing 1 1234501 go func() {02 p := &lt;-ch // Receive03 }()0405 ch &lt;- &quot;paper&quot; // Send 02行的发送 goroutine 是否需要保障05行的通道接收到数据之后才继续执行呢？基于这个问题的答案，你将知道两种类型的通道：Unbuffered 和 Buffered。两种通道提供的投递可靠性保障不同。保障非常重要。当写并发程序时，你要对是否需要保障有一个清晰的认识。 图 1 : 投递可靠性保障 状态通道的状态直接影响其行为。通道的state有 nil, open 和 closed 三种。 Listing 2 演示了如何声明或设置通道的状态。 Listing 2 12345678910111213141516171819// ** nil channel// A channel is in a nil state when it is declared to its zero valuevar ch chan string// A channel can be placed in a nil state by explicitly setting it to nil.ch = nil// ** open channel// A channel is in a open state when it’s made using the built-in function make.ch := make(chan string) // ** closed channel// A channel is in a closed state when it’s closed using the built-in function close.close(ch) 状态决定了发送和接收动作是怎样的行为。 信号通过通道发送和接收（sent/received）。不要说read/write 因为channel没有 I/O 操作。 图 2 : 状态当一个通道处于 nil 状态时，任何发送或接收都会被阻塞。当一个通道处于 open 状态时，信号可以被发送或接收。当一个通道处于 closed 状态时，不能发送，但仍然可以接收信号。这些状态提供了不同场景下的行为。将状态和投递可靠性保障结合起来，当你遇到 性能/收益 的权衡时，就能更好地分析。 是否携带数据最后一个需要考虑的信号属性是，你是否需要携带/不携带数据的信号。给一个通道发送数据的代码如下： Listing 3 101 ch &lt;- &quot;paper&quot; 使用携带数据的信号，通常情况是因为： 一个 goroutine 用来启动一个新任务。 一个 goroutine 将结果回报。通过关闭通道，你可以不需要数据来进行通信。 Listing 4 101 close(ch) 当你不需要数据进行通信时，通常情况是因为： 一个 goroutine 被告知需要停止正在进行的工作。 一个 goroutine 回报它已经完成指令，但是没有执行结果。 一个 goroutine 报告它已经完成处理并关闭。 这些规则也有例外，但这些是主要用例，也是本文重点关注的内容。我觉得这些规则的例外是原始的 代码异味。 不携带数据进行通信的一个优点是单个 goroutine 能够立刻和多个 goroutine 进行通信。携带数据进行通信，通常是 goroutine 之间的一对一交换。 携带数据进行通信当你需要携带数据进行通信时，你有三种类型的 channel 配置选项可供选择，取决于你需要的投递保障类型。 图 3 : 携带数据进行通信这三种通道选项分别是无缓冲（Unbuffered）, 缓冲区大于 1（Buffered &gt;1） 和缓冲区等于 1（Buffered =1）。 有保障 一个无缓冲的通道确保已接收到正在发送的信号。 因为接收信号的动作，发生在发送信息的动作结束之前。 无保障 一个缓冲区大于 1 的通道不会确保已接收到正在发送的信号。 因为发送信号的动作，发生再接收信号的动作结束之前。 延迟保障 一个缓冲区为 1 的通道提供延迟的保障。它能确保前一个已经发送的信号被接收到。 因为第一个信号的接收，发生在第二个信号发送完成之前。 缓冲区的大小不得为随机数，通常根据明确定义的约束条件计算得出。计算机的运算能力是有限的，所以无论时间和空间，都需要仔细进行衡量。*/ 不携带数据进行通信不携带数据的通信，通常用于取消操作。它允许一个 goroutine 通知另一个 goroutine 取消它正在进行的工作。取消操作可以用无缓冲和有缓冲两种类型的通道来实现，但是使用有缓冲而无数据的通道，是一个差的实践。 图 4 : 不携带数据进行通信 内置函数 close 被用来不携带数据进行通信。如「状态」一节的解释，一个通道被关闭后，仍然可以接收信号。 已关闭通道上的任何接收动作，都不会被阻塞，接收操作总会返回。 很多时候你需要标准库 context 包来实现不携带数据通信。context 包在底层使用无缓冲通道和内置函数 close来进行不携带数据的通信。 如果你选择使用自己的通知，而不是 context 包，来进行取消操作，那么你定义的通道应该是 type chan struct{} 类型。 它是一种零空间占用、惯用的方式，用来指示通道仅用于通信。 使用场景了解以上的知识之后，最好的深入理解的方法是通过不同使用场景的例子进行实践。 携带数据通信 - 保障 - 无缓冲通道当你需要知道一个发送中的信号，是否被接收，这时就有两个场景： 等待任务（ Wait For Task）和等待场景（Wait For Result）。 场景 1 - 等待任务 假设你是一个经理，雇了一名新员工。在这个场景中，你需要员工完成一项任务，但他需要你交给他一沓文件后，他才能开始工作。 Listing 5https://play.golang.org/p/AqqhbjDYDV5 12345678910111213141516171819202122232425261 package main2 3 import (4 &quot;math/rand&quot;5 &quot;time&quot;6 &quot;fmt&quot;7 )8 9 func main() {10 ch := make(chan string,1)11 12 go func() {13 p := &lt;-ch14 15 // Employee performs work here.16 fmt.Println(&quot;The employee starts to handle &quot;+p)17 18 // Employee is done and free to go.19 }()20 21 time.Sleep(time.Duration(rand.Intn(500)) * time.Millisecond)22 fmt.Println(&quot;Manager give paper&quot;)23 ch &lt;- &quot;paper&quot;24 //time.Sleep(time.Duration(rand.Intn(500)) * time.Millisecond)25 fmt.Println(&quot;Manager do other works&quot;)26 } 第 10 行，创建了一个无缓冲通道，字符串类型的数据将通过信号被发送；第 12 行，员工被雇用；第 13 行，员工需要等待经理的信号，才能继续工作；员工在等待经理递交文件的过程中处于阻塞状态。一旦文件被接收，员工开始工作；完成工作之后，员工就可以玩耍了。你作为经理，与你的新员工同步工作。所以你在第 12 行雇用新员工后，你需要做的事是解锁和通知员工（第 21 行）。注意，你整理需要发送的文件的时间是不可预知的。 终于你准备好通知雇员了。 在第 23 行，你携带数据进行通信，数据就是文件。因为使用了无缓冲通道，所以可以确保当你的发送操作完成时，员工已经接收了文件。接收发生在发送结束之前。 从技术上来讲，你所知道的是在你的通道发送操作完成时，员工已经收到了文件。两个通道操作之后，调度器（scheduler）可以选择任意的语句来执行。下一行被执行的代码是你还是员工，是不确定的。这意味着 print 语句的执行顺序可能不如你的预期那样。 场景 2 - 等待结果 在这个场景下，次序是颠倒的。这次你的员工被雇用后立刻就会投入工作，你需要等待他们工作的结果。你需要等待，因为你需要员工递交的文件。 Listing 6https://play.golang.org/p/l8JW8AP5-KO 123456789101112131415161718192021221 package main2 3 import (4 &quot;math/rand&quot;5 &quot;time&quot;6 &quot;fmt&quot;7 )8 9 func main() {10 ch := make(chan string)11 12 go func() {13 time.Sleep(time.Duration(rand.Intn(500)) * time.Millisecond)14 15 ch &lt;- &quot;paper&quot;16 17 // Employee is done and free to go.18 }()19 20 p := &lt;-ch21 fmt.Println(p)22 } 第 10 行，创建了一个无缓冲通道，字符串类型的数据将通过信号被发送；第 12 行，员工被雇用并立即投入工作；第 20 行，你等待文件的报告。 第 13 行，员工的工作完成后，在第 15 行将结果同步给你。因为通道是无缓冲通道，接收发生在发送完成时，员工能够保证你能接收到结果。一旦员工完成保障，他们的工作就完成了。在这个场景下，你不知道员工完成任务需要花费的时间。 代价/收益 无缓冲通道确保了发送中的信号一定会被接收。这个特性很棒，但是有代价的。可靠性保障的代价的未知的延迟时间。在「等待任务」场景里，员工在文件交给自己之前，不知道等待多长时间；在「等待结果」场景里，你不知道员工花费多长时间才将结果返回。 这两个场景里，为了保障投递的可靠性，我们必须要忍受未知的延迟时间。 没有这种可靠性保障，逻辑就无法正常工作。 携带数据进行通信 - 无保障 - 缓冲区大于 1 的通道当你不需要知道发送中的信号是否被接收，这时有两种场景：扇出（Fan Out）和丢弃（Drop）。 带缓冲区的通道拥有定义明确的空间，来存储发送中的数据。所以你如何确定需要多少空间呢？可以尝试回答以下问题: 我有定义明确的工作需要完成吗？ 哪里有多少工作？ 如果员工不能赶上进度，我能丢弃新工作吗？ 有多少未完成的工作使我满负荷工作? 如果我的程序异常结束，我能接受哪种级别的风险? 所有缓冲区的数据将会丢失。 如果以上问题对你正在建模的行为没有任何意义，那么不建议使用缓冲区大于 1 的通道。 场景 1 - 扇出 扇出模式允许在遇到一定问题时，安排一定数量的员工同时工作。因为每个任务由一个员工负责，因此你准确地知道你将接收到多少报告。你可以确定盒子里有足够的空间来接收所有这些报告。这样的好处是你的员工无需等待即可提交他们的报告。但是，当他们同时或几乎同时到达盒子时，则需要轮流将报告放入箱子中。 假设你再次成为经理，但是这次你雇用了一组员工。你需要每一个员工去执行一个任务。每个员工完成各自的任务后，他们需要把报告文件放置到你办公桌前的盒子里。 Listing 7https://play.golang.org/p/8HIt2sabs_ 123456789101112131415161701 func fanOut() {02 emps := 2003 ch := make(chan string, emps)0405 for e := 0; e &lt; emps; e++ {06 go func() {07 time.Sleep(time.Duration(rand.Intn(200)) * time.Millisecond)08 ch &lt;- &quot;paper&quot;09 }()10 }1112 for emps &gt; 0 {13 p := &lt;-ch14 fmt.Println(p)15 emps--16 }17 } 第 3 行创建了一个字符串类型的、有缓冲区的通道。由于第 2 行员工能够的数量为 20，所以通道的缓冲区大小也设置为 20。 第 5 至第 10 行，20 个员工被雇用并立即开始工作。你不知道在第 7 行员工花费多长时间。第 8 行，员工们提交他们的报告文件，但是这次发送操作不会阻塞等待接收操作。 因为每一个员工在盒子里都有空间，因此通道上的发送操作只有在他们的发送时间同时或接近时才会竞争。 第 12 至 16 行都是你的工作。在这里，你需要等待所有的 20 个员工完成并发送他们的报告。第 12 行，你在一个循环中；第 13 行你被阻塞，然后等待接收报告。当有报告被接收，报告在第 14 行被打印，同时计数器减 1 表明一个员工已经完成任务。 场景 2 - 丢弃 丢弃模式允许你在员工满负荷时丢弃工作。这样的好处是可以继续接受客户端的请求，并且在接受工作时不会增加后端的压力或延迟。关键是要知道什么时候达到满负载，这样才不会过少或过多地接受工作。通常需要通过集成测试或性能测试来帮助你识别这个数字。 再次想象你是那个经理。你雇用 1 个员工来完成工作。你需要你的员工完成 1 项工作。当员工完成他们的工作时，你不必关注他们已经完成了。真正重要的是你是否可以在盒子里放置新任务。如果你不能执行发送操作，你就知道盒子是满的，员工已经满负荷工作了。这时新任务会被丢弃以保证事情向前推进。 Listing 8https://play.golang.org/p/PhFUN5itiv 1234567891011121314151617181920212201 func selectDrop() {02 const cap = 503 ch := make(chan string, cap)0405 go func() {06 for p := range ch {07 fmt.Println(&quot;employee : received :&quot;, p)08 }09 }()1011 const work = 2012 for w := 0; w &lt; work; w++ {13 select {14 case ch &lt;- &quot;paper&quot;:15 fmt.Println(&quot;manager : send ack&quot;)16 default:17 fmt.Println(&quot;manager : drop&quot;)18 }19 }2021 close(ch)22 } 第 3 行创建了一个有缓冲区的通道。这次在第 2 行定义了通道有 5 个缓冲区。第 5 至 9 行，只有一个员工来处理工作。 for range 用来处理通道的接收工作。每一份文件被接收后，都在第 7 行进行了处理。第 11 至 19 行，你尝试发送 20 份文件给你的员工。这时在第 14 行使用 select 语句的第 1 个 case 条件来执行发送动作。因为第 16 行使用了默认的条件：当发送动作由于缓冲区无空间而阻塞时，在第 17 行发送动作被放弃执行。 第 21 行调用了内置函数 close。当员工们完成分配的工作后就可以自由离开。 代价/收益 缓冲区大于 1 的通道不确保发出的信号被接收。其中一个收益是减少或消除 goroutine 之间通信的延迟。在“扇出”场景下，每一个即将发送报告的员工都有一个缓冲区。在“丢弃”场景下，缓冲区进行过容量测试。如果达到容量限制，任务将会被丢弃以保证事情顺利进行。 这两种场景下，我们必须要忍受可靠性保证的缺失，因为延迟的减少更重要。 零延迟或低延迟不会对整个系统造成影响。 携带数据进行通信 - 延迟保障 - 缓冲区等于 1 的通道如果发送新信号时需要知道前一个发出的信号是否被接收，就需要「等待任务」（Wait For Tasks）场景。 场景 1 - 等待任务 在这个场景下，你有 1 个新员工，但他们不止做 1 个工作。你需要他们一个接一个地做很多工作。但是他们只有在完成 1 个任务后，才能开始新工作。由于他们一次只能专注于一件工作，因此工作切换之间可能存在延迟问题。在员工执行下一个任务，不损失可靠性的前提下，减少延迟是非常有帮助的。 这是缓冲区为 1 的通道的优点。 如果一切都按照你和员工所期待的速度进行，那么你们两个都不需要等待对方。你每次发送一个报告时，缓冲区都是空的。你的员工每次需要更多工作时，缓冲区都已满。 这是工作流程的完美对称。 以上是最好的部分。如果你每次试图发送一个报告时，都因为缓冲区已满而无法发送，你知道你的员工遇到了问题，而你也需要停下来。这就是造成延迟保障的原因。当缓冲区为空，你执行了发送动作，你知道你的员工已经完成了你之前发送的工作。如果你将要发送但还不能发送，说明员工尚未完成之前的工作。 Listing 9https://play.golang.org/p/4pcuKCcAK3 1234567891011121314151601 func waitForTasks() {02 ch := make(chan string, 1)0304 go func() {05 for p := range ch {06 fmt.Println(&quot;employee : working :&quot;, p)07 }08 }()0910 const work = 1011 for w := 0; w &lt; work; w++ {12 ch &lt;- &quot;paper&quot;13 }1415 close(ch)16 } 第 2 行定义了一个缓冲区为 1 的通道。第 4 至 8 行，你雇用了 1 个员工来完成工作。 for range 用来处理通道的接收工作。每一份文件被接收后，都在第 6 行进行了处理。 第 10 至 13 行，你开始把任务发送给员工。 如果你的员工处理任务的速度和你发送的速度一样快，那么你们之间的延迟会减少。但是每次你成功发送，你可以相信上次提交的任务正在被处理。 第 15 行，调用了内置函数 close。当员工们完成分配的工作后就可以自由离开。但是，最后 1 个工作在 for range 结束之前会被接收。 不携带数据进行通信 - Context最后一个场景下，你会看到如何使用 context 包的 Context 来取消一个正在运行的 goroutine。这是利用关闭无缓冲区的通道来进行不携带数据通信来实现的。 你是经理，雇用了 1 名员工。这次你不希望等待未知的时间。你有个慎重考虑过的截止日期。如果员工届时无法完成，那么你就不等了。 Listing 10https://play.golang.org/p/6GQbN5Z7vC 12345678910111213141516171819202101 func withTimeout() {02 duration := 50 * time.Millisecond0304 ctx, cancel := context.WithTimeout(context.Background(), duration)05 defer cancel()0607 ch := make(chan string, 1)0809 go func() {10 time.Sleep(time.Duration(rand.Intn(100)) * time.Millisecond)11 ch &lt;- &quot;paper&quot;12 }()1314 select {15 case p := &lt;-ch:16 fmt.Println(&quot;work complete&quot;, p)1718 case &lt;-ctx.Done():19 fmt.Println(&quot;moving on&quot;)20 }21 } 第 2 行定义了一个 duration 值，用来声明员工完成任务需要花费的时间。第 4 行创建了一个超时时间为 50ms 的 context.Context 值。context 包的 WithTimeout 函数返回一个 Context 值和 cancellation (取消)函数。 context 包创建了一个 goroutine，它的作用是当 duration 值满足时，关闭与Context 关联的无缓冲区的通道。无论结果如何，你都有责任调用 cancel 函数。 这将清除为 Context 创建的内容。cancel 函数被多次调用是允许的。 第 5 行，cancel 函数被延迟到 withTimeout 函数结束之前执行。第 7 行创建了一个缓冲区等于 1 的通道，将被用于员工给你发送工作成果。第 9 至 12 行，员工被雇用并立即投入工作。你不知道员工花费多长时间来完成。第 14 至 20 行，你使用 select 语句来接收两个通道的信息。第 15 行的接收动作，你等待员工发送他们的结果。第 18 行的接收动作，你等待 50ms 达到时发出的信号。哪个信号先到，哪个优先处理。 该算法的一个重要方面是使用了缓冲区为 1 的通道。如果员工没有及时完成，你不用给员工任何通知就可以继续。从员工的角度，他们在第 11 行发送报告，并且不知道你是否接收。如果你使用无缓冲区通道，当你继续工作时，员工会永远阻塞。这将产生 goroutine 泄漏。所以使用缓冲区为 1 的通道来阻止这种情况发生。 结论通信的可靠性保障、通道状态和发送等特性，对于我们使用或理解通道（并发）非常重要。它将帮助我们在写并发程序和算法时实现最佳行为，也帮助我们发现错误。 在这篇博文里，我分享了一些样例程序来演示不同场景下的通信工作的属性。每个规则都有例外，但这些模式是一个不错的基础。 以下是一些总结： 语言技巧 使用通道来编排和协调 goroutine 关注通信属性而不是数据分享 携带数据进行通信，或者不携带数据进行通信 质疑同步访问共享状态的用途 在某些情况下，通道可能更简单 无缓冲区通道： 接收动作发生在发送动作之前 收益：100% 保证信号被接收 代价: 信号被接收的时间延迟为未知 缓冲区通道: 发送动作发生在接收动作之前 收益：减少通信之间的阻塞延时 代价：无法保证信号何时被接收。 缓冲区越大，保证越弱。 缓冲区为 1 可以提供延迟保障 关闭通道： 关闭发生在接收动作之前（类似缓冲区） 不携带数据进行通信 适合取消和截止时间的通信 nil 通道: 发送和接收都会阻塞 关闭通信 非常适合速率限制或暂时停工 设计理念 如果一个通道上的任意发送会导致发送 goroutine 阻塞： 不要使用缓冲区大于 1 的通道 使用缓冲区大于 1 的通道需要合理的理由和测量 必须知道发送 goroutine 阻塞时会发生什么 如果一个通道上的任意发送不会造成发送 goroutine 阻塞： 每次发送你有确切的缓冲区的数目 扇出模式 你有缓冲区的最大容量 丢弃模式 缓冲区宁少勿多 使用缓冲区时，不要考虑性能 缓冲区可以帮助减少通信之间的阻塞延时 将阻塞延时减少到零并不意味着更高的吞吐量 如果缓冲区为 1 已经给你足够的吞吐量，那么保持就好了 质疑大于 1 的缓冲区。如果有必要采用大于 1 的缓冲区，需要通过测试来确定缓冲区的大小 在保证足够吞吐量的同时，缓冲区越小越好 译文原文Behavior Channels","link":"/2020/06/23/go-channel-study.html"},{"title":"canvas toDataURL方法中的quality参数对PNG图片不生效","text":"在使用 canvas 生成图片时，使用到了 toDataURL 的方法。 1canvas.toDataURL(&quot;image/png&quot;,quality); 对于参数 0 &lt;= quality &lt;= 1，quality 越小，压缩质量越差，生成的图片尺寸越小，图像越模糊。对于 jpg 格式的图片，这个参数是生效的。但对于 png 格式的图片，无论 quality 是任何值，最终生成的图片都一样的体积。 这是因为 png 格式输入无损图片格式，不管怎么设置，quality 永远等于 1。 这就是 quality 参数对 png 格式图片不生效的原因。 只有在指定图片格式为 image/jpeg 或 image/webp 的情况下，可以从 0 到 1 的区间内选择图片的质量。也就是说 quality 参数只对 jpg 和 webp 图片有效。 参考资料：Canvas toDataURL文档","link":"/2023/11/20/png-image-canvas-to-dataurl-quality-not-working.html"},{"title":"CoreDNS 的 template、hosts、file 插件使用方法","text":"CoreDNS 是一个开源的域名系统（DNS）服务器，用于将域名解析为 IP 地址以实现网络通信。它是一个用 Go 语言编写的可扩展 DNS 服务器，旨在取代传统的 DNS 服务器并提供更灵活、可配置的解析方案。 安装配置一个极简的Corefile配置如下 1234.:PORT { whoami log} DNS服务默认端口为53。本机测试期间，选择使用 1053 端口。 首先创建名为 Corefile1 的配置文件，内容如下： 1234.:1053 { whoami log} 启动 coredns 服务： 1234% coredns -conf ./Corefile1.:1053CoreDNS-1.11.1darwin/arm64, go1.21.0, 使用hosts插件在某个项目中，我要用CoreDNS来 为特定域名指定hosts。这时用到了 hosts 插件。 例如：要实现把 a.example.com 指向 192.168.1.2。 创建名为 Corefile2 的配置文件，内容如下： 12345678.:1053 { whoami hosts { 192.168.1.2 a.example.com fallthrough } log} 启动 coredns服务： 1% coredns -conf ./Corefile2 查询一下 a.example.com 对应的IP 12345678910111213141516171819202122dig @127.0.0.1 -p 1053 a.example.com; &lt;&lt;&gt;&gt; DiG 9.10.6 &lt;&lt;&gt;&gt; @127.0.0.1 -p 1053 a.example.com; (1 server found);; global options: +cmd;; Got answer:;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 64095;; flags: qr aa rd; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1;; WARNING: recursion requested but not available;; OPT PSEUDOSECTION:; EDNS: version: 0, flags:; udp: 4096;; QUESTION SECTION:;a.example.com. IN A;; ANSWER SECTION:a.example.com. 3600 IN A 192.168.1.2;; Query time: 3 msec;; SERVER: 127.0.0.1#1053(127.0.0.1);; WHEN: Thu Nov 23 10:58:16 CST 2023;; MSG SIZE rcvd: 71 可以看到 a.example.com. 3600 IN A 192.168.1.2，符合预期。 使用template插件要用CoreDNS来实现泛域名或具有特定规则的域名指定hosts，需要 template 插件。 例如：要实现把 *.example.com 指向 192.168.1.1。创建名为 Corefile3 的配置文件，内容如下： 12345678910111213.:1053 { whoami hosts { 192.168.1.2 a.example.com fallthrough } template IN A example.com { match .*\\.example\\.com answer &quot;{{ .Name }} 60 IN A 192.168.1.1&quot; fallthrough } log} 启动 coredns服务： 1% coredns -conf ./Corefile3 查询一下 a.example.com 对应的IP 12345678910111213141516171819202122dig @127.0.0.1 -p 1053 b.example.com; &lt;&lt;&gt;&gt; DiG 9.10.6 &lt;&lt;&gt;&gt; @127.0.0.1 -p 1053 b.example.com; (1 server found);; global options: +cmd;; Got answer:;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 49105;; flags: qr aa rd; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1;; WARNING: recursion requested but not available;; OPT PSEUDOSECTION:; EDNS: version: 0, flags:; udp: 4096;; QUESTION SECTION:;b.example.com. IN A;; ANSWER SECTION:b.example.com. 60 IN A 192.168.1.1;; Query time: 1 msec;; SERVER: 127.0.0.1#1053(127.0.0.1);; WHEN: Thu Nov 23 11:11:36 CST 2023;; MSG SIZE rcvd: 71 可以看到 b.example.com. 60 IN A 192.168.1.1，符合预期。 同时使用 hosts 和 template 插件在使用 Corefile3 时，如果查询 a.example.com 的解析，就会发现一个奇怪的现象。 12345678910111213141516171819202122dig @127.0.0.1 -p 1053 a.example.com; &lt;&lt;&gt;&gt; DiG 9.10.6 &lt;&lt;&gt;&gt; @127.0.0.1 -p 1053 a.example.com; (1 server found);; global options: +cmd;; Got answer:;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 28857;; flags: qr aa rd; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1;; WARNING: recursion requested but not available;; OPT PSEUDOSECTION:; EDNS: version: 0, flags:; udp: 4096;; QUESTION SECTION:;a.example.com. IN A;; ANSWER SECTION:a.example.com. 60 IN A 192.168.1.1;; Query time: 0 msec;; SERVER: 127.0.0.1#1053(127.0.0.1);; WHEN: Thu Nov 23 11:16:16 CST 2023;; MSG SIZE rcvd: 71 从直观看，a.example.com设置了hosts，应该是优先级更高才对，应该返回 192.168.1.2。但实际返回了templeat插件里的结果 192.168.1.1。 那么是不是插件顺序的问题呢？因为template插件在后面，覆盖了hosts插件的结果呢？实验一下。 创建 Corefile4，颠倒一下template和hosts的位置。 12345678910111213.:1053 { whoami template IN A example.com { match .*\\.example\\.com answer &quot;{{ .Name }} 60 IN A 192.168.1.1&quot; fallthrough } hosts { 192.168.1.2 a.example.com fallthrough } log} 重新启动coredns 1coredns -conf ./Corefile4 运行结果： 12345678910111213141516171819202122dig @127.0.0.1 -p 1053 a.example.com; &lt;&lt;&gt;&gt; DiG 9.10.6 &lt;&lt;&gt;&gt; @127.0.0.1 -p 1053 a.example.com; (1 server found);; global options: +cmd;; Got answer:;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 4163;; flags: qr aa rd; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1;; WARNING: recursion requested but not available;; OPT PSEUDOSECTION:; EDNS: version: 0, flags:; udp: 4096;; QUESTION SECTION:;a.example.com. IN A;; ANSWER SECTION:a.example.com. 60 IN A 192.168.1.1;; Query time: 0 msec;; SERVER: 127.0.0.1#1053(127.0.0.1);; WHEN: Thu Nov 23 11:21:59 CST 2023;; MSG SIZE rcvd: 71 发现结果没有变化。 这是因为：template和hosts插件同时使用时，template的优先级更高，执行的顺序是先执行hosts 插件，后执行template 插件。源码：plugin.cfg 注意注释： 12345678# Directives are registered in the order they should be executed.## Ordering is VERY important. Every plugin will feel the effects of all other# plugin below (after) them during a request, but they must not care what plugin# above them are doing.指令按照它们应该执行的顺序进行注册。顺序非常重要。在一个请求期间，每个插件都会感受到所有其它在它们之后（下方）的插件的影响，但是它们不必关心在上面的插件正在做什么。 翻译一下就是：hosts插件不能关注到template插件所做的变更，但template插件能够改变hosts插件所做的改变。 如何解决第1种方法是改变coredns插件的顺序，这个需要自己更改代码并编译coredns，一般不建议。 第2种方法是新起一个端口来提供服务。创建 Corefile5 ，内容如下： 12345678910111213141516171819.:1053 { whoami hosts { 192.168.1.2 a.example.com fallthrough } forward . 127.0.0.1:2053 log }.:2053 { whoami template IN A example.com { match .*\\.example\\.com answer &quot;{{ .Name }} 60 IN A 192.168.1.1&quot; fallthrough } log} 启动coredns 12345coredns -conf ./Corefile5.:1053.:2053CoreDNS-1.11.1darwin/arm64, go1.21.0, 测试一下 12345678910111213141516171819202122dig @127.0.0.1 -p 1053 a.example.com; &lt;&lt;&gt;&gt; DiG 9.10.6 &lt;&lt;&gt;&gt; @127.0.0.1 -p 1053 a.example.com; (1 server found);; global options: +cmd;; Got answer:;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 60952;; flags: qr aa rd; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1;; WARNING: recursion requested but not available;; OPT PSEUDOSECTION:; EDNS: version: 0, flags:; udp: 4096;; QUESTION SECTION:;a.example.com. IN A;; ANSWER SECTION:a.example.com. 3600 IN A 192.168.1.2;; Query time: 3 msec;; SERVER: 127.0.0.1#1053(127.0.0.1);; WHEN: Thu Nov 23 11:51:35 CST 2023;; MSG SIZE rcvd: 71 发现解析地址已经变为 a.example.com. 3600 IN A 192.168.1.2，符合预期了。 这种方法的弊端是要多发起一次请求，性能有所下降。 有没有其它更好的方法？有的，使用file插件。 使用 file 插件fiel 插件允许使用一个 zone 文件来解析泛域名和指定域名。 首先创建 example.db 的配置文件： 1234567891011$TTL 3600$ORIGIN example.com.@ IN SOA ns.example.com. contact.example.com. ( 2023112301 ; serial 1d ; refresh 2h ; retry 4w ; expire 1h ; nxdomain ttl )a.example.com. IN A 192.168.1.2*.example.com. IN A 192.168.1.1 创建 Corefile6，内容如下： 12345.:1053 { whoami file ./example.db log} 启动coredns 1coredns -conf ./Corefile6 然后查一下 a.example.com 和 b.example.com 的解析： 1234567891011dig @127.0.0.1 -p 1053 a.example.com......;; ANSWER SECTION:a.example.com. 3600 IN A 192.168.1.2......dig @127.0.0.1 -p 1053 b.example.com......;; ANSWER SECTION:b.example.com. 3600 IN A 192.168.1.1 可以看到是符合预期的。","link":"/2023/11/23/coredns-usage-hosts-templeate-file-plugins.html"},{"title":"跨域请求获取不到特定 header 头的解决办法","text":"一个需求是这样：要用 js 来读 ajax 接口中的header头的某个字段，例如 Etag。这时有可能出现两种报错信息。第一：全部 Response 字段里获取不到指定字段。 12345console.log(xhr.getAllResponseHeaders());/*content-length: 44content-type: application/json; charset=utf-8*/ 发现没有 Etag 这个字段。 第二：获取指定字段时提示错误(Cross Domain Resource Sharing GET: ‘refused to get unsafe header “xxx”‘ from Response) 12xhr.getResponseHeader('Etag')VM172:1 Refused to get unsafe header &quot;Etag&quot; 这是因为跨域请求中，不能随意去读取 Response 中的字段，需要在网关（nginx、apache）等加上如下字段： 1Access-Control-Expose-Headers: ETag 显式地把字段暴露出来，才能在 ajax 请求中使用。 加上以上字段之后，发现可以获取到了： 123456console.log(xhr.getAllResponseHeaders());/*content-length: 44content-type: application/json; charset=utf-8Etag:xxxx*/","link":"/2023/11/26/cros-refused-to-get-unsafe-header.html"},{"title":"Uncaught ReferenceError: Buffer is not defined 的错误处理","text":"在使用 xml2json 包时，发现使用 json2xml 函数时，出现以下错误 Uncaught ReferenceError: Buffer is not defined 。 查看代码发现报错语句为 json2xml.js 的第 4 行： 1if (json instanceof Buffer) 这是因为在某些情况下，Buffer 包不存在。 解决这个问题的方法，第 1 步是先安装包： 1npm install --save buffer 第 2 步是引用包，并且赋值给 window。 1234import { Buffer } from 'buffer';// @ts-ignorewindow.Buffer = Buffer; 在浏览器场景下，这样即可解决。","link":"/2023/12/06/uncaught-reference-error-buffer-is-not-defined.html"},{"title":"DNS 基础知识","text":"在排查一些DNS 问题时，顺便学了些DNS的基础知识，记录一下。 什么是根、顶级、权威、本地域名服务？域名解析过程涉及4个DNS服务器，分别如下： 分类 作用 根DNS服务器 英文：Root nameserver。本地域名服务器在本地查询不到解析结果时，则第一步会向它进行查询，并获取顶级域名服务器的IP地址。 顶级域名服务器 英文：Tld nameserver。负责管理在该顶级域名服务器下注册的二级域名，例如 www.example.com，.com则是顶级域名服务器，在向它查询时，可以返回一级域名“example.com”所在的权威域名服务器地址。 权威域名服务器 英文：authoritative nameserver。在特定区域内具有唯一性，负责维护该区域内的域名与IP地址之间的对应关系，例如云解析DNS。 本地域名服务器 英文：DNS resolver或Local DNS。本地域名服务器是响应来自客户端的递归请求，并最终跟踪直到获取到解析结果的DNS服务器。例如用户本机自动分配的DNS、运营商ISP分配的DNS、谷歌/114公共DNS等。 什么是 FQDN(Fully qualified domain name)？FQDN是完整域名。以.结束的域名是FQDN，例如example.com.是FQDN，但example.com不是。对FQDN，操作系统会直接查询DNS server。那么非FQDN呢？需要做一番操作，使其变为 FQDN之后发起查询。接着就用到search和ndots了。 什么是search和ndots？查看一下 /etc/resolv.conf 文件，大概内容如下： 123nameserver xx.xx.0.10search kube-system.svc.cluster.local svc.cluster.local cluster.localoptions ndots:5 要查一个域名，例如查找 pod-1.namespace-1，会拼接pod-1.namespace-1.svc.cluster.local 再进行查询。 参数 描述 nameserver 定义DNS服务器的IP地址。 search 设置域名的查找后缀规则，查找配置越多，说明域名解析查找匹配次数越多。ACK集群匹配有kube-system.svc.cluster.local、svc.cluster.local、cluster.local3个后缀，最多进行8次查询才能得到正确解析结果，因为集群里面进行IPV4和IPV6查询各四次。 options 定义域名解析配置文件选项，支持多个KV值。例如该参数设置成ndots:5，说明如果访问的域名字符串内的点字符数量超过ndots值，则认为是完整域名，并被直接解析；如果不足ndots值，则追加search段后缀再进行查询。 rcode 表示什么意思？DNS返回值。rcode枚举值如下：Rcode=0（无错误条件）Rcode=3（名称错误）、1（格式错误）、2（服务器故障）、4（未实现）或5（拒绝）查询没有有效答案。 DNS日志格式参考1234567891011{ &quot;src_port&quot;: &quot;53&quot;, &quot;dst_addr&quot;: &quot;10.10.11.12&quot;, &quot;dst_port&quot;: &quot;54017&quot;, &quot;transport&quot;: &quot;UDP&quot;, &quot;dns_msg_id&quot;: &quot;12095&quot;, &quot;query_type&quot;: &quot;AAAA&quot;, &quot;dns_msg_flags&quot;: &quot;QR RD&quot;, &quot;query_name&quot;: &quot;demo.example.com.&quot;, &quot;rcode&quot;: &quot;2&quot; } query_type什么意思？query_type是解析记录类型，就是常见的A记录、CNAME记录等类型。 解析记录类型 场景 A记录 添加 A 记录可实现将域名指向 IPv4 地址。 CNAME记录 当需要将域名指向另一个域名，再由另一个域名提供 IP 地址，就需要添加 CNAME 记录，最常用到 CNAME 的场景包括做 CDN、企业邮箱、全局流量管理等。 MX记录 设置邮箱时，让邮箱能收到邮件，就需要添加 MX 记录。MX全称为mail exchanger，用于电子邮件系统发邮件时根据收信人的地址后缀来定位邮件服务器。例如，当有人发邮件给“abc@example.com”时，系统将对“example.com”进行DNS中的MX记录解析。如果MX记录存在，系统就根据MX记录的优先级，将邮件转发到与该MX相应的邮件服务器上。 AAAA记录 当预期是实现访问者通过 IPv6 地址访问网站，可以使用 AAAA 记录实现。 TXT记录 如果希望对域名进行标识和说明，可以使用 TXT 记录， TXT 记录多用来做 SPF 记录（反垃圾邮件）。 URL显性/隐性转发 将一个域名指向另外一个已经存在的站点时，需要添加 URL 记录。 NS记录 如果需要把子域名交给其他DNS服务商解析，就需要添加NS记录。 SRV记录 SRV 记录用来标识某台服务器使用了某个服务，常见于微软系统的目录管理。 CAA记录 CAA（Certificate Authority Authorization），即证书颁发机构授权。是一项新的可以添加到DNS记录中的额外字段，通过DNS机制创建CAA资源记录，可以限定域名颁发的证书和CA（证书颁发机构）之间的联系。未经授权的第三方尝试通过其他CA注册获取用于该域名的SSL/TLS证书将被拒绝。域名设置 CAA 记录，使网站所有者，可授权指定CA机构为自己的域名颁发证书，以防止HTTPS证书错误签发，从而提高网站安全性。 PTR记录 反向解析则是指将IP地址映射到域名上，需要您与IDC机房或主机服务商联系实现。 dns_msg_flags什么意思？DNS信息Flags，有如下枚举值： ●QR（Response）：查询请求/响应的标志信息。查询请求时，值为 0；响应时，值为 1。●Opcode：操作码。其中，0 表示标准查询；1 表示反向查询；2 表示服务器状态请求。●AA（Authoritative）：授权应答，该字段在响应报文中有效。值为 1 时，表示名称服务器是权威服务器；值为 0 时，表示不是权威服务器。●TC（Truncated）：表示是否被截断。值为 1 时，表示响应已超过 512 字节并已被截断，只返回前 512 个字节。●RD（Recursion Desired）：期望递归。该字段能在一个查询中设置，并在响应中返回。该标志告诉名称服务器必须处理这个查询，这种方式被称为一个递归查询。如果该位为 0，且被请求的名称服务器没有一个授权回答，它将返回一个能解答该查询的其他名称服务器列表。这种方式被称为迭代查询。●RA（Recursion Available）：可用递归。该字段只出现在响应报文中。当值为 1 时，表示服务器支持递归查询。●Z：保留字段，在所有的请求和应答报文中，它的值必须为 0。●rcode（Reply code）：返回码字段，表示响应的差错状态。当值为 0 时，表示没有错误；当值为 1 时，表示报文格式错误（Format error），服务器不能理解请求的报文；当值为 2 时，表示域名服务器失败（Server failure），因为服务器的原因导致没办法处理这个请求；当值为 3 时，表示名字错误（Name Error），只有对授权域名解析服务器有意义，指出解析的域名不存在；当值为 4 时，表示查询类型不支持（Not Implemented），即域名服务器不支持查询类型；当值为 5 时，表示拒绝（Refused），一般是服务器由于设置的策略拒绝给出应答，如服务器不希望对某些请求者给出应答。","link":"/2023/11/24/dns-basic-knowledge.html"}],"tags":[{"name":"腾讯云","slug":"腾讯云","link":"/tags/%E8%85%BE%E8%AE%AF%E4%BA%91/"},{"name":"版本控制","slug":"版本控制","link":"/tags/%E7%89%88%E6%9C%AC%E6%8E%A7%E5%88%B6/"},{"name":"桌面程序","slug":"桌面程序","link":"/tags/%E6%A1%8C%E9%9D%A2%E7%A8%8B%E5%BA%8F/"},{"name":"http","slug":"http","link":"/tags/http/"},{"name":"断点续传","slug":"断点续传","link":"/tags/%E6%96%AD%E7%82%B9%E7%BB%AD%E4%BC%A0/"},{"name":"php扩展","slug":"php扩展","link":"/tags/php%E6%89%A9%E5%B1%95/"},{"name":"mysql","slug":"mysql","link":"/tags/mysql/"},{"name":"弱关系","slug":"弱关系","link":"/tags/%E5%BC%B1%E5%85%B3%E7%B3%BB/"},{"name":"关系链","slug":"关系链","link":"/tags/%E5%85%B3%E7%B3%BB%E9%93%BE/"},{"name":"cgi","slug":"cgi","link":"/tags/cgi/"},{"name":"fastcgi","slug":"fastcgi","link":"/tags/fastcgi/"},{"name":"query cache","slug":"query-cache","link":"/tags/query-cache/"},{"name":"office","slug":"office","link":"/tags/office/"},{"name":"redis","slug":"redis","link":"/tags/redis/"},{"name":"zunionstore","slug":"zunionstore","link":"/tags/zunionstore/"},{"name":"php","slug":"php","link":"/tags/php/"},{"name":"bug","slug":"bug","link":"/tags/bug/"},{"name":"中文分词","slug":"中文分词","link":"/tags/%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D/"},{"name":"接口","slug":"接口","link":"/tags/%E6%8E%A5%E5%8F%A3/"},{"name":"api","slug":"api","link":"/tags/api/"},{"name":"东京","slug":"东京","link":"/tags/%E4%B8%9C%E4%BA%AC/"},{"name":"旅游","slug":"旅游","link":"/tags/%E6%97%85%E6%B8%B8/"},{"name":"nginx","slug":"nginx","link":"/tags/nginx/"},{"name":"sticky","slug":"sticky","link":"/tags/sticky/"},{"name":"负载均衡","slug":"负载均衡","link":"/tags/%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/"},{"name":"cache","slug":"cache","link":"/tags/cache/"},{"name":"hash tag","slug":"hash-tag","link":"/tags/hash-tag/"},{"name":"swift","slug":"swift","link":"/tags/swift/"},{"name":"sns","slug":"sns","link":"/tags/sns/"},{"name":"社交网站","slug":"社交网站","link":"/tags/%E7%A4%BE%E4%BA%A4%E7%BD%91%E7%AB%99/"},{"name":"PHP","slug":"PHP","link":"/tags/PHP/"},{"name":"性能","slug":"性能","link":"/tags/%E6%80%A7%E8%83%BD/"},{"name":"leetcode","slug":"leetcode","link":"/tags/leetcode/"},{"name":"算法","slug":"算法","link":"/tags/%E7%AE%97%E6%B3%95/"},{"name":"git","slug":"git","link":"/tags/git/"},{"name":"restful","slug":"restful","link":"/tags/restful/"},{"name":"json","slug":"json","link":"/tags/json/"},{"name":"linux","slug":"linux","link":"/tags/linux/"},{"name":"docker","slug":"docker","link":"/tags/docker/"},{"name":"服务器","slug":"服务器","link":"/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8/"},{"name":"证书","slug":"证书","link":"/tags/%E8%AF%81%E4%B9%A6/"},{"name":"HTTPS","slug":"HTTPS","link":"/tags/HTTPS/"},{"name":"js","slug":"js","link":"/tags/js/"},{"name":"markdown","slug":"markdown","link":"/tags/markdown/"},{"name":"微信","slug":"微信","link":"/tags/%E5%BE%AE%E4%BF%A1/"},{"name":"公众号","slug":"公众号","link":"/tags/%E5%85%AC%E4%BC%97%E5%8F%B7/"},{"name":"小程序","slug":"小程序","link":"/tags/%E5%B0%8F%E7%A8%8B%E5%BA%8F/"},{"name":"postman","slug":"postman","link":"/tags/postman/"},{"name":"newman","slug":"newman","link":"/tags/newman/"},{"name":"nodejs","slug":"nodejs","link":"/tags/nodejs/"},{"name":"Phabricator","slug":"Phabricator","link":"/tags/Phabricator/"},{"name":"数学","slug":"数学","link":"/tags/%E6%95%B0%E5%AD%A6/"},{"name":"go","slug":"go","link":"/tags/go/"},{"name":"每周学习","slug":"每周学习","link":"/tags/%E6%AF%8F%E5%91%A8%E5%AD%A6%E4%B9%A0/"},{"name":"https","slug":"https","link":"/tags/https/"},{"name":"scala","slug":"scala","link":"/tags/scala/"},{"name":"分布式系统","slug":"分布式系统","link":"/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/"},{"name":"CAP","slug":"CAP","link":"/tags/CAP/"},{"name":"golang","slug":"golang","link":"/tags/golang/"},{"name":"JAVASCRIPT","slug":"JAVASCRIPT","link":"/tags/JAVASCRIPT/"},{"name":"CANVAS","slug":"CANVAS","link":"/tags/CANVAS/"},{"name":"云原生","slug":"云原生","link":"/tags/%E4%BA%91%E5%8E%9F%E7%94%9F/"},{"name":"CoreDNS","slug":"CoreDNS","link":"/tags/CoreDNS/"},{"name":"javascript","slug":"javascript","link":"/tags/javascript/"},{"name":"gateway","slug":"gateway","link":"/tags/gateway/"},{"name":"DNS","slug":"DNS","link":"/tags/DNS/"}],"categories":[{"name":"服务器","slug":"服务器","link":"/categories/%E6%9C%8D%E5%8A%A1%E5%99%A8/"},{"name":"php","slug":"php","link":"/categories/php/"},{"name":"桌面程序","slug":"桌面程序","link":"/categories/%E6%A1%8C%E9%9D%A2%E7%A8%8B%E5%BA%8F/"},{"name":"mysql","slug":"mysql","link":"/categories/mysql/"},{"name":"散文","slug":"散文","link":"/categories/%E6%95%A3%E6%96%87/"},{"name":"redis","slug":"redis","link":"/categories/redis/"},{"name":"接口","slug":"接口","link":"/categories/%E6%8E%A5%E5%8F%A3/"},{"name":"旅游","slug":"旅游","link":"/categories/%E6%97%85%E6%B8%B8/"},{"name":"swift","slug":"swift","link":"/categories/swift/"},{"name":"社交网站","slug":"社交网站","link":"/categories/%E7%A4%BE%E4%BA%A4%E7%BD%91%E7%AB%99/"},{"name":"PHP","slug":"PHP","link":"/categories/PHP/"},{"name":"算法","slug":"算法","link":"/categories/%E7%AE%97%E6%B3%95/"},{"name":"git","slug":"git","link":"/categories/git/"},{"name":"api","slug":"api","link":"/categories/api/"},{"name":"linux","slug":"linux","link":"/categories/linux/"},{"name":"docker","slug":"docker","link":"/categories/docker/"},{"name":"nginx","slug":"nginx","link":"/categories/nginx/"},{"name":"js","slug":"js","link":"/categories/js/"},{"name":"微信","slug":"微信","link":"/categories/%E5%BE%AE%E4%BF%A1/"},{"name":"nodejs","slug":"nodejs","link":"/categories/nodejs/"},{"name":"数学","slug":"数学","link":"/categories/%E6%95%B0%E5%AD%A6/"},{"name":"https","slug":"https","link":"/categories/https/"},{"name":"scala","slug":"scala","link":"/categories/scala/"},{"name":"CAP","slug":"CAP","link":"/categories/CAP/"},{"name":"JAVASCRIPT","slug":"JAVASCRIPT","link":"/categories/JAVASCRIPT/"},{"name":"云原生","slug":"云原生","link":"/categories/%E4%BA%91%E5%8E%9F%E7%94%9F/"},{"name":"javascript","slug":"javascript","link":"/categories/javascript/"}],"pages":[]}